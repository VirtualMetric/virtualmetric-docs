---
sidebar_label: From Logstash
---

# Migrating from Logstash to DataStream

This guide provides a comprehensive approach to migrating from Logstash to DataStream, focusing on configuration conversion, conceptual mapping, and practical implementation steps.

## Feature Comparison

| Feature | Logstash | DataStream |
|---------|----------|------------|
| Configuration Format | Ruby-like DSL | YAML |
| Components | Inputs, Filters, Outputs | Devices, Processors, Outputs |
| Plugin System | Ruby Gems | Built-in Types |
| Pipeline Structure | Inputs → Filters → Outputs | Devices → Pipelines → Outputs |
| Runtime | JVM-based | Go-based |
| Resource Usage | Higher memory footprint | Lower memory footprint |
| Configuration Validation | Runtime | Pre-deployment |

## Component Mapping

### Inputs to Devices

Logstash inputs map to DataStream devices:

| Logstash Input | DataStream Device |
|----------------|-------------------|
| http | http |
| syslog | syslog |
| tcp | tcp |
| udp | udp |
| beats | http |
| file | file (coming soon) |
| azure_event_hubs | azure_monitor |
| elasticsearch | elasticsearch_in (coming soon) |

### Filters to Processors

Logstash filters map to DataStream processors:

| Logstash Filter | DataStream Processor |
|-----------------|----------------------|
| grok | grok |
| csv | csv |
| json | json |
| kv | kv |
| mutate (add_field) | set |
| mutate (remove_field) | remove |
| mutate (rename) | rename |
| ruby | script |
| date | date (coming soon) |

### Outputs to Outputs

Logstash outputs map to DataStream outputs:

| Logstash Output | DataStream Output |
|-----------------|-------------------|
| elasticsearch | elasticsearch |
| s3 | s3 |
| http | http |
| kafka | kafka (coming soon) |
| stdout | stdout (for testing) |

## Migration Steps

1. **Analyze Current Logstash Configuration**
   - Identify all inputs, filters, and outputs
   - Document conditional logic and Ruby code
   - Catalog custom plugins and gems

2. **Create Device Configurations**
   - Convert Logstash inputs to DataStream devices
   - Example: Convert HTTP input to HTTP device

   ```ruby
   # Logstash HTTP Input
   input {
     http {
       host => "0.0.0.0"
       port => 8080
       codec => "json"
     }
   }
   ```

   ```yaml
   # DataStream HTTP Device
   - id: 1
     name: http_input
     type: http
     properties:
       address: "0.0.0.0"
       port: 8080
       content_type: "application/json"
   ```

3. **Create Processor Configurations**
   - Map Logstash filters to DataStream processors
   - Example: Convert Grok filter to Grok processor

   ```ruby
   # Logstash Grok Filter
   filter {
     grok {
       match => { "message" => "%{COMBINEDAPACHELOG}" }
     }
   }
   ```

   ```yaml
   # DataStream Grok Processor
   - grok:
     - field: message
     - patterns:
       - "%{COMBINEDAPACHELOG}"
   ```

4. **Create Output Configurations**
   - Transform Logstash outputs to DataStream outputs
   - Example: Convert Elasticsearch output

   ```ruby
   # Logstash Elasticsearch Output
   output {
     elasticsearch {
       hosts => ["localhost:9200"]
       index => "logs-%{+YYYY.MM.dd}"
       user => "elastic"
       password => "changeme"
     }
   }
   ```

   ```yaml
   # DataStream Elasticsearch Output
   - id: 1
     name: es_output
     type: elasticsearch
     properties:
       hosts: ["localhost:9200"]
       index: "logs-{@timestamp:YYYY.MM.dd}"
       auth:
         username: "elastic"
         password: "changeme"
   ```

5. **Test Configuration**
   - Deploy in a test environment
   - Validate data flows correctly
   - Compare output with Logstash results

6. **Production Deployment**
   - Run both systems in parallel
   - Gradually transition traffic to DataStream
   - Monitor performance and data integrity

## Configuration Examples

### Conditional Logic Conversion

```ruby
# Logstash Conditional
filter {
  if [type] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  } else if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
    }
  }
}
```

```yaml
# DataStream Equivalent
pipelines:
  - name: router
    processors:
      - script:
        - lang: golang
          source: |
            package main
            
            func main() {
              logType, ok := logEntry["type"].(string)
              if ok {
                if logType == "apache" {
                  setPipeline("apache_pipeline")
                } else if logType == "syslog" {
                  setPipeline("syslog_pipeline")
                }
              }
            }

  - name: apache_pipeline
    processors:
      - grok:
        - field: message
        - patterns:
          - "%{COMBINEDAPACHELOG}"

  - name: syslog_pipeline
    processors:
      - grok:
        - field: message
        - patterns:
          - "%{SYSLOGLINE}"
```

### Ruby Code Conversion

```ruby
# Logstash Ruby Filter
filter {
  ruby {
    code => "
      event.set('uppercase_message', event.get('message').upcase)
      if event.get('status').to_i >= 400
        event.set('is_error', true)
      end
    "
  }
}
```

```yaml
# DataStream Script Processor
- script:
  - lang: golang
    source: |
      package main
      
      func main() {
        if msg, ok := logEntry["message"].(string); ok {
          logEntry["uppercase_message"] = strings.ToUpper(msg)
        }
        
        if status, ok := logEntry["status"].(string); ok {
          statusCode, err := strconv.Atoi(status)
          if err == nil && statusCode >= 400 {
            logEntry["is_error"] = true
          }
        }
      }
```

## Advanced Considerations

### Multiple Pipelines

For Logstash configurations with multiple pipelines, map each pipeline to a distinct DataStream pipeline and configure appropriate routing.

### Custom Plugins

For custom Logstash plugins, implement equivalent functionality:

1. Use the DataStream Script processor for custom logic
2. Create reusable pipeline components
3. Implement data transformations in Go

### Date/Timestamp Handling

For Logstash date filter functionality:

```yaml
# DataStream Date Handling
- script:
  - lang: golang
    source: |
      package main
      
      func main() {
        if timestamp, ok := logEntry["timestamp"].(string); ok {
          parsedTime, err := time.Parse("2006-01-02T15:04:05", timestamp)
          if err == nil {
            logEntry["@timestamp"] = parsedTime.Format(time.RFC3339)
          }
        }
      }
```

## Performance Tuning

After migrating, optimize DataStream's performance:

1. Configure appropriate batch sizes and flush intervals
2. Adjust buffer settings to match your throughput requirements
3. Implement efficient filtering early in processing pipelines
4. Use appropriate worker counts for high-volume inputs

## Logstash-Specific Feature Migration

| Logstash Feature | DataStream Approach |
|------------------|---------------------|
| Persistent Queues | Use DataStream's disk persistence |
| Dead Letter Queues | Implement error handling in processors |
| Pipeline-to-Pipeline | Use DataStream's pipeline routing |
| Monitoring API | Use DataStream's metrics endpoint |

## Additional Resources

- Sample configuration templates
- Migration validation tools
- Community support forum
- Performance tuning guidelines

For assistance with complex migrations, contact our professional services team.
