---
sidebar_label: From Logstash
---

# Migrating from Logstash to DataStream

This guide provides a comprehensive approach to migrating from **Logstash** to **DataStream**, focusing on configuration conversion, conceptual mapping, and practical implementation steps.

## Feature Comparison

| Feature | Logstash | DataStream |
|---------|----------|------------|
| Configuration Format | Ruby-like DSL | YAML |
| Components | Inputs, Filters, Outputs | Devices, Processors, Outputs |
| Plugin System | Ruby Gems | Built-in Types |
| Pipeline Structure | Inputs → Filters → Outputs | Devices → Pipelines → Outputs |
| Runtime | JVM-based | Go-based |
| Resource Usage | Higher memory footprint | Lower memory footprint |
| Configuration Validation | Runtime | Pre-deployment |

## Component Mapping

### Inputs to Devices

**Logstash** inputs map to **DataStream** devices:

| Logstash Input | DataStream Device |
|----------------|-------------------|
| http | http |
| syslog | syslog |
| tcp | tcp |
| udp | udp |
| beats | http |
| file | file (coming soon) |
| azure_event_hubs | azure_monitor |
| elasticsearch | elasticsearch_in (coming soon) |

### Filters to Processors

**Logstash** filters map to **DataStream** processors:

| Logstash Filter | DataStream Processor |
|-----------------|----------------------|
| grok | grok |
| csv | csv |
| json | json |
| kv | kv |
| mutate (add_field) | set |
| mutate (remove_field) | remove |
| mutate (rename) | rename |
| ruby | script |
| date | date (coming soon) |

### Outputs to Outputs

**Logstash** outputs map to **DataStream** targets:

| Logstash Output | DataStream Output |
|-----------------|-------------------|
| elasticsearch | elasticsearch |
| s3 | s3 |
| http | http |
| kafka | kafka (coming soon) |
| stdout | stdout (for testing) |

## Migration Steps

1. **Analyze Current Logstash Configuration** - Identify all inputs, filters, and outputs. Document conditional logic and Ruby code. Catalog custom plugins and gems.

2. **Create Device Configurations** - Convert **Logstash** inputs to **DataStream** devices, e.g.:

    <ExampleGrid>
      <CommentCol>
        **Logstash** _HTTP_ input:
      </CommentCol>
      <CodeCol>
        ```ruby title="Ruby"
        input {
          http {
            host => "0.0.0.0"
            port => 8080
            codec => "json"
          }
        }
        ```
      </CodeCol>
      <CommentCol>
        **DataStream** _HTTP_ device:
      </CommentCol>
      <CodeCol>
        ```yaml title="YAML"
        - id: 1
          name: http_input
          type: http
          properties:
            address: "0.0.0.0"
            port: 8080
            content_type: "application/json"
        ```
      </CodeCol>
    </ExampleGrid>

3. **Create Processor Configurations** - Map Logstash filters to DataStream processors, e.g.:

    <ExampleGrid>
      <CommentCol>
        **Logstash** _Grok_ filter:
      </CommentCol>
      <CodeCol>
        ```ruby title="Ruby"
        filter {
          grok {
            match => { "message" => "%{COMBINEDAPACHELOG}" }
          }
        }
        ```
      </CodeCol>
      <CommentCol>
        **DataStream** _Grok_ processor:
      </CommentCol>
      <CodeCol>
        ```yaml title="YAML"
        - grok:
          - field: message
          - patterns:
            - "%{COMBINEDAPACHELOG}"
        ```
      </CodeCol>
    </ExampleGrid>

4. **Create Target Configurations** - Transform **Logstash** outputs to **DataStream** targets, e.g.:

    <ExampleGrid>
      <CommentCol>
        **Logstash** _Elasticsearch_ output:
      </CommentCol>
      <CodeCol>
        ```ruby title="Ruby"
        output {
          elasticsearch {
            hosts => ["localhost:9200"]
            index => "logs-%{+YYYY.MM.dd}"
            user => "elastic"
            password => "changeme"
          }
        }
        ```
      </CodeCol>
      <CommentCol>
        **DataStream** _Elasticsearch_ output:
      </CommentCol>
      <CodeCol>
        ```yaml title="YAML"
        - name: es_target
          type: elasticsearch
          properties:
            hosts: ["localhost:9200"]
            index: "logs-{@timestamp:YYYY.MM.dd}"
            auth:
              username: "elastic"
              password: "changeme"
        ```
      </CodeCol>
    </ExampleGrid>

5. **Test Configuration**  - Deploy in a test environment. Validate data flows correctly. Compare output with **Logstash** results.

6. **Production Deployment** - Run both systems in parallel. Gradually transition traffic to **DataStream**. Monitor performance and data integrity

## Configuration Examples

* A conditional Logic conversion:

  <ExampleGrid>
    <CommentCol>
      **Logstash** conditional:
    </CommentCol>
    <CodeCol>
      ```ruby title="Ruby"
      filter {
        if [type] == "apache" {
          grok {
            match => { "message" => "%{COMBINEDAPACHELOG}" }
          }
        } else if [type] == "syslog" {
          grok {
            match => { "message" => "%{SYSLOGLINE}" }
          }
        }
      }
      ```
    </CodeCol>
    <CommentCol>
      **DataStream** equivalent:
    </CommentCol>
    <CodeCol>
      ```yaml title="YAML"
      pipelines:
        - name: router
          processors:
            - script:
              - lang: golang
                source: |
                  package main
                  
                  func main() {
                    logType, ok := logEntry["type"].(string)
                    if ok {
                      if logType == "apache" {
                        setPipeline("apache_pipeline")
                      } else if logType == "syslog" {
                        setPipeline("syslog_pipeline")
                      }
                    }
                  }

        - name: apache_pipeline
          processors:
            - grok:
              field: message
              patterns:
                - "%{COMBINEDAPACHELOG}"

        - name: syslog_pipeline
          processors:
            - grok:
              field: message
              patterns:
                - "%{SYSLOGLINE}"
      ```
    </CodeCol>
  </ExampleGrid>

* Ruby code conversion:

  <ExampleGrid>
    <CommentCol>
      **Logstash** Ruby filter:
    </CommentCol>
    <CodeCol>
      ```ruby title="Ruby"
      filter {
        ruby {
          code => "
            event.set('uppercase_message', event.get('message').upcase)
            if event.get('status').to_i >= 400
              event.set('is_error', true)
            end
          "
        }
      }
      ```
    </CodeCol>
    <CommentCol>
      **DataStream** _Script_ processor:
    </CommentCol>
    <CodeCol>
      ```yaml title="YAML"
      - script:
        - lang: golang
          source: |
            package main
            
            func main() {
              if msg, ok := logEntry["message"].(string); ok {
                logEntry["uppercase_message"] = strings.ToUpper(msg)
              }
              
              if status, ok := logEntry["status"].(string); ok {
                statusCode, err := strconv.Atoi(status)
                if err != nil && statusCode >= 400 {
                  logEntry["is_error"] = true
                }
              }
            }
          
      ```
    </CodeCol>
  </ExampleGrid>

## Advanced Considerations

### Multiple Pipelines

For Logstash configurations with multiple pipelines, map each pipeline to a distinct DataStream pipeline and configure appropriate routing.

### Custom Plugins

For custom **Logstash** plugins, implement equivalent functionality:

1. Use the **DataStream** _Script_ processor for custom logic
2. Create reusable pipeline components
3. Implement data transformations in Go

### Date/Timestamp Handling

For **Logstash** date filter functionality, **DataStream** date handling:

```yaml title="YAML" 
- script:
  - lang: golang
    source: |
      package main
      
      func main() {
        if timestamp, ok := logEntry["timestamp"].(string); ok {
          parsedTime, err := time.Parse("2006-01-02T15:04:05", timestamp)
          if err == nil {
            logEntry["@timestamp"] = parsedTime.Format(time.RFC3339)
          }
        }
      }
```

## Performance Tuning

After migrating, optimize **DataStream**'s performance:

- Configure appropriate batch sizes and flush intervals
- Adjust buffer settings to match your throughput requirements
- Implement efficient filtering early in processing pipelines
- Use appropriate worker counts for high-volume inputs

## Logstash-Specific Feature Migration

| Logstash Feature | DataStream Approach |
|------------------|---------------------|
| Persistent Queues | Use **DataStream**'s disk persistence |
| Dead Letter Queues | Implement error handling in processors |
| Pipeline-to-Pipeline | Use **DataStream**'s pipeline routing |
| Monitoring API | Use **DataStream**'s metrics endpoint |

## Additional Resources

- Sample configuration templates
- Migration validation tools
- Community support forum
- Performance tuning guidelines

For assistance with complex migrations, contact our professional services team.
