---
pagination_prev: null
sidebar_label: Overview
---

# Targets: Overview

Targets are the **final stage** in the DataStream processing flow. They forward processed data to external consumers and convert from standardized pipeline output to destination-specific formats.

> Provider → Device → Preprocessing → Pipeline → Postprocessing → **Target** → Consumer

<Image id="targets-flow" />

Targets provide flexible options for data persistence and integration with various analysis platforms.

## Definitions

Targets serve as the endpoint destinations for processed data within the **Director** system. They operate on the following principles:

1. **Output Configuration**: Targets define where and how data should be delivered after processing.
2. **Format Adaptation**: They handle the conversion of internal data structures to destination-specific formats.
3. **Delivery Management**: Targets manage connection pooling, batching, and retry mechanisms.
4. **Destination Integration**: They provide authentication and protocol-specific features for various systems.

:::note
Targets enable:

> **Persistence**: Local and remote storage with various retention options.\
> **Integration**: Seamless connection to analytics platforms and messaging systems.

They also support _data transformation_, and _delivery confirmation_.
:::

## Target Architecture

DataStream uses a small number of **target implementations** that each support multiple **integrations** (services). This efficiency pattern means similar services share implementation code:

| Implementation | Integrations Supported |
|---------------|------------------------|
| **awss3Target** | Amazon S3, Security Lake, Cloudflare R2, MinIO, DigitalOcean Spaces, and 6 others |
| **kafkaTarget** | Apache Kafka, Confluent Cloud, Aiven Kafka, Redpanda, WarpStream, Amazon MSK, IBM Event Streams |
| **elasticTarget** | Elasticsearch, Amazon OpenSearch, Elastic Security |
| **splunkTarget** | Splunk, Splunk Security |

Each integration gets its own documentation chapter with platform-specific details, even when the underlying implementation is shared.

## Postprocessing Pipelines

Targets can have pipelines attached for output format transformation via the `pipelines` field. These **postprocessing pipelines** convert standardized pipeline output to target-specific formats before delivery.

Common postprocessing tasks include:
- Field mapping to destination schemas (ECS, CIM, ASIM)
- Format conversion for target requirements
- Data filtering before delivery

## Configuration

All targets share the following base configuration fields:

|Field|Required|Default|Description|
|---|---|---|---|
|`name`|Y||Unique identifier for the target|
|`description`|N||Optional explanation|
|`type`|Y||Target type|
|`status`|N|`true`|Enable/disable the target|
|`batch_size`|N|`1000`|Records per batch|

:::tip
Each target type provides specific configuration options detailed in their respective sections.
:::

Use the `name` of the target to refer to it in your configurations.

**Example**:

```yaml
targets:
  - name: elasticsearch
    type: elasticsearch
    properties:
      hosts: ["http://elasticsearch:9200"]
      index: "logs-%{+yyyy.MM.dd}"
      username: "elastic"
      password: "${PASSWORD}"
```

The target listed here is of type `elasticsearch`. It specifies the _host_ that it will forward the data to and the _index_ on the host to which the data will be appended. To access the host, it specifies a _username_ and a _password_.

:::tip
You can use environment variables like `${PASSWORD}` for your credentials. This will improve security by removing the credentials from your configuration file.
:::

## Debug Options

Targets support debug configuration options for testing, troubleshooting, and development purposes. These options allow you to inspect data flow without affecting production systems.

### Configuration

Debug options are configured under the `debug` property within target properties:

```yaml
targets:
  - name: test_elastic
    type: elastic
    properties:
      index: "test-logs"
      endpoints:
        - endpoint: "http://elasticsearch:9200"
      debug:
        status: true
        dont_send_logs: false
```

### Debug Fields

|Field|Required|Default|Description|
|---|---|---|---|
|`debug.status`|N|`false`|Enable debug logging for the target|
|`debug.dont_send_logs`|N|`false`|Prevent logs from being sent to the actual target|

### Debug Status

When `debug.status` is set to `true`, the target logs each event to the internal debugger before processing. This provides visibility into:

- Message content being sent
- Device information (ID, name, type)
- Target type and operation details
- Timing and sequence of events

Debug logs are written to the system's debug output and can be used to:

- Verify data transformation and formatting
- Troubleshoot pipeline processing issues
- Monitor data flow in development environments
- Audit message content during testing

### Don't Send Logs

When `debug.dont_send_logs` is set to `true`, events are logged to the debugger but **not** sent to the actual target destination. This is useful for:

- **Safe Testing**: Test configuration changes without affecting production systems
- **Development**: Develop and validate pipelines without external dependencies
- **Cost Control**: Avoid charges from cloud services during testing
- **Dry Runs**: Verify event formatting and routing logic before deployment

:::warning
The `dont_send_logs` option only works when `debug.status` is also set to `true`. If debugging is disabled, logs will be sent normally regardless of the `dont_send_logs` setting.
:::

### Use Cases

#### Development Environment

Test your configuration safely without sending data to production targets:

```yaml
targets:
  - name: dev_splunk
    type: splunk
    properties:
      endpoints:
        - endpoint: "https://splunk.example.com:8088/services/collector"
          token: "YOUR-TOKEN"
      index: "main"
      debug:
        status: true
        dont_send_logs: true
```

#### Troubleshooting

Enable debug logging to diagnose issues while still sending data:

```yaml
targets:
  - name: debug_elastic
    type: elastic
    properties:
      index: "production-logs"
      endpoints:
        - endpoint: "http://elasticsearch:9200"
      debug:
        status: true
        dont_send_logs: false
```

#### Pipeline Validation

Verify pipeline transformations before enabling the target:

```yaml
targets:
  - name: validate_transformations
    type: splunk
    properties:
      endpoints:
        - endpoint: "https://splunk.example.com:8088/services/collector"
          token: "YOUR-TOKEN"
      field_format: "cim"
      debug:
        status: true
        dont_send_logs: true
    pipelines:
      - name: test_pipeline
        processors:
          - set:
              field: environment
              value: "development"
```

#### Staged Deployment

Test new target configurations in parallel with existing ones:

```yaml
targets:
  # Production target (normal operation)
  - name: prod_elastic
    type: elastic
    properties:
      index: "production-logs"
      endpoints:
        - endpoint: "http://prod-elasticsearch:9200"
  
  # Test target (debug mode, no actual sending)
  - name: test_elastic
    type: elastic
    properties:
      index: "test-logs"
      endpoints:
        - endpoint: "http://test-elasticsearch:9200"
      debug:
        status: true
        dont_send_logs: true
```

## Deployment

The following deployment types can be used:

* **One-to-many** - data from a single source is routed to one or more destinations:

  > Syslog → Local Storage + Analysis Platform

* **Many-to-one** - data from multiple sources is routed to one destination:

  > Syslog + Windows → Local Storage

* **Many-to-many** - data from multiple sources is routed to multiple destinations:

  > Syslog + Windows → Local Storage
  >
  > Syslog + Elasticsearch → Cloud Upload + Analysis Platform

* **Chained** - data is routed sequentially from one destination to the next:

  > Syslog → Local Storage → Analysis Platform

Multiple targets can be used for redundancy, <Topic id="pipelines-normalization">normalization</Topic> rules can be implemented, and alerts can be put in place for notification and error handling.

## Target Types

Targets are organized by platform and function:

* **Analytics Platforms** - Search, analyze, and visualize data:
  * **ClickHouse**: Columnar database for real-time analytics
  * **Elasticsearch**: Search and analytics engine (includes Elastic Security)
  * **Splunk**: SIEM and observability platform (includes Splunk Security)
  * **Databricks**: Unified analytics platform (via Blob or S3)
  * **Snowflake**: Cloud data warehouse (via Blob or S3)

* **AWS Services** - Amazon Web Services integration:
  * **CloudWatch**, **Kinesis**, **MSK**, **OpenSearch**, **Redshift**, **S3**, **Security Lake**, **SNS**, **SQS**

* **Azure Services** - Microsoft Azure integration:
  * **Blob Storage**, **Data Explorer**, **Event Hubs**, **Monitor**, **Service Bus**, **Microsoft Sentinel**, **Sentinel Data Lake**

* **Google Cloud** - Google Cloud Platform integration:
  * **BigQuery**, **Chronicle**, **Cloud Logging**, **Cloud Pub/Sub**, **Cloud Storage**, **SecOps**

* **IBM Cloud** - IBM Cloud integration:
  * **Cloud Logs**, **Cloud Object Storage**, **Event Streams**

* **Cloud Storage** - S3-compatible object storage:
  * **Alibaba OSS**, **Backblaze B2**, **Cloudflare R2**, **DigitalOcean Spaces**, **MinIO**, **Oracle Cloud OS**, **Scaleway OS**, **Wasabi**

* **Message Queues** - Distributed messaging systems:
  * **Apache Kafka**, **Aiven Kafka**, **Confluent Cloud**, **MQTT**, **NATS**, **RabbitMQ**, **Redpanda**, **Synadia Cloud**, **WarpStream**

* **Standard Outputs** - Basic output destinations:
  * **Console**: Debug output to stdout
  * **Discard**: Drop events (for testing)
  * **File**: Local file storage (JSON, Avro, Parquet)
  * **Syslog**: Forward to syslog servers

## Use Cases

The most common uses of targets are:

* **Local analysis** - Debug logging, performance analysis, audit trails, and temporary storage.

* **Cloud integration** - Long-term storage, data warehousing, security analysis, and compliance monitoring.

* **Real-time analysis** - Live monitoring, alert generation, trend analysis, and performance tracking.

* **Data lake building** - Raw data storage, schema evolution, data partitioning, and analytics preparation.

To serve these ends, the following _processing options_ are available:

* **Pipelines** - Field normalization (for ECS, CIM, ASIM, CEF, LEEF, and CSL), data transformation, message batching, custom field mapping, schema validation, and format conversion.

* **Buffer management** - Configurable buffer sizes, batch processing, flush intervals, queue management, checkpoint recovery, and error handling.

* **Performance** - Asynchronous writing, buffer optimization, connection pooling, retry mechanisms, resource monitoring, and size-based rotation.

* **Security** - Authentication using API keys, service principals, and client certificates. Encryption with TLS/SSL, HTTPS, or custom algorithms. Also, access control and audit logging.

## Implementation Strategies

The following strategies configure target output and storage options.

### Storage Types

#### Local

**Director** supports the following local data output methods:

* **Console** - Direct _stdout_ writing with real-time message viewing. Also provides debugging and testing capability, format normalization, and synchronous writing with mutex locking

* **Files** - Multiple file formats are supported:
  * `json` - Each log entry is written as a separate JSON line (JSONL format)
  * `multijson` - All log entries are written as a single JSON array
  * `avro` - Apache Avro format with schema
  * `parquet` - Apache Parquet columnar format with schema
  
  Compression options like ZSTD, GZIP, Snappy, Brotli, and LZ4 are also supported. Additional features include dynamic file naming, size-based rotation, buffer management, and schema validation.

#### Cloud

The integration options for the cloud are below:

* **Azure Blob** - Direct blob writing and multiple containers are supported. Available authentication methods are service principal and managed Identity. Other features include automatic retries, exponential backoff, size-based chunking, connection pooling, and buffer management.

* **Microsoft Sentinel** - Direct DCR integration and ASIM normalization are supported. In addition to standard tables, _WindowsEvent_, _SecurityEvent_, _CommonSecurityLog_, and _Syslog_ can be used. Various ASIM tables are also available. (See the <Topic id="appendix-asim">ASIM section</Topic> for a complete list.)