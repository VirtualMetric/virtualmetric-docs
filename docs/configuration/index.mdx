---
sidebar_label: Overview
pagination_prev: null
pagination_next: null
---

# Configuration: Overview

In order to create its telemetry pipelines, **DataStream** uses five key components: **Devices**, **Targets**, **Pipelines**, **Processors**, and **Routes**. Configuration involves handling and managing text-based structured files (_YAML_) that specify values for various settings required for running these components.

The various stages where these components are used and how they connect to each other can be described schematically as:

<blockquote style={{textAlign: "center"}}>
**Ingest** [_Source_<sub>1</sub>, _Source_<sub>2</sub>, &#8230;, _Source_<sub>n</sub>]<br/>
&darr;<br/>
**Preprocess** (Normalize) &map; **Route** [_Enrich_ &compfn; _Transform_ &compfn; _Select_] &map; **Postprocess** (Normalize)<br/>
&darr;<br/>
**Forward** [_Destination_<sub>1</sub>, _Destination_<sub>2</sub>, &#8230;, _Destination_<sub>n</sub>]
</blockquote>

* To ingest data from _Sources_ and to communicate with them, **DataStream** uses **Devices** which are listeners. Each dedicated device type is conformant with the layout of a specific log data generator.

* For the **Preprocessing**, **Routing**, and **Postprocessing** stages, **DataStream** uses **Pipelines** which are sequential arrangements of **Processors**, i.e. functions that handle and transform data stored in various fields for a wide variety of purposes. (They also help normalize data for the purposes of either transformation or enrichment as well as storage.)

* To forward processed data to _Destinations_ and to communicate with them, **DataStream** uses **Targets** which are senders. Each dedicated target type is conformant with the layout of a specific log data receiver.

## Managing YAML Files

To help design the logic of the components that will define and run the processes of the above-mentioned stages, **DataStream** utilizes YAML-based configuration files. These can be found in some of the folders under its root directory, here indicated with `<vm_root>`:

```powershell
<vm_root>
│   vmetric.yml
│
├───config
│   ├───devices
│   │       estreamer.yml
│   │       syslog.yml
│   │       tcp.yml
│   │
│   ├───routes
│   │       default.yml
│   │
│   └───targets
│           console.yml
│           file.yml
│
├───package
│   └───definitions
│       └───pipelines
│               checkpoint.yml
│               cisco.yml
│               normalize.yml
│
└───user
    └───definitions
        └───pipelines
                checkpoint.yml
                normalize.yml
```

All tasks are carried out using these configuration files.

By default, these files are located based on their component types. They contain predefined fields that the components recognize, and **Director** uses the settings in these fields to spawn and run its processes.

* Two directories are of importance: `package` and `user`.

  The `package` directory contains templates and ready-to-use definitions. These definitions are updated with newer versions of **Director**.

  :::warning
  Never modify the definition files under `package` directly. To create a configuration using one of these as a template, copy the relevant file to the corresponding location under `user` first, and then edit it.
  :::

  The `user` directory contains custom configurations. These definitions take precedence over those under the `package` directory. The definitions here are preserved between updates.

* The configurations may be placed in separate files or they may be grouped together logically, i.e. based on their intended purpose of use or the type of data streams they process. By default, these files reside in the directories under the `config` folder.

  :::tip
  You can place your files anywhere you wish under the `config` directory. **Director** discovers all of them by traversing the folders recursively.
  :::

  To illustrate, a target configuration file can be named as, e.g.

  <Tabs>
    <TabItem value="windows" label="Windows" default>
      > `C:\<vm_root>\config\target.yml`

      -or-

      > `C:\<vm_root>\config\targets\outputs.yml`

      -or-

      > `C:\<vm_root>\config\targets\outputs\sentinel.yml`

    </TabItem>
    <TabItem value="linux" label="Linux" default>
      > `/usr/bin/<vm_root>/config/target.yml`

      -or-

      > `/usr/bin/<vm_root>/config/targets/outputs.yml`

      -or-

      > `/usr/bin/<vm_root>/config/targets/outputs/sentinel.yml`

    </TabItem>
    <TabItem value="macos" label="macOS" default>
      > `/Applications/<vm_root>/config/target.yml`

      -or-

      > `/Applications/<vm_root>/config/targets/outputs.yml`

      -or-

      > `/Applications/<vm_root>/config/targets/outputs/sentinel.yml`

    </TabItem>
  </Tabs>

  As the nesting level increases, file names become more specific, offering additional context for classification.

  :::note
  It is also possible to use _standalone_ files placed in arbitrary directories. However, the full paths of these files must be supplied to **Director**.
  :::

  Select the organizational style that best suits your needs.

## General Format

All components follow a consistent YAML structure that emphasizes readability and maintainability:

:::warning[attention]
Configurations must conform to [**these syntactic rules**](../appendix.mdx#configuration-bnf).

The **Overview** sections of the component types summarize their common fields under the **Configuration** heading. 

The **Schema** paragraphs of specific component sections provide their fields.
:::

### Properties

All components have properties&mdash;i.e. YAML fields with parameters&mdash;that define their specific behavior. Some of these are mandatory and must be present in every component.

A few are worth mentioning since they are frequently used:

* **Identification** - Every component requires unique identification:

  ```yaml
  - id: 1                   # Numeric identifier
    name: example_component # Human-readable name
  ```

* **Status Control** - Components can be enabled or disabled:

  ```yaml
  - id: 1
    name: example_component
    status: true  # Enables this component
  ```

* **Environment Variables** - Sensitive information can be stored in system variables:

  ```yaml
  properties:
    username: admin
    password: ${SECRET_PASSWORD}  # References environment variable
  ```

* **Tagging** - Components can be tagged for better organization, and&mdash;optionally&mdash;a description can be used to document its purpose:

  ```yaml
  - id: 1
    name: example_component
    description: "Optional detailed explanation" # Documentation
    tags:
      - production  # For quick search
      - security
  ```

### Data Flow

The **DataStream** system implements a modular architecture of components working together to create complete data flows. The following example illustrates a security monitoring flow:

**Example**: Assume we have a route named `critical_security` defined like so:

```yaml
routes:
  - name: critical_security
    devices:
      - name: firewall_logs
    if: "event.severity == 'critical'"
    pipelines:
      - name: security_enrichment
    targets:
      - name: security_elasticsearch
      - name: security_team_notification
```

This route refers to:

- a device named `firewall_logs`:

    ```yaml
    devices:
      - id: 1
        name: firewall_logs
        type: syslog
        properties:
          port: 514
    ```

- a pipeline named `security_enrichment`:

    ```yaml
    pipelines:
      - name: security_enrichment
        processors:
          - grok:
            field: message
            patterns:
              - "%{CISCOFW106001}"
          - set:
            field: event.category
            value: security
          - geoip:
            field: source.ip
            target_field: source.geo
    ```

- and two targets named `security_elasticsearch` and `security_team_notification` respectively:

    ```yaml
    targets:
      - name: security_elasticsearch
        type: elasticsearch
        properties:
          url: "https://es.example.com:9200"
          index: "security-%{+yyyy.MM.dd}"
      - name: security_team_notification
        type: webhook
        properties:
          url: "https://alerts.example.com/security"
          method: POST
    ```

This configuration is intended to do the following:

* the device collects firewall logs from _Syslog_ that have _critical_ status

* the pipeline selects (via the `grok` processor) _Cisco_ events, categorizes them as security events, and enriches them by adding their _source IP_ and _geographic location_

* the targets forward the events curated to _Elasticsearch_ and a notification system for a security team

### Validation

Before deploying your configuration, check it using the available tools. The validator checks for syntactic correctness, required field presence, reference integrity, and logical consistency.

- Check the configuration file

  <Tabs>
    <TabItem value="windows" label="Windows" default>
      ```powershell
      vmetric -validate -c critical_security.yml
      ```
    </TabItem>
    <TabItem value="linux" label="Linux" default>
      ```bash
      vmetric -validate -c critical_security.yml
      ```
    </TabItem>
    <TabItem value="macos" label="macOS" default>
      ```bash
      vmetric -validate -c critical_security.yml
      ```
    </TabItem>
  </Tabs>

- Test configuration with sample data

  <Tabs>
    <TabItem value="windows" label="Windows" default>
      ```powershell
      vmetric -test -c critical_security.yml -i sample-data.json
      ```
    </TabItem>
    <TabItem value="linux" label="Linux" default>
      ```bash
      vmetric -test -c critical_security.yml -i sample-data.json
      ```
    </TabItem>
    <TabItem value="macos" label="macOS" default>
      ```bash
      vmetric -test -c critical_security.yml -i sample-data.json
      ```
    </TabItem>
  </Tabs>

## Best Practices

To maintain system integrity, robustness, and health, follow these key guidelines:

### Design

The following factors and recommendations should be considered in the design of configurations:

|Factor|Recommendations|
|--:|:--|
|Modularization|<ul><li>Use multiple _smaller pipelines_ instead of monolithic ones</li><li>Create _focused devices_ for specific data sources</li><li>Implement targeted routes for distinct data types</li></ul>|
|Maintainability|<ul><li>Add meaningful descriptions to all components</li><li>Use consistent naming conventions</li><li>Document complex logic with comments</li></ul>|
|Reliability|<ul><li>Implement _redundant targets_ for critical data</li><li>Configure _error handling_ in pipelines</li><li>Use _disk buffering_ for unreliable network conditions</li></ul>|
|Security|<ul><li>Store credentials in _environment variables_</li><li>Use appropriate authentication for all components</li><li>Implement _TLS_ for network communications</li></ul>|
|Performance|<ul><li>Configure appropriate _batch sizes_ for high-volume flows</li><li>Use _conditionals_ to process only necessary data</li><li>Monitor resource usage across components</li><li>Optimize processing times for better efficiency</li></ul>|

### Administration

This covers creating, modifying, managing, and maintaining the configuration files. The following table summarizes common tasks and suitable approaches to carry them out:

| Task | Approach |
|--:|:--|
| Add a new data source | Create a new device with appropriate type and properties |
| Customize data processing | Develop or modify pipelines with relevant processors |
| Change data destination | Update route targets or create new target definitions |
| Filter specific events | Add or modify route conditions |
| Secure sensitive data | Use environment variables for credentials and keys |
| Structure complex configurations | Split into multiple files with logical organization |

:::tip
Refer to specific component documentation for detailed options and best practices.
:::
