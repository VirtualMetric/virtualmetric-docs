---
pagination_prev: null
sidebar_label: Overview
---

import useBaseUrl from '@docusaurus/useBaseUrl';

# Configuration: Overview

In order to create its telemetry pipelines, **DataStream** uses five key components: **Devices**, **Targets**, **Pipelines**, **Processors**, and **Routes**. Configuration involves handling and managing text-based structured files (YAML) that specify values for various settings required for running these components.

The various stages where these components are used and how they connect to each other can be described schematically as:

<blockquote style={{textAlign: "center"}}>
**Ingest** [_Source_<sub>1</sub>, _Source_<sub>2</sub>, &#8230;, _Source_<sub>n</sub>]<br/>
&darr;<br/>
**Preprocess** (Normalize) &map; **Route** [_Enrich_ &compfn; _Transform_ &compfn; _Select_] &map; **Postprocess** (Normalize)<br/>
&darr;<br/>
**Forward** [_Destination_<sub>1</sub>, _Destination_<sub>2</sub>, &#8230;, _Destination_<sub>n</sub>]
</blockquote>

Schematically:

<img src={useBaseUrl('/img/graphs/data-flow.svg')} alt="Data Flow" height="300"/>

* To ingest data from _Sources_ and to communicate with them, **DataStream** uses **Devices** which are listeners. Each dedicated device type is conformant with the layout of a specific log data generator.

* For the **Preprocessing**, **Routing**, and **Postprocessing** stages, **DataStream** uses **Pipelines** which are sequential arrangements of **Processors**, i.e. functions that handle and transform data stored in various fields for a wide variety of purposes. (They also help normalize data for the purposes of either transformation or enrichment as well as storage.)

* To forward processed data to _Destinations_ and to communicate with them, **DataStream** uses **Targets** which are senders. Each dedicated target type is conformant with the layout of a specific log data receiver.

By using these dedicated components, you can design powerful and efficient telemetry systems very elegantly. You only need to understand how they work, how they interact, and how they can be configured and combined to achieve your objectives.

## Directory Tree

**VirtualMetric**'s installation folder contains the following directory tree:

<img src={useBaseUrl('/img/graphs/directory-tree.svg')} alt="Directory Tree" height="300"/>

All configuration files needed to define and run telemetry streams can be found&mdash;and are placed in&mdash;specific folders under this tree.

## YAML Files

The most fundamental tools you use to design your telemetry streams are YAML files.

To implement the logic of the components that will define and run the processes of the above-mentioned stages, **DataStream** uses YAML-based configuration files.

These can be found in the folders under `config`. These are

- `devices`
- `routes`
- `targets`

All management tasks related to running the telemetry operation are carried out using the files under these directories.

By default, the files are placed in these folders according to their component types. They contain predefined fields that the components recognize, and **DataStream** uses the setting values in these fields to spawn and run its processes.

* Two directories are of importance: `package` and `user`.

  The `package` directory contains templates and ready-to-use definitions. These definitions are updated with newer versions of **Director**.

  :::warning
  Never modify the definition files under `package` directly. To create a configuration using one of these as a template, copy the relevant file to the corresponding location under `user` first, and then edit it.
  :::

  The `user` directory contains custom configurations. These definitions take precedence over those under the `package` directory. The definitions here are preserved between updates.

* The configurations may be placed in separate files or they may be grouped together logically, i.e. based on their intended purpose of use or the type of data streams they process. By default, these files reside in the directories under the `config` folder.

  :::tip
  You can place your files anywhere you wish under the `config` directory. **Director** discovers all of them by traversing the folders recursively.
  :::

  To illustrate, a target configuration file can be named as, e.g.

  <Tabs>
    <TabItem value="powershell" label="PowerShell" default>
      ```powershell
      <vm_root>\config\target.yml
      ```

      -or-

      ```powershell
      <vm_root>\config\targets\outputs.yml
      ```

      -or-

      ```powershell
      <vm_root>\config\targets\outputs\sentinel.yml
      ```
    </TabItem>
    <TabItem value="bash" label="Bash">
      ```bash
      <vm_root>/config/target.yml
      ```

      -or-

      ```bash
      <vm_root>/config/targets/outputs.yml
      ```

      -or-

      ```bash
      <vm_root>/config/targets/outputs/sentinel.yml
      ```
    </TabItem>
  </Tabs>

  As the nesting level increases, the file names become more specific, offering additional context for classification.

  Select the organizational style that best suits your needs.

## General Format

All components follow a consistent YAML structure that emphasizes readability and maintainability:

:::info
Configurations must conform to [**these syntactic rules**](../appendix.mdx#configuration-bnf).
:::

:::tip
The **Overview** sections of the component types summarize their common fields under the **Configuration** heading. 

The **Schema** paragraphs of specific component sections provide their fields.
:::

All components have properties&mdash;i.e. YAML fields with parameters&mdash;that define their specific behavior. Some of these are mandatory and must be present in every component.

### Commonly Used Fields

A few are worth mentioning since they are frequently used:

- **Identification** - Every component requires a unique identification:

    ```yaml
    - id: 1
      name: example_component
    ```

  Here, `1` is the numeric identifier used internally, whereas `example_component` is the human-readable name.

- **Status Control** - Components can be enabled or disabled using the `status` field:

    ```yaml
    - id: 1
      name: component
      status: true
    ```

- **Environment Variables** - Sensitive information can be stored in system variables:

    ```yaml
    properties:
      username: admin
      password: ${PASSWORD}
    ```

- **Tagging** - Optionally, components can have descriptions to document its purpose, and they can be tagged for better organization and facilitating searching:

    ```yaml
    - id: 1
      name: component
      description: "Optional detailed explanation"
      tags:
        - production
        - security
    ```

### Scoping and Referencing

Field names within the same YAML file are all part of the same scope, and they can be referred to from other fields. This can be done as in two ways:

- **Directly** - with plain syntax:

    ```yaml
    processors:
      - json:
          field: user_name
          ...
      - set:
          field: display
          value: user_name
    ```

- **Indirectly** -

  - with the so-called _mustache_ syntax:

    ```yaml
    processors:
      - json:
          field: user_name
          ...
      - set:
          field: display
          value: "Sent by {{user_name}}"
    ```

    :::caution
    The double mustache operators do _not_ escape HTML entities. A third pair have to be added for that purpose.

    Assume, for example, that we have field a named `menu` which contains the text &quot;**Users > Workspace &rarr; Book&copy;**&quot; in encoded form. In that case, `{{menu}}` returns &quot;**Users > Workspace &rarr; Book&copy;**&quot; whereas `{{{menu}}}` returns &quot;**Users \&gt; Workspace \&rarr; Book\&copy;**&quot;.

    This may be relevant due to security considerations.
    :::

  - with the so-called _dot_ notation:

      ```yaml
      processors:
        - json:
            field: user
            ...
        - set:
            field: display
            value: user.name
      ```

### Meta Fields

**DataStream** uses the following meta fields to carry out some of its pipeline marshalling operations in the background.

#### `_ingest` Field

The `_ingest` field serves as a temporary internal namespace for data processing operations within the pipeline. It contains ephemeral metadata that facilitates various processor operations and is typically cleaned up after processing is complete.

**Structure and Subfields:**

- **`_ingest._key`** - Contains the current iteration key when processing arrays or maps in the `foreach` processor. This field is dynamically populated during iteration and represents either the array index (as integer) or the map key (as string).

- **`_ingest._value`** - Holds the current iteration value when processing collections in the `foreach` processor. This field contains the actual data being processed for each iteration step.

- **`_ingest._message`** - Stores the complete JSON representation of the log entry during enrichment operations. This field is used by the `enrich` processor to create temporary database tables and perform SQL-based data enrichment queries.

**Lifecycle and Nullability:**

The `_ingest` field and its subfields are temporary by design. They are created during specific processor operations and automatically cleaned up afterward. The nullability patterns follow these principles:

- Fields may be `null` or undefined (`?`) when not actively being used by a processor
- `_ingest._key` and `_ingest._value` are only populated during `foreach` operations
- `_ingest._message` is only populated during `enrich` operations
- The entire `_ingest` object is deleted after processing completes

**Configuration Context:**

The `_ingest` field also plays a crucial role in configuration evaluation and definition processing. During device configuration processing, the field is populated with definition and input metadata to enable conditional evaluation using expression language conditions. This allows for dynamic configuration filtering based on device properties and current processing context.

**Error Handling Context:**

When processor failures occur, the `_ingest` namespace is also used to capture error metadata:

- `_ingest.on_failure_pipeline` - Name of the pipeline where the error occurred
- `_ingest.on_failure_processor_type` - Type of processor that failed
- `_ingest.on_failure_processor_tag` - Tag of the specific processor instance
- `_ingest.on_failure_message` - Detailed error message

#### `_vmetric` Field

The `_vmetric` field appears to be referenced in the system architecture but is not directly visible in the current processor implementations. Based on the codebase patterns, this field likely serves as a system-level metadata container for VirtualMetric-specific operational data.

**Expected Structure:**

The `_vmetric` field serves as a system-level metadata container that provides context about the VirtualMetric processing environment. Based on the codebase architecture, this field contains:

- Device identification and metadata (ID, name, type, tags)
- Processing pipeline information and configuration
- System service context and ingestion channel details
- Cross-component communication metadata

**Nullability and Usage:**

The `_vmetric` field follows similar nullability patterns to `_ingest`, where subfields may be undefined (`?`) when not actively populated by system operations. Unlike the ephemeral `_ingest` data, this field typically persists throughout the processing lifecycle to provide consistent system context.

**Integration with Processing Pipeline:**

Both meta fields integrate seamlessly with the processing pipeline's field manipulation system, supporting dot notation access patterns and dynamic field creation/deletion operations.

## Data Flow

**DataStream** implements a modular architecture of components working together to create complete data flows. The following example illustrates a security monitoring flow:

**Example**: Assume we have a route named `critical_security` defined like so:

```yaml
routes:
  - name: critical_security
    devices:
      - name: firewall_logs
    if: "event.severity == 'critical'"
    pipelines:
      - name: security_enrichment
    targets:
      - name: security_elasticsearch
      - name: security_team_notification
```

This route refers to:

- a device named `firewall_logs`:

    ```yaml
    devices:
      - id: 1
        name: firewall_logs
        type: syslog
        properties:
          port: 514
    ```

- a pipeline named `security_enrichment`:

    ```yaml
    pipelines:
      - name: security_enrichment
        processors:
          - grok:
              field: message
              patterns:
                - "%{CISCOFW106001}"
          - set:
              field: event.category
              value: security
          - geoip:
              field: source.ip
              target_field: source.geo
    ```

- two targets named `security_elasticsearch` and `security_team_notification` respectively:

    ```yaml
    targets:
      - name: security_elasticsearch
        type: elasticsearch
        properties:
          url: "https://es.example.com:9200"
          index: "security-%{+yyyy.MM.dd}"
      - name: security_team_notification
        type: webhook
        properties:
          url: "https://alerts.example.com/security"
          method: POST
    ```

This configuration is intended to implement the following:

* the device collects firewall logs from _Syslog_ that have _critical_ status

* the pipeline selects (via the `grok` processor) _Cisco_ events, categorizes them as security events, and enriches them by adding their _source IP_ and _geographic location_

* the targets forward the events curated to _Elasticsearch_ and a notification system for a security team

:::tip
Refer to component-specific documentation for details of the available options.
:::
