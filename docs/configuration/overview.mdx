---
pagination_prev: null
sidebar_label: Overview
---

import useBaseUrl from '@docusaurus/useBaseUrl';

# Configuration: Overview

In order to create its telemetry pipelines, **DataStream** uses five key components: **Devices**, **Targets**, **Pipelines**, **Processors**, and **Routes**. Configuration involves handling and managing text-based structured files (YAML) that specify values for various settings required for running these components.

The various stages where these components are used and how they connect to each other can be described schematically as:

<blockquote style={{textAlign: "center"}}>
**Ingest** [_Source_<sub>1</sub>, _Source_<sub>2</sub>, &#8230;, _Source_<sub>n</sub>]<br/>
&darr;<br/>
**Preprocess** (Normalize) &map; **Route** [_Enrich_ &compfn; _Transform_ &compfn; _Select_] &map; **Postprocess** (Normalize)<br/>
&darr;<br/>
**Forward** [_Destination_<sub>1</sub>, _Destination_<sub>2</sub>, &#8230;, _Destination_<sub>n</sub>]
</blockquote>

Schematically:

<img src={useBaseUrl('/img/graphs/data-flow.svg')} alt="Data Flow" height="300"/>

* To ingest data from _Sources_ and to communicate with them, **DataStream** uses **Devices** which are listeners. Each dedicated device type is conformant with the layout of a specific log data generator.

* For the **Preprocessing**, **Routing**, and **Postprocessing** stages, **DataStream** uses **Pipelines** which are sequential arrangements of **Processors**, i.e. functions that handle and transform data stored in various fields for a wide variety of purposes. (They also help normalize data for the purposes of either transformation or enrichment as well as storage.)

* To forward processed data to _Destinations_ and to communicate with them, **DataStream** uses **Targets** which are senders. Each dedicated target type is conformant with the layout of a specific log data receiver.

By using these dedicated components, you can design powerful and efficient telemetry systems very elegantly. You only need to understand how they work, how they interact, and how they can be configured and combined to achieve your objectives.

## Directory Tree

**VirtualMetric**'s installation folder contains the following directory tree:

<img src={useBaseUrl('/img/graphs/directory-tree.svg')} alt="Directory Tree" height="300"/>

All configuration files needed to define and run telemetry streams can be found&mdash;and are placed in&mdash;specific folders under this tree.

## YAML Files

The most fundamental tools you use to design your telemetry streams are YAML files.

To implement the logic of the components that will define and run the processes of the above-mentioned stages, **DataStream** uses YAML-based configuration files.

These can be found in the folders under `config`. These are

- `devices`
- `routes`
- `targets`

All management tasks related to running the telemetry operation are carried out using the files under these directories.

By default, the files are placed in these folders according to their component types. They contain predefined fields that the components recognize, and **DataStream** uses the setting values in these fields to spawn and run its processes.

* Two directories are of importance: `package` and `user`.

  The `package` directory contains templates and ready-to-use definitions. These definitions are updated with newer versions of **Director**.

  :::warning
  Never modify the definition files under `package` directly. To create a configuration using one of these as a template, copy the relevant file to the corresponding location under `user` first, and then edit it.
  :::

  The `user` directory contains custom configurations. These definitions take precedence over those under the `package` directory. The definitions here are preserved between updates.

* The configurations may be placed in separate files or they may be grouped together logically, i.e. based on their intended purpose of use or the type of data streams they process. By default, these files reside in the directories under the `config` folder.

  :::tip
  You can place your files anywhere you wish under the `config` directory. **Director** discovers all of them by traversing the folders recursively.
  :::

  To illustrate, a target configuration file can be named as, e.g.

  <Tabs>
    <TabItem value="powershell" label="PowerShell" default>
      ```powershell
      <vm_root>\config\target.yml
      ```

      -or-

      ```powershell
      <vm_root>\config\targets\outputs.yml
      ```

      -or-

      ```powershell
      <vm_root>\config\targets\outputs\sentinel.yml
      ```
    </TabItem>
    <TabItem value="bash" label="Bash">
      ```bash
      <vm_root>/config/target.yml
      ```

      -or-

      ```bash
      <vm_root>/config/targets/outputs.yml
      ```

      -or-

      ```bash
      <vm_root>/config/targets/outputs/sentinel.yml
      ```
    </TabItem>
  </Tabs>

  As the nesting level increases, the file names become more specific, offering additional context for classification.

  Select the organizational style that best suits your needs.

## General Format

All components follow a consistent YAML structure that emphasizes readability and maintainability:

:::caution
Configurations must conform to [**these syntactic rules**](../appendix.mdx#configuration-bnf).

The **Overview** sections of the component types summarize their common fields under the **Configuration** heading. 

The **Schema** paragraphs of specific component sections provide their fields.
:::

All components have properties&mdash;i.e. YAML fields with parameters&mdash;that define their specific behavior. Some of these are mandatory and must be present in every component.

### Commonly Used Fields

A few are worth mentioning since they are frequently used:

- **Identification** - Every component requires a unique identification:

    ```yaml
    - id: 1
      name: example_component
    ```

  Here, `1` is the numeric identifier used internally, whereas `example_component` is the human-readable name.

- **Status Control** - Components can be enabled or disabled using the `status` field:

    ```yaml
    - id: 1
      name: example_component
      status: true
    ```

- **Environment Variables** - Sensitive information can be stored in system variables:

    ```yaml
    properties:
      username: admin
      password: ${SECRET_PASSWORD}
    ```

- **Tagging** - Optionally, components can have descriptions to document its purpose, and they can be tagged for better organization and facilitating searching:

    ```yaml
    - id: 1
      name: example_component
      description: "Optional detailed explanation"
      tags:
        - production
        - security
    ```

### Scoping and Referencing

Field names within the same YAML file are all part of the same scope, and they can be referred to from other fields. This can be done as in two ways:

- **Directly** - with plain syntax:

    ```yaml
    processors:
      - json:
          field: user_name
          ...
      - set:
          field: display
          value: user_name
    ```

- **Indirectly** -

  - with the so-called _mustache_ syntax:

    ```yaml
    processors:
      - json:
          field: user_name
          ...
      - set:
          field: display
          value: "Sent by {{user_name}}"
    ```

    :::caution
    The double mustache operators do _not_ escape HTML entities; a third pair have to be added for that purpose.

    Assume, for example, that we have field a named `menu` which contains the encoded text

    > **Users > Workspace &rarr; Book&copy;**

    In that case:

    - `{{menu}}` returns **Users > Workspace &rarr; Book&copy;**
    - `{{{menu}}}` returns **Users \&gt; Workspace \&rarr; Book\&copy;**

    :::

  - with the so-called _dot_ notation:

      ```yaml
      processors:
        - json:
            field: user
            ...
        - set:
            field: display
            value: user.name
      ```

### Meta Fields

TODO: Mention these fields:

- `_ingest` and its subfields `_key`, `_value`, and `_message`
- `_vmetric`

TODO: Mention nullability

- `_vmetric?.pipeline`


## Data Flow

**DataStream** implements a modular architecture of components working together to create complete data flows. The following example illustrates a security monitoring flow:

**Example**: Assume we have a route named `critical_security` defined like so:

```yaml
routes:
  - name: critical_security
    devices:
      - name: firewall_logs
    if: "event.severity == 'critical'"
    pipelines:
      - name: security_enrichment
    targets:
      - name: security_elasticsearch
      - name: security_team_notification
```

This route refers to:

- a device named `firewall_logs`:

    ```yaml
    devices:
      - id: 1
        name: firewall_logs
        type: syslog
        properties:
          port: 514
    ```

- a pipeline named `security_enrichment`:

    ```yaml
    pipelines:
      - name: security_enrichment
        processors:
          - grok:
              field: message
              patterns:
                - "%{CISCOFW106001}"
          - set:
              field: event.category
              value: security
          - geoip:
              field: source.ip
              target_field: source.geo
    ```

- two targets named `security_elasticsearch` and `security_team_notification` respectively:

    ```yaml
    targets:
      - name: security_elasticsearch
        type: elasticsearch
        properties:
          url: "https://es.example.com:9200"
          index: "security-%{+yyyy.MM.dd}"
      - name: security_team_notification
        type: webhook
        properties:
          url: "https://alerts.example.com/security"
          method: POST
    ```

This configuration is intended to implement the following:

* the device collects firewall logs from _Syslog_ that have _critical_ status

* the pipeline selects (via the `grok` processor) _Cisco_ events, categorizes them as security events, and enriches them by adding their _source IP_ and _geographic location_

* the targets forward the events curated to _Elasticsearch_ and a notification system for a security team

:::tip
Refer to component-specific documentation for details of the available options.
:::

## Validation

Before deploying your configuration, check it using the available tools. The validator checks for syntactic correctness, required field presence, reference integrity, and logical consistency.

- Check the configuration file

  <Tabs>
    <TabItem value="powershell" label="PowerShell" default>
      ```powershell
      vmetric -validate -c critical_security.yml
      ```
    </TabItem>
    <TabItem value="bash" label="Bash">
      ```bash
      vmetric -validate -c critical_security.yml
      ```
    </TabItem>
  </Tabs>

- Test configuration with sample data

  <Tabs>
    <TabItem value="powershell" label="PowerShell" default>
      ```powershell
      vmetric -test -c critical_security.yml -i sample-data.json
      ```
    </TabItem>
    <TabItem value="bash" label="Bash">
      ```bash
      vmetric -test -c critical_security.yml -i sample-data.json
      ```
    </TabItem>
  </Tabs>
