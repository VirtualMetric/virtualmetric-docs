---
sidebar_label: Features
---

# Features

VirtualMetric DataStream offers a range of features that make it a powerful and flexible telemetry pipeline solution for Microsoft Sentinel and other Azure services.
Enterprises can leverage these features to streamline their data collection, processing, and routing operations:

## Agentless Data Collection ##

DataStream's agentless design enables data collection from Windows, Linux, Unix, macOS, Solaris, AIX, and many others without requiring third-party tools or complicated configurations. This approach significantly reduces operational overhead and eliminates common deployment challenges associated with traditional agent-based solutions.

The system operates through secure, read-only connections to target systems using standard system protocols. For Windows environments, it leverages Windows Management Instrumentation (WMI) and Windows Remote Management (WinRM). On Unix-based systems including Linux, macOS, Solaris, and AIX, it utilizes SSH and native logging facilities to collect data securely.

The agentless approach also ensures that DataStream can begin collecting data immediately after configuration, without requiring system restarts or extensive change management processes. This makes it particularly valuable for large-scale enterprise deployments where deploying and maintaining agents across thousands of systems would be impractical.

```mermaid
graph LR
    DS([DataStream System])
    CS[Credential Store]
    AD[Active Directory]
    
    subgraph Target Systems
        W[Windows]
        L[Linux]
        M[macOS]
        S[Solaris]
        A[AIX]
    end

    CS <-.-> |Secure Credentials| DS
    AD <-.-> |Service Accounts| DS
    
    DS <-.-> |Read-only Access| W
    DS <-.-> |Read-only Access| L
    DS <-.-> |Read-only Access| M
    DS <-.-> |Read-only Access| S
    DS <-.-> |Read-only Access| A

    style DS fill:#87CEEB
    style CS fill:#98FB98
    style AD fill:#98FB98
    style W fill:#FFE4B5
    style L fill:#FFE4B5
    style M fill:#FFE4B5
    style S fill:#FFE4B5
    style A fill:#FFE4B5
```

The system leverages read-only user rights for secure remote access, ensuring data integrity and compliance. By integrating with _Credential Stores_ and _Active Directory Service Accounts_, it eliminates the need for user credentials, simplifying creation of secure connections.

Key benefits of this agentless architecture include:

* Zero deployment overhead - no software installation required on target systems
* Minimal system footprint - uses native protocols and interfaces
* Reduced attack surface - operates with read-only permissions
* Simplified maintenance - no agent updates or patches to manage
* Enterprise-grade security - leverages existing authentication infrastructure
* Cross-platform compatibility - works consistently across different operating systems

## Vectorized Processing Architecture ##

**DataStream**'s pipeline engine employs a sophisticated vectorized processing architecture that maximizes system resources by utilizing all available CPU cores. This design enables efficient processing of large log volumes and facilitates parallel data ingestion with multiple target SIEMs simultaneously.

The vectorized architecture breaks down incoming log streams into optimized chunks that can be processed independently across multiple cores. This parallelization ensures that system resources are used efficiently, preventing bottlenecks that commonly occur in single-threaded processing systems.

Each processing core operates independently on its assigned data chunk, performing tasks such as parsing, filtering, and enrichment. This parallel processing approach significantly reduces the overall processing time and enables real-time data handling even under heavy loads.

```mermaid
graph TD
    subgraph Cores[Multiple CPU Cores]
        C1(Core 1)
        C2(Core 2)
        C3(Core 3)
        C4(Core N)
    end
    
    Data[Log Data] -.-> Pipeline([Pipeline Engine])
    Pipeline -.-> Cores
    
    Cores -.-> |Parallel Ingestion| SIEM1[SIEM Target 1]
    Cores -.-> |Parallel Ingestion| SIEM2[SIEM Target 2]
    
    style Pipeline fill:#87CEEB
```

With over _10 times_ the ingestion speed of traditional solutions, **DataStream** reduces bandwidth usage down to the bare minimum, delivering significant cost savings. The high-performance architecture ensures that data is processed and delivered to target systems with minimal latency.

Key advantages of the vectorized processing architecture include:

* Maximum resource utilization - efficiently uses all available CPU cores
* Parallel data processing - handles multiple data streams simultaneously
* High-throughput ingestion - processes large volumes of logs in real-time
* Scalable performance - processing capacity scales with available cores
* Low latency - minimizes delay between data collection and delivery
* Resource optimization - intelligent workload distribution across cores
* Multi-target support - concurrent data delivery to multiple SIEM platforms

The vectorized processing architecture also includes built-in load balancing capabilities, automatically distributing workloads across available cores based on system metrics and processing demands. This ensures optimal performance even as data volumes fluctuate throughout the day.

## Lossless Pipeline Engine ##

Our _Write-Ahead Log_ (WAL) architecture provides a robust foundation for data integrity by securely storing all routing and pipeline states on disk. This architecture ensures that every piece of data is safely persisted before processing, creating a reliable recovery point in case of system failures or unexpected shutdowns.

The WAL implementation in DataStream operates as a high-performance buffer between data collection and processing stages. When data arrives, it is immediately written to the WAL before any processing begins, creating a durable record of all incoming information. This approach guarantees that no data is lost during pipeline processing or system transitions.

```mermaid
flowchart LR
    Input[Input Data] --> WAL[Write-Ahead Log]
    WAL --> Disk[(Disk Storage)]
    WAL --> Pipeline([Pipeline Processing])
    Pipeline --> Output[Output Data]
    
    subgraph Recovery
        Disk --> |Crash Restore| Pipeline
    end
    
    style WAL fill:#98FB98
    style Disk fill:#FFE4B5
```

Unlike solutions that require additional components like Kafka, **DataStream** caps log duplication at just one message. This ensures zero data loss, even in the event of a crash, while maintaining efficient storage utilization. The system achieves this through a sophisticated checkpoint mechanism that tracks the processing state of each message.

The _WAL_ approach also minimizes the risk of system downtime, ensuring that your telemetry pipeline is always up and running, and consistent, even under heavy loads. When the system restarts after an interruption, it can seamlessly resume processing from the last recorded state without data loss or duplication.

Key features of the Lossless Pipeline Engine include:

* Zero data loss guarantee - all data is persisted before processing
* Minimal message duplication - maximum of one copy per message
* Automatic crash recovery - seamless resumption after system interruptions
* State preservation - maintains pipeline and routing states on disk
* High throughput - efficient write-ahead logging with minimal overhead
* System consistency - ensures data integrity across pipeline stages
* Resource efficiency - optimized storage utilization without external dependencies

The lossless architecture is particularly valuable in environments where data completeness is critical, such as security monitoring, compliance reporting, and audit trails. By ensuring that no data is lost during collection and processing, DataStream provides a reliable foundation for these essential business functions.

## Datasets and RBAC Structure ##

We are revolutionizing the traditional concept of _source_ in telemetry pipelines with our introduction of _Datasets_. This innovative approach transforms how telemetry data is organized, accessed, and secured throughout the entire pipeline process.

DataStream's Dataset architecture provides a sophisticated categorization system that organizes telemetry data at the source level. This categorization happens before data enters the processing pipeline, enabling granular control over data access and processing paths. The system automatically classifies incoming data into logical groups such as Security Events, Audit Events, System Events, and Application Events.

```mermaid
flowchart TD
    subgraph DataSources[Data Sources]
        WE[Windows Events]
        WU[Windows User Activity]
        LA[Linux Audit Logs]
        SS[System Services]
    end

    subgraph Datasets[Dataset Categories]
        direction LR
        SEC[Security Events]
        AUD[Audit Events]
        SYS[System Events]
        APP[Application Events]
    end

    subgraph Access[RBAC Access Control]
        direction LR
        SOC[SOC Team]
        AUD_T[Audit Team]
        OPS[Operations Team]
        DEV[Dev Team]
    end

    WE --> SEC & SYS
    WU --> AUD
    LA --> SEC & AUD
    SS --> APP & SYS

    SEC --> SOC
    AUD --> AUD_T
    SYS --> OPS
    APP --> DEV

    subgraph Pipeline[Pipeline Processing]
        direction LR
        F[Filters]
        E[Enrichment]
        R[Routing]
    end

    SOC & AUD_T & OPS & DEV --> Pipeline
    Pipeline --> SIEM[SIEM Platform]

    style DataSources fill:#FFE4B5
    style Datasets fill:#98FB98
    style Access fill:#87CEEB
    style Pipeline fill:#98FB98
    style SIEM fill:#87CEEB
```

Unlike other solutions that focus solely on data collection via protocols or third-party agents, _Datasets_ categorize telemetry data&mdash;i.e. Windows Event Logs, Windows User Activity, Linux Audit Logs, etc.&mdash;at the source, simplifying pipeline design and enabling advanced _Role-Based Access Control_ (RBAC). This early categorization ensures that data is properly classified and secured from the moment it enters the system.

With _Datasets_, users can define role-based access at the data level to ensure that the teams working on the same source are fully isolated from each other. This innovative approach delivers greater flexibility and tighter security. For example, the SOC team can access security-related events while the Audit team works with compliance-related data, all without interference or overlap.

Key benefits of the Dataset and RBAC structure include:

* Granular access control - precise data access management at the dataset level
* Team isolation - complete separation between different team workspaces
* Simplified compliance - easier implementation of regulatory requirements
* Flexible categorization - adaptable dataset definitions based on business needs
* Efficient data routing - automated routing based on dataset classifications
* Enhanced security - reduced risk of unauthorized data access
* Streamlined management - centralized control over data access and distribution

The Dataset architecture also supports dynamic data routing, allowing organizations to define sophisticated processing rules based on dataset categories. This ensures that data not only reaches the right teams but also undergoes appropriate processing and enrichment before reaching its final destination in the SIEM platform.

## Dedicated Storage Format ##

The _VirtualMetric File Format_ (VMF) was engineered specifically for high-performance pipeline engines. It represents a significant advancement in log data storage and processing, building upon the foundations of Apache Avro while addressing its limitations for telemetry data handling.

VMF combines the efficiency of a row-based format with sophisticated data organization capabilities, enabling it to handle massive volumes of small data chunks efficiently. This hybrid approach provides optimal performance for both sequential processing and random access patterns, making it uniquely suited for telemetry data management.

```mermaid
graph LR
    VMF[VMF]
    
    VMF -.-> Compression([99% Compression])
    VMF -.-> Storage([Zero Trust Storage])
    VMF -.-> Filters([Bloom Filters])
    VMF -.-> Chain([Log Chaining])
    VMF -.-> TSA[(TSA Integration)]
    
    subgraph Benefits
        Compression -.-> |Efficient| Transport[/Network Transport/]
        Filters -.-> |Fast| Search[/Search Capabilities/]
        Chain -.-> |Enhanced| Forensics[/Forensic Integrity/]
        Storage -.-> |Secure| DataHandling[/Data Handling/]
    end
    
    style VMF fill:#FFCCCC
```

With its roots in Apache Avro, VMF overcomes the limitations of Avro OCF through innovative features designed specifically for telemetry data:

### Advanced Compression
  * Achieves up to 99% compression ratio
  * Optimized for both storage efficiency and quick access
  * Intelligent compression selection based on data patterns
  * Minimal CPU overhead during compression/decompression

### Zero Trust Storage
  * End-to-end encryption of stored data
  * Cryptographic verification of data integrity
  * Access control integrated at the storage level
  * Secure key management and rotation

### Bloom Filters
  * Rapid search capabilities across large datasets
  * Efficient index management
  * Minimized false positive rates
  * Optimized memory usage for filter operations

### Log Chaining
  * Cryptographic linking of sequential log entries
  * Tamper-evident log storage
  * Verifiable audit trails
  * Guaranteed log sequence integrity

### TSA Integration
  * Time-stamping authority integration
  * Certified temporal proof of log existence
  * Compliance with regulatory requirements
  * Non-repudiation of log timestamps

The format's design enables disk-level merging without consuming system resources, making it highly efficient for large-scale data operations. This capability is particularly valuable in high-throughput environments where traditional formats would create significant system overhead.

VMF's comprehensive feature set makes it the ideal choice for organizations requiring high-performance, secure, and compliant log data handling. Whether the priority is storage efficiency, search performance, or forensic integrity, VMF provides the necessary capabilities out of the box.

## Advanced Data Routing ##

We have simplified data routing with our advanced `reroute` processor, eliminating the need for manual filtering that is necessary in other solutions. This innovative approach transforms complex routing scenarios into manageable, automated workflows that significantly reduce operational overhead.

The reroute processor operates at multiple levels of abstraction, allowing for both broad and granular control over data flow. At the pipeline level, it handles the overall flow of data between major system components, while at the content pack level, it manages specific data transformations and routing rules for different types of content.

```mermaid
graph TD
    subgraph Sources[Data Sources]
        S1[(Source 1)]
        S2[(Source 2)]
        S3[(Source 3)]
    end
    
    subgraph Routing[Reroute Processor]
        F([Advanced Filters])
        P([Pipeline Level])
        C([Content Pack Level])
    end
    
    Sources -.-> Routing
    Routing -.-> |Dataset Coalescing| SIEM[Single SIEM Endpoint]
    
    style Routing fill:#98FB98
```

This processor allows users to route data to destinations at the pipeline or content pack level. Advanced filters can be applied for precise data routing, and the _Dataset_ concept further streamlines this by enabling multiple data sources to coalesce around a single SIEM endpoint. The result is a more efficient and maintainable routing infrastructure that adapts to changing requirements.

Key capabilities of the advanced routing system include:

### Multi-level Routing Control
  * Pipeline-level traffic management
  * Content pack-specific routing rules
  * Conditional routing based on data attributes
  * Dynamic destination selection

### Advanced Filtering
  * Complex condition evaluation
  * Pattern matching and regular expressions
  * Metadata-based filtering
  * Content-aware routing decisions

### Dataset Coalescing
  * Unified endpoint management
  * Intelligent data aggregation
  * Optimized bandwidth utilization
  * Reduced endpoint complexity

### Scalability Features
  * Horizontal scaling support
  * Load-balanced routing
  * Automatic failover
  * High-availability design

The reroute processor's sophisticated architecture enables organizations to design efficient and scalable routing strategies with ease. By automating complex routing decisions and providing flexible configuration options, it significantly reduces the operational burden of managing large-scale data flows while ensuring optimal data delivery to target systems.

## Extensive Processor Support ##

Our pipeline engine adopts the widely recognized Elastic Ingest Pipeline format, allowing IT and Security Engineers to create and manage pipelines effortlessly. This adoption of a familiar standard significantly reduces the learning curve while providing powerful data processing capabilities through a comprehensive set of built-in processors.

The pipeline architecture is designed with flexibility and ease of use in mind, offering both low-code and no-code options for pipeline configuration. This approach makes it accessible to team members with varying levels of technical expertise while maintaining the power and flexibility needed for complex data processing scenarios.

```mermaid
graph TD
    EIP([Elastic Ingest Pipeline])
    
    subgraph Processors[65+ Processors]
        P1(Parsing)
        P2(Filtering)
        P3(Enrichment)
    end
    
    EIP -.-> |Low Code/No Code| Processors
    Processors -.-> Routing([Routing])
    Routing -.->  |Low Code/No Code| Engineers[(IT/Security Engineers)]
    
    style EIP fill:#87CEEB
    style Processors fill:#98FB98
```

With 65+ processors, **DataStream** has the most comprehensive support in the industry for _low-code_/_no-code_ management enabling tasks like parsing, filtering, enrichment, routing, and more. Engineers with **Elastic** experience can leverage this robust and flexible pipeline engine, reducing onboarding time and enhancing operational efficiency.

Key processor categories and capabilities include:

### Data Parsing
  * Structured log parsing
  * JSON/XML processing
  * CSV handling
  * Regular expression extraction
  * Custom format support

### Filtering Operations
  * Content-based filtering
  * Metadata filtering
  * Conditional processing
  * Pattern matching
  * Threshold-based filtering

### Data Enrichment
  * Metadata addition
  * Field transformation
  * Lookup table integration
  * Geolocation enrichment
  * Threat intelligence correlation

### Advanced Processing
  * Data aggregation
  * Statistical analysis
  * Machine learning integration
  * Complex event processing
  * Custom script execution

The extensive processor library enables organizations to implement sophisticated data processing workflows without extensive coding or development effort. Each processor is designed with both performance and usability in mind, providing powerful capabilities while maintaining ease of configuration and management.

## Microsoft Sentinel Integration ##

The pipeline engine was specifically crafted to integrate seamlessly with Microsoft Sentinel, providing a sophisticated integration layer that understands and optimizes data flow into Sentinel's Advanced Security Information Model (ASIM) structure. This native integration eliminates the complexity typically associated with Sentinel data ingestion and normalization.

Our integration approach focuses on intelligent context inference, automatically analyzing incoming log messages to determine their security context and relevance. This automated classification ensures that data is properly categorized and routed to the appropriate ASIM tables without manual intervention.

```mermaid
graph LR
    Log[Log Messages] -.-> Context([Context Inference])
    Context -.-> Filter{Contextual Filters}
    
    subgraph Sentinel[Microsoft Sentinel]
        ASIM[(ASIM Tables)]
    end
    
    Filter -.-> |Optimized Data| ASIM
    
    style Sentinel fill:#87CEEB
    style Context fill:#98FB98
```

By inferring context from log messages, our solution automatically ingests data into the appropriate ASIM tables, drastically reducing the manual effort and accelerating integration. This intelligent mapping ensures that security events are properly normalized and enriched before reaching Sentinel's analytics engine.

Key features of the Sentinel integration include:

### Intelligent Context Inference
  * Automatic event classification
  * Smart field mapping
  * Metadata enrichment
  * Relationship detection
  * Pattern recognition

### ASIM-Optimized Processing
  * Native ASIM table mapping
  * Schema validation
  * Field normalization
  * Data type conversion
  * Relationship preservation

### Cost Optimization
  * Smart data filtering
  * Deduplication
  * Aggregation options
  * Volume optimization
  * Ingestion rate control

###  Performance Features
  * Parallel processing
  * Batch optimization
  * Connection pooling
  * Buffer management
  * Rate limiting

With contextual filters, users can easily optimize data ingestion to ensure only relevant information is sent to Sentinel, saving time and reducing costs by increasing efficiency. The system's intelligent filtering capabilities help organizations maintain high-quality security data while minimizing storage and processing overhead in their Sentinel deployment.

---

<div className="about-byline">
Whatever your telemetry needs, **DataStream** has something to offer to make your operations much more robust, secure, streamlined, and efficient at drastically reduced costs.
</div>

---