---
sidebar_label: Overview
---

# Routes: Overview

The **Routes** section in DataStream provides comprehensive data routing management, enabling you to define how data flows from devices through pipelines to targets. Routes serve as the orchestration layer that connects your data sources, processing logic, and output destinations into cohesive data processing workflows.

## Routes Architecture

Routes in DataStream act as the coordination mechanism between three core components:
- **Devices**: Data input sources (Syslog, HTTP, TCP, UDP, Agents, etc.)
- **Pipelines**: Data processing and transformation logic
- **Targets**: Output destinations (Azure Sentinel, Files, Databases, etc.)

By defining routes, you create end-to-end data processing workflows that automatically handle data collection, processing, and delivery according to your specific requirements.

## Route Types

DataStream provides two primary route types designed to accommodate different complexity levels and use cases:

### Quick Routes

**Quick Routes** are designed for straightforward data routing scenarios where you need to establish direct connections between data sources and destinations with minimal configuration complexity.

**Key Characteristics:**
- **Simplified Configuration**: Streamlined setup process with essential options
- **Direct Data Flow**: Straightforward source-to-destination routing
- **Rapid Deployment**: Quick setup for common use cases
- **Minimal Processing**: Basic data handling with standard transformations
- **Ideal For**: Standard log forwarding, basic data collection, simple integrations

**Common Use Cases:**
- Forward syslog data to Azure Sentinel
- Collect application logs to file storage
- Route network data to analytics platforms
- Basic compliance data collection

### Advanced Routes

**Advanced Routes** provide comprehensive routing capabilities for complex scenarios requiring sophisticated data processing, conditional logic, and multi-destination routing.

**Key Characteristics:**
- **Complex Configuration**: Full access to all routing and processing options
- **Conditional Logic**: Route data based on content, metadata, or other criteria
- **Multi-Destination Routing**: Send data to multiple targets simultaneously
- **Advanced Processing**: Complex transformations, enrichment, and filtering
- **Ideal For**: Enterprise environments, complex compliance requirements, multi-tenant scenarios

**Common Use Cases:**
- Route security events to multiple SIEM platforms based on severity
- Apply different processing logic based on data source
- Implement complex compliance workflows with multiple outputs
- Create data lakes with sophisticated routing and transformation logic

## Route Management Interface

The routes management interface provides comprehensive tools for creating, monitoring, and maintaining your data routing configurations:

### Route Visualization
- **Route Status Dashboard**: Overview of all configured routes with status indicators
- **Data Flow Monitoring**: Real-time visualization of data movement through routes
- **Performance Metrics**: Throughput, latency, and error rate monitoring
- **Health Indicators**: Route operational status and component health

### Management Capabilities
- **Create New Routes**: Access to both Quick Route and Advanced Route creation wizards
- **Edit Existing Routes**: Modify route configurations and processing logic
- **Clone Routes**: Duplicate successful configurations for similar use cases
- **Delete Routes**: Remove routes with impact assessment and safety confirmations

### Search and Organization
- **Search Functionality**: Locate routes by name, device, target, or pipeline criteria
- **Filtering Options**: Filter by route status, type, creation date, or performance metrics
- **Category Organization**: Group routes by environment, use case, or organizational structure
- **Tag Management**: Use custom tags for enhanced route organization and discovery

## Route Components Integration

### Device Integration
Routes seamlessly integrate with all supported device types:
- **Push Devices**: Syslog, HTTP, TCP, UDP, eStreamer protocols
- **Pull Devices**: Windows Agents for local data collection
- **Cloud Sources**: API integrations and cloud service connectors
- **File Sources**: File monitoring and batch processing capabilities

### Pipeline Integration
Routes leverage the full power of DataStream's processing capabilities:
- **Data Transformation**: Convert between formats, normalize schemas
- **Data Enrichment**: Add context, lookup tables, threat intelligence
- **Filtering and Routing**: Conditional processing based on data content
- **Performance Optimization**: Batching, compression, and throughput optimization

### Target Integration
Routes support all available output destinations:
- **Cloud Platforms**: Azure Sentinel, AWS, Google Cloud
- **SIEM Systems**: Splunk, QRadar, ArcSight
- **Databases**: SQL Server, PostgreSQL, Elasticsearch
- **File Systems**: Local storage, network shares, cloud storage
- **Message Queues**: Kafka, RabbitMQ, NATS

## Route Performance and Monitoring

### Real-Time Monitoring
Each route provides comprehensive monitoring capabilities:
- **Data Flow Status**: Whether data is actively flowing through the route
- **Processing Metrics**: Volume and speed of data processing
- **Error Rates**: Issues with data delivery or transformation
- **Resource Usage**: System resources consumed by the route
- **Last Activity**: Timestamp of most recent data processing

### Performance Optimization
- **Throughput Analysis**: Monitor data processing rates and identify bottlenecks
- **Resource Utilization**: Track CPU, memory, and network usage
- **Error Analysis**: Identify and resolve processing issues
- **Capacity Planning**: Plan for data volume growth and scaling requirements

## Route Lifecycle Management

### Route Creation
1. **Requirements Analysis**: Identify data sources, processing needs, and output requirements
2. **Route Type Selection**: Choose between Quick Routes for simplicity or Advanced Routes for complexity
3. **Configuration**: Define devices, pipelines, and targets using the appropriate wizard
4. **Testing**: Validate route functionality before production deployment
5. **Deployment**: Activate the route and begin data processing

### Route Maintenance
- **Configuration Updates**: Modify routing logic, processing rules, or destination settings
- **Performance Tuning**: Optimize route performance based on monitoring data
- **Troubleshooting**: Diagnose and resolve route issues using built-in diagnostic tools
- **Scaling**: Adjust route capacity for changing data volumes

### Route Evolution
- **Version Control**: Track configuration changes and maintain route history
- **Migration**: Move routes between environments (development, staging, production)
- **Upgrade Paths**: Migrate from Quick Routes to Advanced Routes as requirements evolve
- **Decommissioning**: Safely remove routes with proper impact assessment

## Best Practices

### Route Design
- **Clear Naming**: Use descriptive names that indicate data flow and purpose
- **Logical Organization**: Group related routes for easier management
- **Documentation**: Maintain clear descriptions of route functionality and business purpose
- **Modular Design**: Create reusable components that can be shared across routes

### Performance Optimization
- **Resource Monitoring**: Track route performance and resource utilization
- **Batch Processing**: Optimize batch sizes for efficient data processing
- **Error Handling**: Implement robust error handling and recovery mechanisms
- **Load Balancing**: Distribute processing load across multiple resources

### Security and Compliance
- **Access Control**: Implement proper permissions for route management
- **Data Protection**: Ensure appropriate security measures for sensitive data
- **Audit Logging**: Maintain comprehensive audit trails for compliance
- **Regular Reviews**: Periodically review route configurations for security and compliance

The Routes overview provides the foundation for effective data flow orchestration, enabling you to build robust, scalable, and maintainable data processing workflows that meet your organization's specific requirements.
