---
sidebar_label: Overview
---

# Pipelines: Overview

The **Pipelines** section in DataStream provides comprehensive pipeline management capabilities, serving as the central workspace for creating, monitoring, and managing your data processing pipelines. Pipelines are the core components that transform, enrich, and route data from devices to targets within your DataStream environment.

## Pipeline Management Interface

The pipelines interface serves as the primary working area where you create, monitor, and manage your pipelines. The view provides a comprehensive interface for pipeline management with multiple tools and viewing options designed to support both simple and complex pipeline management workflows.

### Core Pipeline Functions

- **Pipeline Creation**: Build new data processing pipelines using the guided creation wizard
- **Pipeline Monitoring**: Track pipeline performance, status, and data flow metrics
- **Pipeline Management**: Edit, clone, delete, and maintain existing pipelines
- **Multi-View Interface**: Switch between table, card, and list views for optimal workflow
- **Advanced Filtering**: Locate and organize pipelines using comprehensive search and filter options

## Interface Components

### Filtering and Search Tools

The left-hand side of the view contains comprehensive tools for managing your pipelines:

- **Search Pipeline Box**: Type pipeline names for quick location and identification
- **Category Filters**: Group pipelines by type, status, device vendor, or target destination
- **Advanced Filters**: Filter by creation date, last modification, or processing status
- **Clear Filters**: Reset all filtering criteria to display all available pipelines

If there are no pipelines matching your criteria, or if you have not created any pipelines yet, the panel displays a **No pipelines found** message with guidance for creating your first pipeline.

### View Modes

The interface supports multiple viewing modes for pipeline management, allowing you to choose the most suitable format for your workflow:

#### Table View
The table view provides a structured format for detailed pipeline information display with the following columns:
- **Name**: The name of the pipeline for identification
- **Device Type**: The device type that the pipeline receives data from
- **Device Vendor**: Vendor of the source device (see Content Hub for examples)
- **Target**: The target destination for pipeline output (see Content Hub for examples)
- **Last Updated**: Date of last modification made to the pipeline
- **Created**: Date of pipeline creation

#### Alternative View Modes
- **Card View**: Visual cards showing pipeline summaries, status indicators, and key metrics
- **List View**: Compact listing format for quick pipeline identification and navigation

### Pipeline Management Operations

The interface provides comprehensive management capabilities for existing pipelines:

- **Create New Pipeline**: Access the guided pipeline creation wizard
- **Edit Pipeline**: Modify existing pipeline configurations and processing logic
- **Clone Pipeline**: Duplicate pipelines for similar use cases or testing
- **Delete Pipeline**: Remove pipelines with comprehensive impact assessment
- **Status Management**: Enable, disable, or troubleshoot pipeline operations
- **Performance Monitoring**: Track pipeline throughput, latency, and error rates

## Pipeline Lifecycle Management

### Pipeline Creation

Create new pipelines using the **Create new pipeline** button, which launches the comprehensive pipeline creation wizard. This guided process helps you:
- Select appropriate data sources and device types
- Configure data processing logic and transformations
- Define output targets and routing rules
- Test and validate pipeline configuration

### Pipeline Monitoring

Ongoing pipeline monitoring includes:
- **Real-time Status**: Current operational state and health indicators
- **Performance Metrics**: Data throughput, processing latency, and resource utilization
- **Error Tracking**: Failed processing attempts and error analysis
- **Data Flow Visualization**: Visual representation of data movement through the pipeline

### Pipeline Maintenance

Regular pipeline maintenance capabilities:
- **Configuration Updates**: Modify processing logic and routing rules
- **Performance Optimization**: Adjust settings for optimal throughput and efficiency
- **Troubleshooting**: Diagnose and resolve pipeline issues
- **Version Control**: Track configuration changes and maintain pipeline history

## Pipeline Organization

### Categorization and Grouping

Organize pipelines using multiple classification methods:
- **By Device Type**: Group pipelines based on source device types (Syslog, HTTP, TCP, etc.)
- **By Vendor**: Organize by device manufacturer or data source vendor
- **By Target**: Group by output destination (Azure Sentinel, File, Database, etc.)
- **By Environment**: Separate development, staging, and production pipelines
- **By Use Case**: Organize by functional purpose (Security, Operations, Compliance, etc.)

### Search and Discovery

Efficient pipeline location using:
- **Name-based Search**: Quick text search across pipeline names and descriptions
- **Metadata Filtering**: Filter by creation date, modification date, or status
- **Tag-based Organization**: Use custom tags for pipeline categorization
- **Advanced Search**: Combine multiple search criteria for precise pipeline location

## Pipeline Types and Use Cases

### Common Pipeline Categories

- **Security Pipelines**: Process security events, threats intelligence, and compliance data
- **Operational Pipelines**: Handle system logs, performance metrics, and infrastructure monitoring
- **Application Pipelines**: Process application logs, errors, and performance data
- **Network Pipelines**: Handle network flows, traffic analysis, and connectivity monitoring
- **Compliance Pipelines**: Ensure regulatory compliance through specialized data processing

### Integration Patterns

- **Device-to-Target**: Direct data flow from source devices to output targets
- **Multi-Source Aggregation**: Combine data from multiple sources into unified streams
- **Data Enrichment**: Enhance incoming data with additional context and intelligence
- **Format Transformation**: Convert data between different formats and schemas
- **Conditional Routing**: Route data based on content, type, or other criteria

## Best Practices

### Pipeline Design
- **Clear Naming**: Use descriptive names that indicate pipeline purpose and data flow
- **Logical Organization**: Group related pipelines for easier management
- **Documentation**: Maintain clear descriptions of pipeline functionality and purpose
- **Testing**: Validate pipeline functionality before production deployment

### Performance Optimization
- **Resource Monitoring**: Track pipeline resource utilization and performance
- **Batch Processing**: Optimize batch sizes for efficient data processing
- **Error Handling**: Implement robust error handling and recovery mechanisms
- **Capacity Planning**: Plan for data volume growth and processing requirements

### Maintenance and Operations
- **Regular Review**: Periodically review pipeline configurations and performance
- **Update Management**: Keep pipeline configurations current with changing requirements
- **Backup Procedures**: Maintain backups of critical pipeline configurations
- **Change Control**: Implement proper change management for pipeline modifications

The Pipelines overview provides the foundation for effective data processing management, enabling you to build, monitor, and maintain robust data processing workflows that meet your organization's specific requirements.
