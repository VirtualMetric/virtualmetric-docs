---
pagination_prev: null
pagination_next: null
---

# Appendix

## Configuration BNF

All **DataStream** configuration files are in YAML format and conform to the following syntax:

<TermTable>
   <TermCol>`comp-decl`</TermCol>
   <DefCol>`::= <comp-type> ":" "\n\t" <comp-def>`</DefCol>

   <TermCol>`comp-type`</TermCol>
   <DefCol>`::= ("devices"|"targets"|"pipelines"|"routes")`</DefCol>

   <TermCol>`comp-def`</TermCol>
   <DefCol>`::= <id-fld-def> <fld-defs>`</DefCol>

   <TermCol>`id-fld-def`</TermCol>
   <DefCol>`::= "-" <id-fld-name> ":" <id>`</DefCol>

   <TermCol>`id-fld-name`</TermCol>
   <DefCol>`::= "-" <id>`</DefCol>

   <TermCol>`id`</TermCol>
   <DefCol>`::= <txt-val>`</DefCol>

   <TermCol>`fld-defs`</TermCol>
   <DefCol>`::= <fld-def> ["\n" <fld-defs>]*`</DefCol>

   <TermCol>`fld-def`</TermCol>
   <DefCol>`::= <fld-name> ":" <fld-vals>`</DefCol>

   <TermCol>`fld-name`</TermCol>
   <DefCol>`::= <txt-val>`</DefCol>

   <TermCol>`fld-vals`</TermCol>
   <DefCol>`::= (<fld-val>|"[" <fld-val> ["," <fld-vals>]* "]"|["\n\t -" <fld-val>]+)`</DefCol>

   <TermCol>`fld-val`</TermCol>
   <DefCol>`::= (txt-val|num-val)`</DefCol>

   <TermCol>`txt-val`</TermCol>
   <DefCol>`::= <txt-char>+ <alnum-char>*`</DefCol>

   <TermCol>`num-val`</TermCol>
   <DefCol>`::= ('-'|'+')? <num-char>+`</DefCol>

   <TermCol>`alnum-char`</TermCol>
   <DefCol>`::= (<txt-char>|<num-char>)*`</DefCol>

   <TermCol>`txt-char`</TermCol>
   <DefCol>`::= ('a' .. 'z'|'A' .. 'Z'|'_')`</DefCol>

   <TermCol>`num-char`</TermCol>
   <DefCol>`::= '0' .. '9'`</DefCol>
</TermTable>

## File Formats

### Parquet

Apache Parquet is a column-oriented storage format designed for efficiency:

**Row-based storage** (traditional):

```plaintext
id,name,last_name,age
1,John,Buck,35
2,Jane,Doe,27
3,Joe,Dane,42
```

**Column-based Storage** (Parquet):

```plaintext
id:1,2,3
name:John,Jane,Joe
last_name:Buck,Doe,Dane
age:35,27,42
```

Key features:

- Dictionary encoding
- Compressing and bit packing
- Run-length encoding
- Optimal for columnar queries

### PEM

Privacy Enhanced Mail (PEM) files are used for storing cryptographic keys and certificates:

```encoding
---BEGIN CERTIFICATE---
MIIHzTCCBbWgAwIBAgIQaBYE3/M08XHYCnNVmcFBcjANBgkqhkiG9w0BAQsFADBy
...
---END CERTIFICATE---
```

Common uses:

- SSL/TLS certificates
- SSH keys
- RSA private keys
- Certificate chains

File characteristics:

- Base64-encoded content
- Begin/end markers
- Can contain multiple certificates/keys
- Text-based format

### Avro

**Apache Avro** is a data serialization system that provides rich data structures and a compact, fast, binary data format. Originally developed within the Apache Hadoop ecosystem, Avro is designed for schema evolution and language-neutral data exchange.

#### ðŸ“¦ Binary Layout

|Section|Internal Name|Description|Possible Values / Format|
|--:|:-:|:--|:--|
|**File Header**|`magic`|4-byte magic number identifying Avro files|ASCII: `Obj` followed by `1` byte (hex: `4F 62 6A 01`)|
||`meta`|Metadata map storing key-value pairs (e.g., schema, codec)|Map of string keys to byte values (e.g., `"avro.schema"` â†’ JSON schema string)|
||`sync`|16-byte random sync marker used between blocks| 16 random bytes (unique per file)|
|**Data Block**|`blockCount`|Number of records in the block|Long (variable-length zigzag encoding)|
||`blockSize`|Size in bytes of the serialized records (after compression, if any)|Long|
||`blockData`| Serialized records (optionally compressed)|Binary-encoded data per schema|
||`sync`| Sync marker repeated after each block|Same 16-byte value as in header|

#### ðŸ§¬ Schema Types (Stored in Metadata)

|Type|Internal Name|Description|Example / Format|
|--:|:-:|:--|:--|
|Primitive|`null`, `boolean`, `int`, `long`, `float`, `double`, `bytes`, `string`|Basic types|`"type": "string"|
|Record|`record`|Named collection of fields|`{ "type": "record", "name": "Person", "fields": [...] }`|
|Enum|`enum`|Named set of symbols| `{ "type": "enum", "name": "Suit", "symbols": ["SPADES", "HEARTS"] }`|
|Array|`array`|Ordered list of items|`{ "type": "array", "items": "string" }`|
|Map|`map`|Key-value pairs with string keys|`{ "type": "map", "values": "int" }`|
|Union|JSON array|Multiple possible types|`[ "null", "string" ]`|
|Fixed|`fixed`|Fixed-size byte array|`{ "type": "fixed", "name": "md5", "size": 16 }`|

#### ðŸ§° Metadata Keys (in `meta`)

|Key|Description|Example Value|
|--:|:--|:--|
|`avro.schema`|JSON-encoded schema|JSON string defining the schema|
|`avro.codec`|Compression codec used (optional)|`"null"` (default), `"deflate"`, `"snappy"`, `"bzip2"`, `"xz"`|

## Log Formats

### ASIM

The Advanced Security Information Model is a layer between the data and the user to configure what and how to ingest data from a source and to route it to a destination. ASIM provides standardization for security-focused log data.

Available ASIM tables:

- `ASimAuditEventLogs`
- `ASimAuthenticationEventLogs` 
- `ASimDhcpEventLogs`
- `ASimDnsActivityLogs`
- `ASimFileEventLogs`
- `ASimNetworkSessionLogs`
- `ASimProcessEventLogs`
- `ASimRegistryEventLogs`
- `ASimUserManagementActivityLogs`
- `ASimWebSessionLogs`

### CEF

The Common Event Format is a standardized security event logging layout. Its creator is ArcSight, and it has been widely adopted by the industry. Features include:

- Standard header with 7 required fields
- Extensible key-value pair extension format
- Header fields include: version, device vendor, device product, device version, signature ID, name, and severity
- Extension fields use a key=value format

### CIM

The Common Information Model (CIM) is a standardized data model developed by Splunk. It provides:

**Common Fields**:

|Field Category|Fields|Description|
|:-:|:--|:--|
|Base Fields|`source`, `sourcetype`, `timestamp`, `host`, `index`|Core fields for event identification and source tracking|
|Identity Fields|`user`, `src_user`, `dest_user`|User identification and authentication tracking|
|Network Fields|`src_ip`, `dest_ip`, `src_port`, `dest_port`|Network communication endpoints|

**Data Models**:

|Model Type|Fields|Purpose|
|:-:|:--|:--|
|Authentication|`action`, `app`, `status`, `auth_method`|Track authentication events and access control|
|Network Traffic|`bytes`, `protocol`, `direction`, `tcp_flags`|Monitor network communications and traffic patterns|
|Vulnerability|`severity`, `signature`, `vulnerability_id`|Track security vulnerabilities and risks|
|Changes|-|Track system and configuration changes|
|Intrusion Detection|-|Monitor security threats and intrusions|

**Event Categories**:

|Category|Event Types|Description|
|--:|:--|:--|
|Authentication|`success`, `failure`, `logout`|Authentication-related events and outcomes|
|Network|`connection`, `alert`, `traffic`|Network activity and communications|
|System|`change`, `status`, `error`|System-level events and status changes|
|Security|-|Security-related events and alerts|

### ECS

Elastic Common Schema (ECS) is a specification that defines a common set of fields for ingesting data into Elasticsearch. Field groups include:

|Field Group|Core Fields|Description|
|:-:|:--|:--|
|Base Fields|`@timestamp`, `tags`, `labels`, `message`|Universal fields that appear in every event|
|Host|`host.name`, `host.ip`, `host.os.*`, `host.mac`|Information about the host machine|
|Network|`network.protocol`, `network.type`, `network.direction`, `network.bytes`|Network activity details|
|Source/Destination|`source.ip`, `source.port`, `dest.ip`, `dest.port`|Communication endpoint information|
|User|`user.id`, `user.name`, `user.domain`, `user.email`|User-related information|
|Event|`event.category`, `event.type`, `event.action`, `event.outcome`|Event classification details|
|File|`file.path`, `file.size`, `file.type`, `file.hash.*`|File-related information|
|Process|`process.pid`, `process.name`, `process.args`, `process.parent.*`|Process execution details|
|Error|`error.code`, `error.message`, `error.type`, `error.stack_trace`|Error-related information|
|Trace|`trace.id`, `span.id`, `transaction.id`|Distributed tracing data|

### eStreamer

Cisco's event streaming protocol used by Firepower Management Center (FMC) to send events to export security event data, intrusion alerts, connection logs, and other network telemetry in real-time. It enables integration with external SIEMs and analytics platforms, providing deep visibility into network security events.

|Field|Description|
|--:|:--|
|`eventType`|Type of event (e.g., intrusion, connection, malware)|
|`timestamp`|Time the event occurred|
|`sourceIP`|Source IP address|
|`destinationIP`|Destination IP address|
|`sourcePort`|Source port number|
|`destinationPort`|Destination port number|
|`protocol`|Transport protocol (TCP, UDP, etc.)|
|`userIdentity`|Associated user (if available)|
|`deviceUUID`|Unique identifier for the source device|
|`application`|Detected application (e.g., HTTP, SSH)|
|`threatScore`|Severity or risk rating of the event|
|`signatureID`|Identifier for the security rule triggered|
|`signatureName`|Description of the triggered security rule|
|`malwareSHA256`|Hash of detected malware (if applicable)|
|`fileName`|Name of the file involved in the event|

eStreamer provides detailed security telemetry and integrates with SIEMs for real-time threat monitoring and forensic analysis.

### IPFIX

The IP Flow Information Export is an IETF-standardized protocol for exporting flow-based traffic data from routers, switches, and other network devices. It is an evolution of NetFlow, offering greater flexibility by supporting custom fields and templates for diverse network monitoring, security, and analytics applications. IPFIX allows vendors to define and export additional data types beyond traditional NetFlow fields.

|Field|Description|
|--:|:--|
|`sourceIPv4Address`|Source IP address (IPv4)|
|`destinationIPv4Address`|Destination IP address (IPv4)|
|`sourceIPv6Address`|Source IP address (IPv6)|
|`destinationIPv6Address`|Destination IP address (IPv6)|
|`sourceTransportPort`|Source port number|
|`destinationTransportPort`|Destination port number|
|`protocolIdentifier`|Transport protocol (TCP, UDP, etc.)|
|`packetTotalCount`|Number of packets in the flow|
|`octetTotalCount`|Total bytes transferred|
|`flowStartMilliseconds`|Start timestamp in milliseconds|
|`flowEndMilliseconds`|End timestamp in milliseconds|
|`tcpControlBits`|TCP control tcp_flags|
|`ipClassOfService`|Type of Service (QoS marking)|
|`bgpSourceAsNumber`|Source BGP Autonomous System (AS) number|
|`bgpDestinationAsNumber`|Destination BGP AS number|
|`flowEndReason`|Reason the flow ended (e.g. timeout, TCP FIN)|

IPFIX extends NetFlow by supporting variable-length fields and user-defined templates, making it highly adaptable for modern network monitoring needs.

### LEEF

The Log Event Extended Format is an enterprise security event logging format created by IBM QRadar. 

Features:

- Lightweight parsing requirements
- Fixed header fields: version, vendor, product, version, eventID
- Variable attributes section
- Optimized for SIEM processing

### NetFlow

A network protocol developed by Cisco for collecting, analyzing, and monitoring network traffic. It captures metadata about IP traffic flows, providing insights into bandwidth usage, security threats, and network performance. NetFlow records include key details such as source and destination IPs, ports, protocol types, and timestamps.

|Field|Description|
|--:|:--|
|`SrcAddr`|Source IP address|
|`DstAddr`|Destination IP address|
|`SrcPort`|Source port number|
|`DstPort`|Destination port number|
|`Protocol`|Transport protocol (TCP, UDP, etc.)|
|`Packets`|Number of packets in the flow|
|`Bytes`|Total bytes transferred|
|`StartTime`|Timestamp of the first packet in the flow|
|`EndTime`|Timestamp of the last packet in the flow|
|`SrcAS`|Source Autonomous System (AS) number|
|`DstAS`|Destination Autonomous System (AS) number|
|`TCPFlags`|TCP control flags for the flow|
|`ToS`|Type of Service (QoS marking)|
|`NextHop`|IP address of the next hop router|
|`FlowDuration`|Duration of the flow in milliseconds|

This is a general overview; actual fields may vary depending on the versions and implementations.

### sFlow

sFlow (Sampled Flow) is a network monitoring protocol designed for high-speed networks. Unlike NetFlow and IPFIX, which capture complete flow records, sFlow uses packet sampling to provide scalable and efficient traffic analysis. It operates by embedding monitoring agents in network devices that randomly sample packets and send them to a central collector for analysis.  

|Field|Description|
|--:|:--|
|`sampleSequenceNumber`|Unique identifier for the sampled packet|
|`sourceIP`|Source IP address|
|`destinationIP`|Destination IP address|
|`sourcePort`|Source port number|
|`destinationPort`|Destination port number|
|`protocol`|Transport protocol (TCP, UDP, etc.)|
|`sampledPacketSize`|Size of the sampled packet in bytes|
|`inputInterface`|Interface where the packet was received|
|`outputInterface`|Interface where the packet was forwarded|
|`vlanID`|VLAN identifier of the packet|
|`tcpFlags`|TCP control flags|
|`flowSampleType`|Type of sampling (e.g., packet, counter)|
|`samplingRate`|Ratio of sampled packets to total packets|
|`agentAddress`|IP address of the device performing sampling|
|`collectorAddress`|IP address of the sFlow collector|

sFlow's lightweight sampling approach makes it ideal for real-time traffic monitoring in large-scale, high-speed networks.

## Pattern Matching Formats

### Grok Patterns

Common patterns used in log processing:

|Category|Patterns|
|--:|:--|
|General|`DATA` `GREEDYDATA` `NOTSPACE` `SPACE` `WORD`|
|Numeric|`BASE10NUM` `INT` `NUMBER`|
|Networking|`HOSTNAME` `IP` `IPV4` `IPV6` `MAC`|
|Data and Time|`DATESTAMP` `DATESTAMP_RFC822` `TIMESTAMP_ISO8601`|
|File System|`FILENAME` `PATH`|
|HTTP|`HTTPDATE` `HTTPDERRORLOG` `HTTPDUSER`|
|System|`SYSLOGBASE` `SYSLOGHOST` `SYSLOGTIMESTAMP`|
|Other|`EMAILADDRESS` `URIPARAM` `URIPATH` `UUID`|

### Metadata Tags

Common metadata fields used in log processing:

|Field|Subfields|
|--:|:--|
|`_ingest`|`on_failure_processor_tag` `on_failure_processor_type`|
|`_temp`|`observer.mac`|
|`destination`|`bytes` `domain` `ip` `nat.port` `port` `user.domain` `name`|
|`email`|`from.address` `to.address`|
|`event`|`category` `kind` `original` `outcome` `type`|
|`source`|`bytes` `ip` `user.domain` `group.name` `id` `xlatesrc`|
|`observer`|`product` `type` `vendor`|
|`related`|`hash` `ip`|

## Protocols

### Syslog

Standard protocol for system logging:

#### Message Format

**RFC 3164**:

|Field| Description|Example Value|
|---|---|---|
|`PRI`|Priority value = Facility * 8 + Severity, enclosed in angle brackets|`<34>`|
|`TIMESTAMP`|Date and time in "Mmm dd hh:mm:ss" format|Oct 22 12:34:56|
|`HOSTNAME`|Hostname or IP address of the sender|`<hostname>`|
|`TAG`|Application name and optional `PID`| `appname[1234]`|
|`MESSAGE`|Free-form message content|`This is a log message.`|

**RFC 5424**:

|Field|Description|Example Value|
|---|---|---|
|`PRI`|Priority value = Facility * 8 + Severity, enclosed in angle brackets|`<34>`|
|`VERSION`|Syslog protocol version (always 1 for RFC 5424)|`1`|
|`TIMESTAMP`|ISO 8601 timestamp with optional timezone|`2025-01-03T14:07:15.003Z`|
|`HOSTNAME`|FQDN or IP address of the sender|`host.example.com`|
|`APP-NAME`|Application name|`appname`|
|`PROCID`|Process ID|`1234`|
|`MSGID`|Identifier for the type of message|`ID47`|
|`STRUCTURED-DATA`|Optional structured key-value pairs|`[exampleSDID@32473 iut="3"]`|
|`MESSAGE`|Free-form message content|This is a structured log message.|

#### Facility Values

|Code|Facility|
|---|---|
|`0`|kernel messages|
|`1`|user-level messages|
|`2`|mail system|
|...|...|
|`16`â€“`23`|`local0` to `local7`|

#### Severity Levels

|Code|Level|
|---|---|
|`0`|Emergency|
|`1`|Alert|
|`2`|Critical|
|`3`|Error|
|`4`|Warning|
|`5`|Notice|
|`6`|Informational|
|`7`|Debug|

### Kafka

#### ðŸ“¦ Binary Layout

|Field|Internal Name|Description|Type / Format|Example / Values|
|--:|:--:|:--|:--|:--|
|**Size**|`length`|Total size of the request (excluding this field)|`int32`|e.g. `0x0000012C`|
|**API Key**|`api_key`|Identifies the type of request|`int16`|`0` = Produce, `1` = Fetch, etc.|
|**API Version**|`api_version`|Version of the API being used|`int16`| e.g. `7`|
|**Correlation ID**|`correlation_id`|Used to match requests to responses|`int32`|e.g. `12345`|
|**Client ID**|`client_id`|Optional identifier of the client|`string` (nullable)|e.g. `"my-client"`|
|**Request Body**|*(varies by API)*|The actual request payload|Structured binary|Depends on `api_key` and `api_version`|

#### ðŸ”‘ Common API Keys

|API Key|Name|Purpose|
|--:|:--|:--|
|`0`|Produce|Send messages to a topic|
|`1`|Fetch|Retrieve messages from a topic|
|`3`|Metadata|Get topic/partition info|
|`8`|Offset|Get earliest/latest offsets|
|`18`|ApiVersions|Discover supported API versions|
|`21`|SaslHandshake|SASL authentication handshake|
|`22`|SaslAuthenticate|SASL authentication|
|`42`|DescribeCluster|Get cluster metadata|

#### ðŸ§± Primitive Types Used

|Type|Description|
|:-:|---|
|`int8/16/32/64`|Signed integers (big-endian)|
|`string`|Length-prefixed UTF-8 string|
|`array<T>`|Length-prefixed array of type `T`|
|`bytes`|Length-prefixed byte array|
|`varint`|Variable-length integer (zigzag encoding)|

#### ðŸ”„ Response Structure

|Field|Description|
|--:|:--|
|`correlation_id`|Matches the request|
|`response_body`|Depends on the request type|
