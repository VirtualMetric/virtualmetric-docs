---
sidebar_label: High-Volume Sampling
---


# High-Volume Log Processing with Intelligent Sampling Strategies

## Synopsis

This tutorial demonstrates advanced techniques for handling extremely high-volume log streams through intelligent sampling, load balancing, and performance optimization. You'll learn to implement dynamic sampling algorithms, traffic shaping, and distributed processing to maintain real-time analysis capabilities while managing resource constraints and costs.

**Prerequisites:**

- High-volume log sources (>10GB/day or >10,000 events/second)
- Understanding of sampling theory and statistical analysis
- Knowledge of performance tuning and resource management

**Configuration files used:**

- `high-volume-sampling.yml` - Main configuration with sampling strategies
- `load-balancing.yml` - Distributed processing setup

**Sample data:**

- High-frequency web server logs
- Application performance monitoring data
- Network flow records

## Scenario

Organizations processing massive log volumes need to balance comprehensive monitoring with resource efficiency and cost control. This tutorial implements sophisticated sampling and optimization techniques that:

- **Adaptive sampling** - Dynamically adjusts sample rates based on volume and content
- **Stratified sampling** - Ensures representative samples across different event types
- **Priority-based processing** - Fast-tracks critical events while sampling routine traffic
- **Load distribution** - Spreads processing across multiple workers and targets
- **Resource optimization** - Minimizes CPU, memory, and storage requirements
- **Quality metrics** - Maintains statistical validity and monitoring effectiveness

**Data flow:**

High-Volume Input → Intelligent Classification → Adaptive Sampling → Priority Processing → Resource-Optimized Output

**Techniques demonstrated:**

- Dynamic sampling rate calculation
- Statistical sampling strategies
- Performance monitoring and auto-tuning
- Distributed processing architectures
- Resource usage optimization

## Setup

### Key Configuration Components

- **Devices**: High-throughput inputs with buffer management
- **Pipelines**: Multi-tier sampling and optimization strategies
- **Targets**: Load-balanced outputs with compression and batching

### Quick Reference of Processors Used

| Processor | Purpose | Key Parameters |
|-----------|---------|----------------|
| `sample` | Fixed-rate sampling | `rate`, `method` |
| `dynamic_sample` | Adaptive sampling | `base_rate`, `volume_threshold`, `burst_factor` |
| `iff` | Conditional processing | `if`, `then`, `else` |
| `group` | Batch processing | `size`, `timeout`, `sort_by` |
| `script` | Custom sampling logic | `lang`, `source` |

### Expected Input/Output Formats

**Input**: High-frequency JSON logs, CEF events, or structured text
**Output**: Statistically representative samples with processing metadata

## Trial

<Include id="note-config-files-execution" />

Create a configuration file named `high-volume-sampling.yml` in your working directory.

### Step 1: Configure High-Throughput Input Device

```yml title="high-volume-sampling.yml"
devices:
  - id: 1
    name: high_volume_syslog
    type: syslog
    status: true
    properties:
      protocol: udp
      address: 0.0.0.0
      port: 514
      
      # High-performance settings
      buffer_size: 16777216    # 16MB buffer
      workers: 8              # Multiple worker threads
      queue_size: 100000      # Large internal queue
      batch_size: 5000        # Process in large batches
      flush_interval: 1s      # Frequent flushing
      
      # Connection optimization
      receive_buffer: 33554432  # 32MB OS receive buffer
      reuse_port: true         # Allow multiple processes
      no_delay: true          # Disable Nagle algorithm
      
      # Overflow handling
      drop_on_overflow: true   # Drop oldest messages when full
      overflow_action: log     # Log overflow events
      
  # Secondary device for overflow processing
  - id: 2
    name: overflow_processor
    type: file
    status: true
    properties:
      path: "/var/log/overflow/*.log"
      follow: true
      multiline: false
      batch_size: 1000
      polling_interval: 5s
```

### Step 2: Configure Load-Balanced Output Targets

```yml title="high-volume-sampling.yml"
targets:
  # High-priority events (no sampling)
  - name: priority_events
    type: file
    status: true
    properties:
      location: /data/priority
      name: "priority_{{hour}}.json"
      format: json
      compression: lz4        # Fast compression
      rotation: hourly
      buffer_size: 1048576    # 1MB write buffer
      sync_frequency: 5s      # Sync every 5 seconds
      
  # Sampled events for analysis
  - name: sampled_analytics
    type: file
    status: true
    properties:
      location: /data/analytics
      name: "sampled_{{date}}.parquet"
      format: parquet
      compression: zstd       # High compression ratio
      rotation: daily
      
      # Parquet optimization for analytics
      row_group_size: 134217728  # 128MB
      page_size: 1048576         # 1MB
      dictionary_encoding: true
      
  # Archive with maximum compression
  - name: long_term_archive
    type: file
    status: true
    properties:
      location: /data/archive
      name: "archive_{{date}}.avro.xz"
      format: avro
      compression: xz         # Maximum compression
      compression_level: 9
      rotation: weekly
      
      # Archive-specific settings
      write_sync: false       # Async writes for performance
      buffer_size: 8388608    # 8MB buffer
```

### Step 3: Configure Intelligent Sampling Pipeline

```yml title="high-volume-sampling.yml"
pipelines:
  - name: intelligent_sampling_pipeline
    processors:
      # Parse and classify incoming events
      - json:
          field: message
          add_to_root: true
          ignore_failure: true
      
      # Add processing metadata
      - set:
          field: processing_timestamp
          value: "{{_timestamp}}"
      
      - set:
          field: hour_bucket
          value: "{{_timestamp | date('2006-01-02-15')}}"
      
      # Classify event priority
      - set:
          field: event_priority
          value: "critical"
          if: "log_level in ['ERROR', 'FATAL', 'CRITICAL'] or response_time > 10000"
      
      - set:
          field: event_priority
          value: "high"
          if: "event_priority == null and (log_level == 'WARN' or response_time > 5000 or status_code >= 500)"
      
      - set:
          field: event_priority
          value: "medium"
          if: "event_priority == null and (log_level == 'INFO' or status_code >= 400)"
      
      - set:
          field: event_priority
          value: "low"
          if: "event_priority == null"
      
      # Calculate dynamic sampling rate based on volume
      - script:
          lang: javascript
          source: |
            // Volume-based sampling calculation
            var hour = new Date().getHours();
            var baseRate = 0.1; // 10% base sampling rate
            var currentVolume = _internal_stats.events_per_second || 1;
            var targetThroughput = 1000; // Target 1000 events/second
            
            // Adjust sampling rate based on current volume
            var volumeRatio = currentVolume / targetThroughput;
            var adjustedRate = baseRate / Math.max(1, volumeRatio);
            
            // Time-based adjustments (lower sampling during peak hours)
            var peakHours = [9, 10, 11, 14, 15, 16];
            if (peakHours.includes(hour)) {
              adjustedRate *= 0.5; // Reduce sampling during peak
            }
            
            // Priority-based adjustments
            var priorityMultipliers = {
              'critical': 1.0,    // Keep all critical events
              'high': 0.8,        // Keep 80% of high priority
              'medium': 0.3,      // Keep 30% of medium priority
              'low': 0.05         // Keep 5% of low priority
            };
            
            dynamic_sample_rate = Math.min(1.0, adjustedRate * (priorityMultipliers[event_priority] || 0.1));
            volume_stats = {
              current_volume: currentVolume,
              target_throughput: targetThroughput,
              base_rate: baseRate,
              volume_ratio: volumeRatio,
              hour: hour
            };
      
      # Apply stratified sampling by event type
      - script:
          lang: javascript
          source: |
            // Ensure representative sampling across event types
            var eventType = log_level + '_' + (application || 'unknown');
            var typeHash = eventType.split('').reduce((a,b) => {
              a = ((a << 5) - a) + b.charCodeAt(0);
              return a & a;
            }, 0);
            
            // Use hash for consistent stratified sampling
            var stratumSampleRate = Math.abs(typeHash % 100) / 100.0;
            
            // Combine with dynamic rate
            final_sample_rate = Math.min(dynamic_sample_rate, stratumSampleRate + 0.01);
            event_stratum = eventType;
      
      # Apply the calculated sampling
      - dynamic_sample:
          rate: "{{final_sample_rate}}"
          method: "deterministic"
          seed_field: "event_id"
          metadata_field: "sampling_info"
      
      # Add sampling metadata for analysis
      - set:
          field: sampling_info.applied_rate
          copy_from: final_sample_rate
      
      - set:
          field: sampling_info.priority
          copy_from: event_priority
      
      - set:
          field: sampling_info.stratum
          copy_from: event_stratum
      
      - set:
          field: sampling_info.volume_stats
          copy_from: volume_stats
      
      # Performance optimization: remove large fields for sampled data
      - iff:
          if: "sampling_info.sampled == true"
          then:
            # Keep important fields, remove verbose ones
            - remove:
                field: full_user_agent
                ignore_missing: true
            
            - remove:
                field: raw_request_headers
                ignore_missing: true
            
            - remove:
                field: detailed_stack_trace
                ignore_missing: true
                if: "event_priority != 'critical'"
            
            # Truncate long fields
            - script:
                lang: javascript
                source: |
                  if (message_text && message_text.length > 1000) {
                    message_text = message_text.substring(0, 1000) + '... [truncated]';
                  }
                  if (request_body && request_body.length > 500) {
                    request_body = request_body.substring(0, 500) + '... [truncated]';
                  }
      
      # Add statistical weights for analysis
      - math:
          operation: divide
          operands: [1, "final_sample_rate"]
          target_field: statistical_weight
          if: "final_sample_rate > 0"
      
      - set:
          field: statistical_weight
          value: 1
          if: "final_sample_rate == 0 or statistical_weight == null"

  # Performance monitoring pipeline
  - name: performance_monitoring
    processors:
      # Collect performance metrics every minute
      - group:
          group_by: hour_bucket
          timeout: 60
          max_events: 100000
          target_field: performance_batch
      
      # Calculate throughput statistics
      - script:
          lang: javascript
          source: |
            if (performance_batch && performance_batch.length > 0) {
              var totalEvents = performance_batch.length;
              var sampledEvents = performance_batch.filter(e => e.sampling_info && e.sampling_info.sampled).length;
              var criticalEvents = performance_batch.filter(e => e.event_priority === 'critical').length;
              
              var priorities = {};
              var sources = {};
              performance_batch.forEach(e => {
                priorities[e.event_priority] = (priorities[e.event_priority] || 0) + 1;
                sources[e.application || 'unknown'] = (sources[e.application || 'unknown'] || 0) + 1;
              });
              
              performance_metrics = {
                total_events: totalEvents,
                sampled_events: sampledEvents,
                sampling_ratio: sampledEvents / totalEvents,
                critical_events: criticalEvents,
                priority_distribution: priorities,
                source_distribution: sources,
                events_per_second: totalEvents / 60,
                timestamp: new Date().toISOString()
              };
            }
      
      # Log performance metrics
      - debug:
          message: "Performance metrics: {{performance_metrics}}"
```

### Step 4: Configure Conditional Routing

```yml title="high-volume-sampling.yml"
routes:
  # Critical events bypass sampling
  - name: critical_events_route
    devices:
      - name: high_volume_syslog
    pipelines:
      - name: intelligent_sampling_pipeline
    targets:
      - name: priority_events
        if: "event_priority == 'critical'"
    
    # Real-time processing for critical events
    batch_processing: false
    max_latency: 100ms
    
  # Sampled events for analytics
  - name: sampled_analytics_route
    devices:
      - name: high_volume_syslog
    pipelines:
      - name: intelligent_sampling_pipeline
    targets:
      - name: sampled_analytics
        if: "sampling_info.sampled == true"
    
    # Batch processing for efficiency
    batch_processing: true
    batch_size: 10000
    batch_timeout: 30s
    
  # All events to archive (with compression)
  - name: archive_route
    devices:
      - name: high_volume_syslog
    pipelines:
      - name: intelligent_sampling_pipeline
    targets:
      - name: long_term_archive
    
    # Large batches for archive efficiency
    batch_processing: true
    batch_size: 50000
    batch_timeout: 300s
    
  # Performance monitoring route
  - name: performance_monitoring_route
    devices:
      - name: high_volume_syslog
    pipelines:
      - name: performance_monitoring
    targets:
      - name: priority_events  # Log performance metrics
    
    # Periodic performance reporting
    batch_processing: true
    batch_timeout: 60s
```

### Step 5: Configure Advanced Load Balancing

Create a companion file `load-balancing.yml`:

```yml title="load-balancing.yml"
# Multi-instance configuration for horizontal scaling
devices:
  - id: 1
    name: load_balancer_1
    type: syslog
    properties:
      port: 5140
      workers: 4
      instance_id: 1
      
  - id: 2
    name: load_balancer_2
    type: syslog
    properties:
      port: 5141
      workers: 4
      instance_id: 2

# Consistent hashing for load distribution
pipelines:
  - name: load_distribution
    processors:
      - fingerprint:
          fields: [source_ip, application]
          method: sha256
          target_field: routing_hash
      
      - script:
          lang: javascript
          source: |
            // Consistent hashing for load distribution
            var hash = parseInt(routing_hash.substring(0, 8), 16);
            var instance = hash % 4; // Distribute across 4 instances
            processing_instance = instance;
      
      - iff:
          if: "processing_instance == 0"
          then:
            - set:
                field: target_instance
                value: "instance_0"
```

### Step 6: Start High-Volume Processing

Launch multiple instances for load distribution:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```PowerShell
    # Start multiple instances
    Start-Process .\vmetric-director -ArgumentList "-config high-volume-sampling.yml -console -instance 1"
    Start-Process .\vmetric-director -ArgumentList "-config high-volume-sampling.yml -console -instance 2"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    # Start multiple instances
    ./vmetric-director -config high-volume-sampling.yml -console -instance 1 &
    ./vmetric-director -config high-volume-sampling.yml -console -instance 2 &
    ```
  </TabItem>
</Tabs>

### Step 7: Monitor Performance Metrics

Watch for performance indicators in the logs:

```console
[2025-01-12 10:30:45] [Information] [high_volume_syslog] Processing rate: 15,234 events/second
[2025-01-12 10:30:46] [Information] [sampling] Dynamic rate adjusted to 0.067 (6.7%)
[2025-01-12 10:30:47] [Information] [performance] Critical events: 234, Sampled: 1,567 of 15,234 total
[2025-01-12 10:30:48] [Information] [analytics] Archive compression ratio: 12.3:1
```

### Step 8: Verify Statistical Validity

Check sampling output for statistical validity:

```json
{
  "event_id": "evt_12345",
  "timestamp": "2025-01-12T10:30:45Z",
  "log_level": "INFO",
  "application": "web_server",
  "event_priority": "medium",
  "sampling_info": {
    "sampled": true,
    "applied_rate": 0.067,
    "priority": "medium",
    "stratum": "INFO_web_server",
    "volume_stats": {
      "current_volume": 15234,
      "target_throughput": 1000,
      "base_rate": 0.1,
      "volume_ratio": 15.234,
      "hour": 10
    }
  },
  "statistical_weight": 14.93,
  "performance_optimized": true
}
```

## Monitoring

### Key Metrics to Observe

1. **Throughput**: Events processed per second vs. input rate
2. **Sampling Rates**: Dynamic adjustment across different event types
3. **Resource Usage**: CPU, memory, and disk utilization
4. **Queue Depths**: Internal buffer utilization and overflow events
5. **Statistical Validity**: Sample distribution and representativeness

### Performance Tuning Guidelines

- **Buffer Sizing**: Increase buffers for higher throughput, balance with memory usage
- **Worker Threads**: Scale based on CPU cores and I/O characteristics
- **Batch Sizes**: Larger batches improve throughput but increase latency
- **Compression**: Balance speed vs. storage efficiency
- **Sampling Rates**: Adjust based on analysis requirements and resources

### Common Troubleshooting

**Issue**: Event drops due to overflow
- Increase buffer sizes
- Add more worker instances
- Implement backpressure handling

**Issue**: Biased sampling results
- Review stratification logic
- Check hash distribution
- Verify priority classifications

**Issue**: High CPU usage
- Optimize processor logic
- Reduce sampling complexity
- Implement preprocessing filters

**Issue**: Storage growing too fast
- Increase compression levels
- Adjust retention policies
- Optimize file formats

### Statistical Analysis

Validate sampling effectiveness:

```python
# Example Python analysis
import json
import pandas as pd

# Load sampled data
with open('sampled_analytics.json') as f:
    data = [json.loads(line) for line in f]

df = pd.DataFrame(data)

# Check sampling distribution
print("Sampling rate by priority:")
print(df.groupby('event_priority')['sampling_info.applied_rate'].mean())

# Verify statistical weights
print("\nWeighted vs. unweighted counts:")
total_weighted = df['statistical_weight'].sum()
total_unweighted = len(df)
print(f"Estimated total events: {total_weighted}")
print(f"Actual sampled events: {total_unweighted}")
```

### Capacity Planning

Monitor trends for capacity planning:

```console
[2025-01-12 10:30:45] [Information] [capacity] Peak processing rate: 18,567 eps
[2025-01-12 10:30:46] [Information] [capacity] Average sampling rate: 8.2%
[2025-01-12 10:30:47] [Information] [capacity] Storage efficiency: 23.4 MB/hour compressed
[2025-01-12 10:30:48] [Information] [capacity] Memory usage: 2.1GB peak, 1.4GB average
```

:::caution
High-volume processing requires careful resource management. Monitor system capacity and implement appropriate alerting for production deployments.
:::
