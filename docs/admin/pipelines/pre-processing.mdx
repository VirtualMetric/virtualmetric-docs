# Pre-processing

This is the stage where the pipelines attached to a Source perform a number of operations before the data is delivered to a later stage such as indexing or analysis.

Pre-processing pipelines help organizations optimize their data ingestion, reduce costs, and improve data quality before it reaches storage or analysis systems.

## Key Benefits

### Data Reduction
Lower downstream costs and improve performance through:
- **Filtering**: Remove unnecessary events and fields
- **Sampling**: Collect representative subsets of high-volume data
- **Deduplication**: Eliminate redundant events
- **Field removal**: Strip unnecessary metadata and fields
- **Aggregation**: Combine similar events into summary records

### Normalization
Convert diverse log formats into consistent, structured schemas:
- **Field standardization**: Map vendor-specific fields to common names
- **Format conversion**: Transform between ECS, CIM, and ASIM formats
- **Time normalization**: Standardize timestamps across sources
- **IP normalization**: Convert various IP address formats
- **Protocol normalization**: Standardize protocol fields

### Data Enrichment
Add context and value to raw data:
- **Geolocation**: Add geographic data for IP addresses
- **Threat intelligence**: Enrich with security context
- **Asset information**: Add system and user context
- **DNS lookups**: Resolve hostnames and domains
- **Custom lookups**: Add business-specific metadata

### Transformation
Modify data to meet security and compliance requirements:
- **PII masking**: Redact sensitive information
- **Field encryption**: Protect confidential data
- **Format conversion**: Transform data structures
- **Field extraction**: Parse complex messages
- **Data validation**: Ensure data quality

## Common Use Cases

### Security Operations
- Normalize various security tool outputs
- Enrich events with threat intelligence
- Mask sensitive data
- Filter out noise events
- Add geographic context

### Infrastructure Monitoring
- Standardize metrics formats
- Aggregate system statistics
- Filter debug messages
- Add asset context
- Convert timestamps

### Application Logging
- Parse structured data
- Extract error codes
- Sample debug logs
- Add service context
- Normalize severity levels

### Compliance
- Mask PII/PCI data
- Validate required fields
- Add compliance metadata
- Filter sensitive data
- Generate audit trails

## Pipeline Examples

### Log Volume Reduction
```yaml
pipeline:
  processors:
    - filter:
        condition: "severity > 'info'"
    - sampling:
        rate: 10
        condition: "event_type == 'debug'"
    - drop_fields:
        fields: ["debug_info", "internal_id"]
```

### Security Event Normalization
```yaml
pipeline:
  processors:
    - normalize:
        target_format: "ecs"
    - geoip:
        source: "source.ip"
        target: "source.geo"
    - lookup:
        file: "threat_intel.csv"
        match_field: "source.ip"
```

### PII Protection
```yaml
pipeline:
  processors:
    - mask:
        regex: "\b\d{16}\b"
        replacement: "XXXX-XXXX-XXXX-****"
    - mask:
        regex: "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b"
        replacement: "email@*****"
```

## Best Practices

1. **Order Matters**
   - Put high-impact filters first
   - Perform heavy enrichment last
   - Group similar operations

2. **Performance Optimization**
   - Use sampling for high-volume sources
   - Cache lookup results
   - Minimize regex usage

3. **Data Quality**
   - Validate after transformations
   - Monitor dropped events
   - Track field presence

4. **Maintenance**
   - Document transformations
   - Monitor pipeline performance
   - Review sampling rates
   - Update enrichment data

:::tip
Start with minimal processing and add steps incrementally while monitoring impact on performance and storage costs.
:::

:::warning
Complex processing can impact throughput. Monitor resource usage and adjust pipeline complexity accordingly.
:::