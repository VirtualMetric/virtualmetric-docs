---
pagination_prev: null
---

# Pipelines Overview

**Director** pipelines were designed to automate your processes dealing with data ingestion and data transformation. They can be used to extract values, transform or convert them, enrich them by correlating them with other available information, and to design the overall workflow of your telemetry data processing scheme.

## Definitions

Pipelines are chains of processors that run sequentially, operating on the incoming data streamed from providers named [sources](../sources/overview.mdx), and routing their output to consumers called [targets](../targets/overview.mdx) for consumption or further processing.

Schematically, this looks like so:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	provider[("`**Provider**
    data
  `")]
  pipeline{"`**Pipeline**
	  processor-1
    processor-2
    ...
    processor-n
  `"}
	consumer[/"`**Consumer**
    processed data
  `"/]
	provider --> pipeline --> consumer
```

As can be expected, in real life scenarious the configurations connecting sources to targets will vary based on the requirements of the various actors.

## Configurations

A pipeline can consume data from multiple providers, and once it is finised processing them, can direct some of the processed data items to a specific consumer in a _one-to-one_ scheme:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	provider1[("`**Provider 1**
    A
    B
  `")]
	provider2[("`**Provider 2**
    C
  `")]
 	pipeline{"`**Pipeline**`"}
	consumer["`**Consumer**
    B
    C
  `"]
	provider1 --> pipeline;
	provider2 --> pipeline;
  pipeline --> consumer
```

It can also consume data from a single provider and deliver the processed results to multiple consumers:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	provider[("`**Provider**
    A
    B
    C
  `")]
 	pipeline{"`**Pipeline**`"}
	consumer1["`**Consumer 1**
    A
    C
  `"]
	consumer2["`**Consumer 2**
    B
  `"]
	provider --> pipeline;
  pipeline --> consumer1;
  pipeline --> consumer2;

```

Finally, and in most real-life situations, we will have multiple providers and multiple consumers directed to each other in complex arrangements:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	provider1[("`**Provider 1**
    A
    B
  `")]
	provider2[("`**Provider 2**
    C
  `")]
	provider3[("`**Provider 3**
    D
  `")]
 	pipeline{"`**Pipeline**`"}
	consumer1["`**Consumer 1**
    A
    C
  `"]
	consumer2["`**Consumer 2**
    A
    B
    D
  `"]
	provider1 --> pipeline;
	provider2 --> pipeline;
	provider3 --> pipeline;
  pipeline --> consumer1;
  pipeline --> consumer2;

```

Effectively, each source may be the target of an upstream pipeline, and each target may serve as the source for a downstream one. The detail to keep in mind is that each **Provider** side is delegating some processing to the next pipeline based on some established requirements of the **Consumer** side. The pipeline is acting as the middleman for the data interchange. Even if it routes _some_ data items without any processing, this should be due to the demands of the **Consumer** side, so the pipeline is performing a meaningful role by at least fulfilling a configured routing policy.

## Telemetry

In telemetry, these basic configurations are clustered according to a scheme where the primary provider is named **Source** and the ultimate consumer is named **Target** or **Destination**.

There is, however, a middle stage&mdash;where the software solutions do their real work&mdash;called **Routing**.

Until now, we have not mentioned two interim stages&mdash;which also act as both consumers and providers&mdash;requried to link those stages in a chain. These are called **Pre-processing** and **Post-processing**, respectively, and they both involve what we have referred to above as [normalization](./normalization.mdx)&mdash;i.e. preparing the data for processing downstream.

Pipelines are, once again, our primary tool at these interim stages. Schematically:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	source1["`Source 1`"]
	source2["`Source 2`"]
	source3["`Source 3`"]
  preprocess["`Preprocess`"]
	route1["`Route 1`"]
	route2["`Route 2`"]
	route3["`Route 3`"]
	route4["`Route 4`"]
  postprocess["`Postprocess`"]
	target1["`Target 1`"]
	target2["`Target 2`"]

	source1 --> preprocess;
	source2 --> preprocess;
	source3 --> preprocess;
	preprocess --> route1;
	preprocess --> route2;
	preprocess --> route3;
	preprocess --> route4;
	route1 --> postprocess;
	route2 --> postprocess;
	route3 --> postprocess;
	route4 --> postprocess;
  postprocess --> target1;
  postprocess --> target2
```

As can be seen from the graphics, the components in the middle of this setup are actually called [routes](../routes/overview.mdx).

The purpose of these simple illustrations is to emphasize one thing: the primary purpose of pipelines is using _division of labor_ to simplify telemetry.

Let us illustrate.

## Data Streams

In any telemetry operation, the incoming raw data streams will naturally have their own structure. This structure, however, may not&mdash;and frequently _does not_&mdash;lend itself to use for specific purposes, such as analysis or transformation. Trying to do so would be time consuming, processor-intensive, and possibly error prone. Therefore, it is unwise to implement a _curating_ strategy on the **Source** side, i.e. where the data originates. Similarly, if the curated data needs to be transformed or enriched as per the requirements of a downstream consumer, it is unwise to try to delegate these transformation tasks to the **Target** side, i.e. where the data items to be consumed are expected to be in an easily consumable format.

And these are exactly the places where pipelines enter the picture.

## Divide and Rule

A pipeline is essentially based on the principle of _divide-and-rule_: divide the operations to be performed on the available data into the smallest meaningful and coherent actions, sequence them in a logical order based on the expected _inputs_ and the generated _outputs_, and run them as a chain.

The basic layout of the configuration for a typical pipeline looks like so:

```yaml
pipeline:
  description: "What the pipeline does"
  processors:
    - processor-1:
        field: foo
        value: "A"
    - processor-2:
        description: "What processor-2 does"
        field: bar
        value: true
    - processor-3:
        field: baz
        value: 10
```

This pipeline is created based on the assumption that the incoming data stream will have the following structure:

```yaml
data:
  - foo: ["A", "B", "C", ...]
  - baz: [5, 10, 1, ...]
  - bar: true
```

Note that the pipeline definition contains a mixture of _required_ and _optional_ fields, and that&mdash;in this form&mdash;it disregards the order of fields in the source data. All that it cares about is that

* the enumerated fields exist, and
* they contain the expected types of data

## Simplified Functionality

Let us point out here that the above pipeline may appear incomplete: it does _not_ explicitly specify what will happen to the selected values when the processors are run. Assume that, after this pipeline has been run, the data appears like so:

```yaml
data:
  - foo: ["Z", "B", "C", ...]
  - baz: [5, 100, 1, ...]
   -bar: true
```

With a brief glance, one can notice that the value `A` in `foo` has become `Z`, that the `10` in `baz` has become `100`, whereas `bar` is left untouched since it was already `true`.

Although the pipeline appears underspecified, the example is intended to illustrate that what a sequence of processors does is as simple as this. Each performs a single operation on a specific field, and that is the pipeline's power. Once the incoming data is [normalized](./normalization.mdx)&mdash;i.e. converted into a structure making it easy to extract data items from it&mdash; it becomes very easy to curate, transform, and enrich its values into a desirable form on the fly.

Let us now explain the concepts of _curation_, _transformation_, and _enrichment_ a bit further.

## Stream Processing

As we have said earlier, the incoming raw data streams will have their own structure. As such, they will at best be partially suitable for analysis and, therefore, decision making. In its raw form, data is frequently not&mdash;at least may not be&mdash;what it seems to be. It has to be normalized and sifted through.

The process of selecting or filtering data based on specific criteria is called _curation_. This involves checking whether a field's value matches or contains specific values or fragments of values.

After this is done, the selected data may need to be converted into forms making them more suitable for analysis and consumption. This second phase is called _transformation_.

Finally, the data may contain hints or fragments of information which, when correlated with other available data, may yield insights that may be required for analysis and use. The process of adding correlated information in order to render the data more relevant&mdash;or increase its relevance&mdash;is known as _enrichment_.

The overall process can be represented schematically as follows:

```mermaid
%%{init: {"flowchart": {"htmlLabels": false}}}%%
flowchart LR
	source1[("`**Source 1**
		A
		B
	`")]
	source2[("`**Source 2**
		C
	`")]
  routing{"`**Routing**
    D = curate(A, B, C)
		E = enrich(transform(D))
	`"}
	target["`**Target**
		E
	`"]
	source1 --> routing;
	source2 --> routing;
	routing --> target
```

It is through this three-fold process that a pipeline truly shines and becomes indispensible.


## Typical Use

It is best to keep the following in mind when creating pipelines.

Processors are designed to select data points by a combination of field values. Their primary design principle, as stated above, is to serve one purpose only to streamline the work they do.

This naturally has an impact on performance: the fewer the data points or field values a processor has to act upon, the better the overall performance. If processor **p** operates on field `foo` only when its values is `A`, whereas processor **q** operates on `bar` if its value is `B`, using the two in sequence guarantees that each completes its work in a shorter time.

---

In the chapters below, you will find a wide palette of processors designed to meet your needs for creating powerful and efficient pipelines.

:::note
While reading the synopses, use the [Appendix](/docs/appendix.mdx#synopsis-type-specifiers) to interpret the type specifiers.
:::
