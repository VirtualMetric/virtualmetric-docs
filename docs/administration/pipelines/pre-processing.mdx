# Pre-processing

This is the stage at which pipelines attached to a source perform some operations to prepare the data for downstream stages such as indexing or analysis.

Pre-processing helps organizations optimize their data ingestion, reduce costs, and improve the quality of the data before it reaches storage or analysis systems.

## Key Benefits

### Data Reduction

Data reduction lowers downstream costs and improves performance by filtering unnecessary events and fields, sampling representative subsets of data, deduplicating redundant events, removing unnecessary metadata and fields, and aggregating to combine similar events.

**Example** - Reducing log volumes:

```yaml
pipeline:
  processors:
    - filter:
        condition: "severity > 'info'"
    - sampling:
        rate: 10
        condition: "event_type == 'debug'"
    - drop_fields:
        fields: ["debug_info", "internal_id"]
```

### Normalization

This involves converting diverse log formats into consistent, structured schemas: vendor-specific fields are mapped to common ones, data is converted to and from ECS, CIM, ASIM, etc. formats, and timestamps, IP address formats, and protocol fields are normalized unified accross sources.

### Enrichment

The purpose here is to add context and value to raw data by inferring geolocation from IP addresses, generating threat intelligence from the security context, create asset information by adding system and user information, resolve hostnames and domains via DNS lookups, and add business-specific metadata through custom lookups.

**Example** - Normalizing security events:

```yaml
pipeline:
  processors:
    - normalize:
        target_format: "ecs"
    - geoip:
        source: "source.ip"
        target: "source.geo"
    - lookup:
        file: "threat_intel.csv"
        match_field: "source.ip"
```

### Transformation

This involves modifying data to meet security and compliance requirements by redacting Personally Identifiable Information (PII), protecting confidential data through field encryption, maintaining compatibility with format conversion, parsing complex messages with field extraction, and ensuring data quality with data validation.

**Example**: Protecting personally identifiable information:

```yaml
pipeline:
  processors:
    - mask:
        regex: "\b\d{16}\b"
        replacement: "XXXX-XXXX-XXXX-****"
    - mask:
        regex: "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b"
        replacement: "email@*****"
```

## Best Practices

For pre-processing pipelines, the following guidelines should be observed:

### Sequencing

Put high-impact filters first, and perform heavy enrichment last. Group similar operations.

### Optimization

Use sampling for high-volume sources. Cache lookup results, and minimize regex use.

:::warning
Complex operations can impact throughput. Monitor resource use, and adjust pipeline complexity accordingly.
:::

### Data Quality

Validate data after transformations. Monitor the dropped events, and track field presence.

### Maintenance

Document the transformations. Monitor pipeline performance, review the sampling rates, and update the enrichment data regularly.

:::tip
Start with minimal processing and add steps incrementally while monitoring their impact on performance and storage
:::
