---
sidebar_label: Draft Quick Start
---

# Quick Start: Building Your First Pipeline

This chapter will guide you through creating your first data processing pipeline using three basic processors. By the end, you'll understand how to combine processors to transform your data effectively.

## Overview

In this quick start guide, we'll build a pipeline that:

1. Reads data from a CSV file
2. Filters out invalid records
3. Enriches the data with additional information

## Prerequisites

Before starting, ensure you have:

- The pipeline framework installed
- Basic understanding of data processing concepts
- Sample CSV data file

## Creating the Pipeline

Let's build our pipeline step by step using three essential processors:

### Step 1: Set Up the CSV Reader Processor

The CSVReader processor handles the initial data ingestion:

```python
from pipeline.processors import CSVReader

csv_reader = CSVReader(
    input_path="data/sales.csv",
    headers=["date", "product_id", "quantity", "price"]
)
```

### Step 2: Add Data Validation

Next, we'll add the DataValidator processor to filter out invalid records:

```python
from pipeline.processors import DataValidator

validator = DataValidator(
    validations={
        "quantity": lambda x: x > 0,
        "price": lambda x: x > 0 and x < 10000
    }
)
```

### Step 3: Implement Data Enrichment

Finally, we'll use the DataEnricher processor to add product information:

```python
from pipeline.processors import DataEnricher

enricher = DataEnricher(
    lookup_table="data/products.csv",
    lookup_key="product_id",
    fields_to_add=["product_name", "category"]
)
```

### Step 4: Combine Processors into a Pipeline

Now let's connect these processors to create our pipeline:

```python
from pipeline import Pipeline

pipeline = Pipeline(
    name="sales_processing",
    processors=[
        csv_reader,
        validator,
        enricher
    ]
)

# Execute the pipeline
results = pipeline.execute()
```

## Understanding the Flow

1. The CSVReader loads the sales data
2. The DataValidator filters out records with invalid quantities or prices
3. The DataEnricher adds product details to each valid record

## Example Output

Before enrichment:

```csv
date,product_id,quantity,price
2024-01-15,A123,5,29.99
```

After pipeline processing:

```csv
date,product_id,quantity,price,product_name,category
2024-01-15,A123,5,29.99,Premium Widget,Electronics
```

## Best Practices

- Always validate your data early in the pipeline
- Keep processors focused on single responsibilities
- Use meaningful names for your pipeline and processors
- Handle errors appropriately at each stage

## Next Steps

Now that you've created your first pipeline, you can:

- Add more processors for complex transformations
- Implement error handling
- Add logging and monitoring
- Optimize performance

## Troubleshooting

Common issues you might encounter:

- Missing input files: Ensure all required data files exist
- Invalid data formats: Check your CSV structure matches expected headers
- Memory constraints: Monitor resource usage for large datasets

Remember that each processor can be configured independently, allowing you to fine-tune the pipeline for your specific needs.
