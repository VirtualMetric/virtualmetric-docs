---
sidebar_label: Quick Start
---

# Pipelines: Quick Start

Creating a pipeline involves two key factors:

**Ingestion Source** The data's origin. Pipelines must handle data characteristics determined by the source.

**Configuration** The processor arrangement. Pipelines must be configured to meet specific output objectives.

A pipeline has an _input_ and an _output_. Processor selection and configuration depend on what is consumed and produced.

## Design Considerations

Key aspects of pipeline design include the _sequential relations_ and _interactions_ between processors.

:::note
Pipeline design is iterative. Start simple and refine as you better understand specific requirements.
:::

### Order and Dependency

Consider processor relationships:

* **Independent Execution** - Processors run simultaneously without dependencies.
  
  ```yaml
  pipelines:
    processors:
      - parser
      - enricher
  ```

* **Sequential Execution** - A processor’s output serves as another’s input.
  
  ```yaml
  pipelines:
    processors:
      - parser
      - normalizer
      - enricher
  ```

* **Conditional Execution** - Processors run based on conditions.
  
### Interaction Patterns

Pipelines interact in different ways:

* **Parallel Execution** - Pipelines run independently.

  ```yaml
  pipelines:
    - name: network_logs
      processors:
        - network_parser
        - network_enricher
    - name: security_logs
      processors:
        - security_parser
        - threat_detector
  ```

* **Triggered Execution** - A pipeline starts after another completes.

  ```yaml
  pipelines:
    - name: primary
      processors:
        - initial_parser
      on_complete:
        trigger: secondary
    - name: secondary
      processors:
        - advanced_enrichment
  ```

* **Hierarchical Execution** - Pipelines execute in a structured order.
  
## Best Practices

### Purpose of Use

Design pipelines according to function:

**Pre-processing** pipelines filter, normalize, and enrich data before routing.

**Normalization** pipelines standardize log formats, ensuring consistency.

**Post-processing** pipelines finalize data for storage and integration.

Keep pipelines focused, modular, and efficient. Optimize performance by handling intensive transformations early, using type-specific metrics, and implementing clear error boundaries.

### Sequencing

Processor sequencing impacts performance. Minimize unnecessary operations and avoid premature processing. Use only essential processors in the correct order.

### Modularity

Reusability improves efficiency. Keep transformations focused and avoid excessive complexity. Ensure all processors serve a clear purpose.

### Volume Handling

Design pipelines to scale. Inefficient designs become evident with high data volume.

### Data Integrity

Ensure consistent data typing, validation, and handling of edge cases. Normalize format variations to maintain reliability.

### Optimization

* **Parallel processing** - Modular pipelines enable concurrency.
* **Streamlined transformations** - Keep operations relevant to the pipeline's goal.
* **Reduced complexity** - Optimize processor order to minimize computational burden.
* **Incremental development** - Test and refine at every stage.

### Failures

Implement robust error handling, particularly for resource-intensive computations. A well-structured logging mechanism aids in troubleshooting and efficiency.

A well-designed pipeline minimizes waste, maximizes modularity, and ensures streamlined processing.
