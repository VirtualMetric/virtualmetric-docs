---
sidebar_label: Quick Start
---

# Pipelines: Quick Start

Creating a pipeline requires a systematic approach that primarily involves two key factors:

**Ingestion Source** The origin of the data. Pipelines have to be designed to be able to handle data with specific characteristics that will be determined by the source

**Configuration** The specific arrangement of the processors. Pipelines need to be configured to meet certain objectives in their output

In other words, the pipeline has an _input_ and an ultimate _output_, and the selection and configuration of the processors that make up the pipeline are dictated by what is to be consumed and what is to be produced.

## Design Considerations

When designing a pipeline, a number of key aspects need to be considered: the _sequential relations_ between the processors, and the _types of interactions_ anticipated to take place between them.

:::note
Pipeline design is an iterative process. Always start simple and progressively improve your configuration as you better grasp the requirements of your specific use cases.
:::

### Order and Dependency

The first thing to consider is the relations between the processors. There are three possibilities:

* Run simultaneously without relying on each other's output

  ```mermaid
  ---
  title: Independent Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 2
      A("Processor 1"):2
      B("Processor 2"):2
      C("..."):2
  ```

  **Example**

  The `parser` and `enricher` processors run independently:

  ```yaml
  pipelines:
      processors:
        - parser
        - enricher
  ```

* Use the output of a previous one as their input

  ```mermaid
  ---
  title: Sequential Flow
  ---
  graph LR
  ```

  ```mermaid
    block-beta
      columns 7
      A("Processor 1"):2
      space
      B("Processor 2"):2
      space
      C("..."):1
      A --> B
      B --> C
  ```

  **Example**

  The `normalizer` processor uses the output of `parser`, and the `enricher` processor uses normalized data:

  ```yaml
  pipelines:
      processors:
        - parser
        - normalizer
        - enricher
  ```

* Run based on specific conditions, such as when the result of a previous one meets certain criteria or when a computation completes:

  ```mermaid
  ---
  title: Conditional Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
      columns 5
      A("Processor 1"):2
      space
      B("Processor 2"):2
      space
      space
      space
      C("Processor 3"):2
      D("Processor 4"):2
      space
      E("..."):1

      A -- "Yes?" --> B
      A -- "No?" --> C
      D -- "Ready!" --> E
  ```

### Interaction Patterns

The next on the list is the interactions between the pipelines. Real-world scenarios often require complex exchanges between them. There are three possible layouts:

* Run simultaneously:

  ```mermaid
  ---
  title: Parallel Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    block
      columns 2
      A("Pipeline A"):1
      B("Pipeline B"):1
      C("Pipeline C"):2
    end
  ```
  
  **Example**
  
  The `network_logs` and `security_logs` pipelines run independently, and so they can run simultaneously:

  ```yaml
  pipelines:
    - name: network_logs
      processors:
        - network_parser
        - network_enricher
    
    - name: security_logs
      processors:
        - security_parser
        - threat_detector
  ```

* Trigger one another upon completion:

  ```mermaid
  ---
  title: Sequential Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 7
    A("Pipeline A"):2
    space
    B("Pipeline B"):2
    space
    C("Pipeline C"):2
    A --> B
    B --> C
  ```

  **Example**

  The `secondary` pipeline is triggered by the `primary` pipeline:

  ```yaml
  pipelines:
    - name: primary
      processors:
        - initial_parser
      on_complete:
        trigger: secondary

    - name: secondary
      processors:
        - advanced_enrichment
  ```

* Run based on a pre-defined hierarchical order, and potentially relay data:

  ```mermaid
  ---
  title: Relay Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 5
    A("Pipeline A"):2
    space
    B("Pipeline B"):2
    C("Pipeline C"):2
    space
    D("Pipeline D"):2
    A --> B
    B --> C
    C --> D
  ```

## Best Practices

Finally, let us consider a few guidelines for designing effective and efficient pipelines.

### Modularity

Reusability is one of the ever-present requirements of design. 
  
In the context of pipelines, this means focusing on specific transformations. If, for example, a string field needs to be stripped of formatting tags before extracting a certain value, it is best to keep these two together.

In addition, complex pipelines are likely to bring an overhead which, if not handled carefully, may degrade performance. Therefore, it is essential to check whether every processor included is absolutely essential for its primary task. Keep in mind that performance is also related to modularity.

### Volume

Anticipate handling varying data volumes.

Pipelines really shine at scale. It is best to keep in mind handling large amounts of data, a consideration which may expose inefficient design choices.

### Data Integrity

This requires consistent data typing across the processors, implementing validation steps, and handling edge cases and unexpected input formats. 

The most common challenge in this regard is **format variations**. Make sure that you have paid   enough attention to normalization.

### Optimization

A few rules of thumb:

* Use **parallel processing** where possible

  If your pipelines are modular enough, you should not have any difficulty running them simultaneously. Conversely, if you want to be able to do as much parallel processing as possible, mind the modularity of your pipelines.

* Streamline **data transformations**

  Unless directly relevant to its goal, do not to include a transformation in a pipeline. Managing intricate manipulations is always a challenge. Make sure that the pipeline serves a very clear and specific purpose.

* Reduce **computational complexity**

  The key to achieving this is choosing the appropriate processor order. A sloppy design, i.e. not paying attention to the input-output sequence, may increase the computational burden of a pipeline in unexpected ways.

* Build **incrementally** and **iteratively**

  Always review the available processors and their specific configurations first before embarking on a design, and at every step _test_ your design.

### Failures

Last but not the least, always implement robust error handling.

This is particularly relevant to expensive computations repeating which is likely to consume extra system resources and therefore may become wasteful.

Given the above, a well-designed logging mechanism is likely to be a good aid in the implementation of error handling.

---

# Building Your First Pipeline

We will now guide you through creating your first data processing pipeline using three basic processors. By the end, you'll understand how to combine processors to transform your data effectively.

## Overview

We will build a pipeline that:

1. Reads data from a CSV file
2. Filters out invalid records
3. Enriches the data with additional information

## Prerequisites

Before starting, ensure you have:

- The pipeline framework installed
- Basic understanding of data processing concepts
- Sample CSV data file

## Creating the Pipeline

Let's build our pipeline step by step using three essential processors:

### Step 1: Set Up the CSV Reader Processor

The CSVReader processor handles the initial data ingestion:

```yaml
processors:
  - name: csv_reader
    type: CSVReader
    config:
      input_path: data/sales.csv
      headers:
        - date
        - product_id
        - quantity
        - price
```

### Step 2: Add Data Validation

Next, we'll add the DataValidator processor to filter out invalid records:

```yaml
  - name: validator
    type: DataValidator
    config:
      validations:
        quantity: "value > 0"
        price: "value > 0 and value < 10000"
```

### Step 3: Implement Data Enrichment

Finally, we'll use the DataEnricher processor to add product information:

```yaml
  - name: enricher
    type: DataEnricher
    config:
      lookup_table: data/products.csv
      lookup_key: product_id
      fields_to_add:
        - product_name
        - category
```

### Step 4: Complete Pipeline Configuration

Here's the complete pipeline configuration combining all processors:

```yaml
pipeline:
  name: sales_processing
  version: "1.0"
  description: "Process sales data with validation and enrichment"
  
  processors:
    - name: csv_reader
      type: CSVReader
      config:
        input_path: data/sales.csv
        headers:
          - date
          - product_id
          - quantity
          - price
    
    - name: validator
      type: DataValidator
      config:
        validations:
          quantity: "value > 0"
          price: "value > 0 and value < 10000"
    
    - name: enricher
      type: DataEnricher
      config:
        lookup_table: data/products.csv
        lookup_key: product_id
        fields_to_add:
          - product_name
          - category
```

## Understanding the Flow

1. The CSVReader loads the sales data
2. The DataValidator filters out records with invalid quantities or prices
3. The DataEnricher adds product details to each valid record

## Example Output

Before enrichment:
```
date,product_id,quantity,price
2024-01-15,A123,5,29.99
```

After pipeline processing:
```
date,product_id,quantity,price,product_name,category
2024-01-15,A123,5,29.99,Premium Widget,Electronics
```

## Best Practices

- Keep YAML configurations clean and well-indented
- Use descriptive names for processors and their configurations
- Document any custom validation expressions
- Version your pipeline configurations
- Store configurations in version control

## Next Steps

Now that you've created your first pipeline, you can:

- Add more processors for complex transformations
- Implement custom validation rules
- Add environment-specific configurations
- Set up monitoring and logging configurations

## Troubleshooting

Common issues you might encounter:

- YAML syntax errors: Ensure proper indentation and formatting
- Missing configuration fields: Check processor documentation for required fields
- Invalid validation expressions: Test expressions independently
- File path issues: Verify paths are relative to configuration location

Remember that each processor's configuration can be adjusted independently, allowing you to fine-tune the pipeline for your specific needs. Store your configurations in a `.yml` file and use the pipeline runner to execute them.
