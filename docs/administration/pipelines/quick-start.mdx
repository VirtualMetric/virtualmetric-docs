---
sidebar_label: Quick Start
---

# Pipelines: Quick Start

Creating a pipeline requires a systematic approach that primarily involves two key factors:

**Ingestion Source** The origin of the data. Pipelines have to be designed to be able to handle data with specific characteristics that will be determined by the source

**Configuration** The specific arrangement of the processors. Pipelines need to be configured to meet certain objectives in their output

In other words, the pipeline has an _input_ and an ultimate _output_, and the selection and configuration of the processors that make up the pipeline are dictated by what is to be consumed and what is to be produced.

## Design Considerations

When designing a pipeline, a number of key aspects need to be considered: the _sequential relations_ between the processors, and the _types of interactions_ anticipated to take place between them.

:::note
Pipeline design is an iterative process. Always start simple and progressively improve your configuration as you better grasp the requirements of your specific use cases.
:::

### Order and Dependency

The first thing to consider is the relations between the processors. There are three possibilities:

* Run simultaneously without relying on each other's output

  ```mermaid
  ---
  title: Independent Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 2
      A("Processor 1"):2
      B("Processor 2"):2
      C("..."):2
  ```

  **Example**

  The `parser` and `enricher` processors run independently:

  ```yaml
  pipelines:
      processors:
        - parser
        - enricher
  ```

* Use the output of a previous one as their input

  ```mermaid
  ---
  title: Sequential Flow
  ---
  graph LR
  ```

  ```mermaid
    block-beta
      columns 7
      A("Processor 1"):2
      space
      B("Processor 2"):2
      space
      C("..."):1
      A --> B
      B --> C
  ```

  **Example**

  The `normalizer` processor uses the output of `parser`, and the `enricher` processor uses normalized data:

  ```yaml
  pipelines:
      processors:
        - parser
        - normalizer
        - enricher
  ```

* Run based on specific conditions, such as when the result of a previous one meets certain criteria or when a computation completes:

  ```mermaid
  ---
  title: Conditional Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
      columns 5
      A("Processor 1"):2
      space
      B("Processor 2"):2
      space
      space
      space
      C("Processor 3"):2
      D("Processor 4"):2
      space
      E("..."):1

      A -- "Yes?" --> B
      A -- "No?" --> C
      D -- "Ready!" --> E
  ```

### Interaction Patterns

The next on the list is the interactions between the pipelines. Real-world scenarios often require complex exchanges between them. There are three possible layouts:

* Run simultaneously:

  ```mermaid
  ---
  title: Parallel Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    block
      columns 2
      A("Pipeline A"):1
      B("Pipeline B"):1
      C("Pipeline C"):2
    end
  ```
  
  **Example**
  
  The `network_logs` and `security_logs` pipelines run independently, and so they can run simultaneously:

  ```yaml
  pipelines:
    - name: network_logs
      processors:
        - network_parser
        - network_enricher
    
    - name: security_logs
      processors:
        - security_parser
        - threat_detector
  ```

* Trigger one another upon completion:

  ```mermaid
  ---
  title: Trigger Order
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 7
    A("Pipeline A"):2
    space
    B("Pipeline B"):2
    space
    C("Pipeline C"):2
    A --> B
    B --> C
  ```

  **Example**

  The `secondary` pipeline is triggered by the `primary` pipeline:

  ```yaml
  pipelines:
    - name: primary
      processors:
        - initial_parser
      on_complete:
        trigger: secondary

    - name: secondary
      processors:
        - advanced_enrichment
  ```

* Run based on a pre-defined hierarchical order, and potentially relay data:

  ```mermaid
  ---
  title: Relay Order
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 5
    A("Pipeline A"):2
    space
    B("Pipeline B"):2
    C("Pipeline C"):2
    space
    D("Pipeline D"):2
    A --> B
    B --> C
    C --> D
  ```

## Best Practices

Finally, let us consider a few guidelines for designing effective and efficient pipelines.

### Purpose of Use

It is essential to be cognizant of the _type_ of pipeline that is being designed.

**Pre-processing** pipelines are attached to _sources_, and prepare the data before it enters the routing stage. They focus on:

- _Data reduction_ - Filtering unnecessary events, removing redundant fields, sampling high-volume data, and aggregating similar events
- _Initial normalization_ - Field and protocol standardization, and format conversion and time normalization
- _Early enrichment_ - Geolocation data, asset information, basic threat intelligence, and custom metadata

**Normalization** pipelines handle the conversion between different log formats throughout the processing chain. The primary transformations are field name standardization, data type normalization, structure unification, and time format alignment.

**Post-processing** pipelines are attached to _targets_, and perform final transformations before data storage and analysis:

- _Format finalization_ - Target-specific formatting, schema alignment, and final field mapping
- _Storage optimization_ - Compression configuration, index preparation, partitioning strategy, and retention setup
- _Integration_ - Target-specific transformations, protocol adaptation, authentication preparation, and error handling

Keep the pipelines focused, and minimize cross-type dependencies.

Leverage the seperation of concerns this brings as it clarifies the role of each, enabling a modular architecture, which, in turn, reduces overhead through better resource utilization, improves scalability and routing efficiency, and makes robust testing possible.

For _performance optimization_, deal with heavy transformations early, optimize routing decisions, and monitor type-specific metrics.

For _error handling_, implement stage-appropriate ones, use type-specific failure responses, maintain clear error boundaries, and log errors with context.

### Sequencing

This naturally has an impact on performance: the fewer the data points or field values a processor has to act upon, the better the overall performance. If processor **p** operates on field `foo` only when its values is `A`, whereas processor **q** operates on `bar` if its value is `B`, using the two in sequence guarantees that each completes its work in a shorter time.

However, this implies that their overuse is likely to cancel whatever benefits they may offer, and their sequencing has to be carefully considered, particularly if the output of one is fed into the next as input.

Additionally, their selection will generally be dictated by the consumers downstream. This is where premature optimization, as usual, can be a source of frustration.

Therefore, the guiding principles should be

* use only the processors directly relevant to the curation process
* use them in the correct order
* avoid any premature processing

### Modularity

Reusability is one of the ever-present requirements of design. 

In the context of pipelines, this means focusing on specific transformations. If, for example, a string field needs to be stripped of formatting tags before extracting a certain value, it is best to keep these two together.

In addition, complex pipelines are likely to bring an overhead which, if not handled carefully, may degrade performance. Therefore, it is essential to check whether every processor included is essential for its primary task. Keep in mind that performance is also related to modularity.

### Volume

Anticipate handling varying data volumes.

Pipelines really shine at scale. It is best to keep in mind handling large amounts of data, a consideration which may expose inefficient design choices.

### Data Integrity

This requires consistent data typing across the processors, implementing validation steps, and handling edge cases and unexpected input formats. 

The most common challenge in this regard is **format variations**. Make sure that you have paid   enough attention to normalization.

### Optimization

A few rules of thumb:

* Use **parallel processing** where possible

  If your pipelines are modular enough, you should not have any difficulty running them simultaneously. Conversely, if you want to be able to do as much parallel processing as possible, mind the modularity of your pipelines.

* Streamline **data transformations**

  Unless directly relevant to its goal, do not to include a transformation in a pipeline. Managing intricate manipulations is always a challenge. Make sure that the pipeline serves a clear and specific purpose.

* Reduce **computational complexity**

  The key to achieving this is choosing the appropriate processor order. A sloppy design, i.e. not paying attention to the input-output sequence, may increase the computational burden of a pipeline in unexpected ways.

* Build **incrementally** and **iteratively**

  Always review the available processors and their specific configurations first before embarking on a design, and at every step _test_ your design.

### Failures

Lastly, always implement robust error handling.

This is particularly relevant to expensive computations repeating which is likely to consume extra system resources and therefore may become wasteful.

Given the above, a well-designed logging mechanism is likely to be a good aid in the implementation of error handling.

Processors are designed to select data points by a combination of field values. Their primary design principle, as stated above, is to serve one purpose only to streamline the work they do.
