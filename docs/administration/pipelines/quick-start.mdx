---
sidebar_label: Quick Start
---

# Pipelines: Quick Start

Creating a pipeline requires a systematic approach that primarily involves two key considerations:

**Ingestion Source** The origin of the data. Pipelines have to be designed to be able to handle data with specific characteristics which will be determined by the source

**Configuration** The specific arrangement of the processors. Pipelines need to be configured to to meet certain objectives in their output

In other words, the pipeline has an _input_ and an ultimate _output_, and the selection and configuration of the processors that make up the pipeline are dictated by what is to be consumed and and what is to be produced.

## Design Considerations

When designing a pipeline, a number of key aspects need to be considered:. These are, the _sequential relations_ between the processors, and how the types of _interactions_ anticipated to take place between them.

### Order and Dependency

First, the relations between the processors. There are three possibilities:

* Run simultaneously without relying on each other's output

  ```mermaid
  ---
  title: Independent Flow
  ---
  graph LR
      A(Processor 1...)
      B(Processor 2...)
      C(Processor 3...)
  ```

* Use the output of a previous one as their input

  ```mermaid
  ---
  title: Sequential Flow
  ---
    graph LR
      A(Processor 1...)
      B(Processor 2...)
      C(Processor 3...)
      A --> B
      B --> C
  ```

* Run based on specific conditions, such as when the result of a previous one meets certain criteria or when a computation completes:

  ```mermaid
  ---
  title: Conditional Flow
  ---
  flowchart LR
      A(Processor 1)
      B(Processor 2)
      C(Processor 3...)
      D(Processor 4...)
      E(Processor 5...)
      A --> |"**Yes?**"| C
      A --> |"**No?**"| E
      B --> |"**Done**"| D
  ```

### Interaction Patterns

Second, real-world scenarios often require complex pipeline interactions. There are three possible layouts:

* Run simultaneously:

  ```mermaid
  ---
  title: Parallel Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    block
      columns 2
      A("Pipeline A\n..."):1
      B("Pipeline B\n..."):1
      C("Pipeline C\n..."):2
    end
  ```

* Trigger one another upon completion:

  ```mermaid
  ---
  title: Sequential Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    columns 5
    block
      A("Pipeline A\n..."):1
      space
      B("Pipeline B\n..."):1
      space
      C("Pipeline C\n..."):1
    end
    A --> B
    B --> C
  ```

* Run based on a pre-defined hierarchical order, and potentially relay data:

  ```mermaid
  ---
  title: Relay Flow
  ---
  graph LR
  ```

  ```mermaid
  block-beta
    block
      columns 4
        A("Pipeline A\n..."):1
        space
        B("Pipeline B\n..."):2
        space
        C("Pipeline C\n..."):2
        space
        D("Pipeline D\n..."):1
    end
    A --> B
    B --> C
    B --> D
  ```

## Typical Configurations

Below you can find some examples of typical pipeline configurations.

### Example 1: Independent Processors

```yaml
pipelines:
    processors:
      - parser    # Processor 1
      - enricher  # Processor 2 (runs independently)
```

### Example 2: Dependent Processors

```yaml
pipelines:
    processors:
      - parser
      - normalizer  # Uses output from parser
      - enricher    # Uses normalized data
```

### Example 3: Multiple Independent Pipelines

```yaml
pipelines:
  - name: network_logs_pipeline   # runs independently
    processors:
      - network_parser
      - network_enricher
  
  - name: security_logs_pipeline  # runs independently
    processors:
      - security_parser
      - threat_detector
```

### Example 4: Sequential Pipeline Triggering

```yaml
pipelines:
  - name: primary_pipeline
    processors:
      - initial_parser
    on_complete:
      trigger: secondary_pipeline

  - name: secondary_pipeline
    processors:
      - advanced_enrichment
```

## Best Practices

1. **Modularity**: Design processors to be reusable and focused on specific transformations
2. **Performance**: Consider the computational overhead of complex pipeline designs
3. **Error Handling**: Implement robust error handling and logging mechanisms
4. **Scalability**: Design pipelines that can handle varying data volumes

## Considerations for Complex Scenarios

### Performance Optimization

- Use parallel processing where possible
- Minimize unnecessary data transformations
- Choose appropriate processor order to reduce computational complexity

### Data Integrity

- Ensure consistent data typing across processors
- Implement validation steps
- Handle edge cases and unexpected input formats

## Common Challenges

- **Data Format Variations**: Handling diverse input formats
- **Performance Bottlenecks**: Identifying and optimizing slow processors
- **Complex Transformation Logic**: Managing intricate data manipulation requirements

## Next Steps

- Review available processors and their specific configurations
- Design your pipeline based on your specific data processing requirements
- Test and iterate on your pipeline design

:::note
Pipeline design is an iterative process. Start simple and progressively enhance your configuration as you understand your specific use cases.
:::
