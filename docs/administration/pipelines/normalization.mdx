# Normalization

Normalization is the stage at which log data from diverse sources are transposed into a standard format, enabling consistent and efficient log analysis across different systems.

:::warning
There is a potential risk of loss of nuance in the process. Always validate transformed logs against their originals.
:::

## Log Formats

There are a number of widely used log data format standards that various systems have adopted for marshalling data.

|Standard|Notation|Key Identifiers|Layout Characteristics|
|---|---|---|---|
|Elastic Common Schema (ECS)|Lowercase dot, e.g. `source.ip`|e.g. `@timestamp`|Hierarchical structure and nested field representation, e.g. `network.direction`|
|Splunk Common Information Model (CIM)|Lowercase underscore, e.g. `src_ip`|e.g. `_time`|Flat and concise names, e.g.`network_direction`|
|Advanced Security Information Model (ASIM)|Pascal case, e.g. `SourceIp`|e.g. `TimeGenerated`|Explicit, verbose field names, e.g. `NetworkDirection`|

The conventions used by their vendors imply that log formats can be identified through&mdash;and therefore detected by&mdash;certain characteristic fields, such as

- `@timestamp`: typically indicates ECS
- `_time`: characteristic of CIM
- `TimeGenerated`: signature of ASIM

## Challenges

The overriding challenge in normalizing logs is **variability across systems**

Different systems generate logs with unique structures, and there are no strict industry-wide norms covering all use cases. There are also context-specific nuances in log generation: field names can be inconsistent, timestamp formats may vary, data structures may be nested or flat, and different units and measurements may be used.

The following approaches may be used to deal with these challenges.

### Field Mapping

Field names can be translated between different conventions while preserving original data semantics to create a consistent data model.

### Timestamps

Timestamps can be converted to a unified format to ensure consistent time representation and handle timezone variations.

### Type Unification

Numeric representations can be standardized, boolean and categorical values can be reduced to common factors, to ensure consistent data typing.

## Best Practices

Like all other aspects of the telemetry process, some guidelines should be observed during normalization.

First, consistency is key. Choose a primary log format, define clear mapping rules around it, and document transformation logic.

Second, always preserve the original data. Keep the original fields whenever possible, add metadata about the transformations, and allow for traceability.

And third, do not skip handling the edge cases. Develop a robust error handling strategy, create fallback mechanisms, and test with diverse log samples.

:::note
Log normalization is an ongoing process. No single approach fits all scenarios perfectly.
:::
