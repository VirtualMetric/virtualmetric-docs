# Post-processing

This is the stage after routing. Pipelines attached to destinations perform final transformations and optimizations to meet the requirements of those destinations&mdash;such as making sure that the data layout enables advanced analytics.

Post-processing helps organizations reduce their costs by improving and optimizing the quality of the data before it reaches storage or analysis systems.

:::note
Post-processing is your last chance to optimize data before it reaches its destination. Choose optimizations that benefit downstream use.
:::

## Key Benefits

The motivation for using post-processing is summarized below by detailing its various aspects.

:::warning
Heavy post-processing can impact delivery latency. Monitor and adjust based on performance requirements.
:::

### Log Routing

Optimize data distribution by sending data in destination-native formats and on conditions to be met by the content or the metadata, by distributing data across multiple endpoints and thereby balancing the forwarding loads, by being able to send the same data to multiple destinations, and by having backup destinations to implement a failover mechanism.

### Storage

Reduce storage costs and improve query performance by choosing specific compression types, aliging with destination schemas, keeping only required fields, partitioning the data for certain query patterns, and sending it in more optimal formats.

**Example** Optimize for cloud storage:

```yaml
pipeline:
  processors:
    - partition:
        field: "timestamp"
        format: "year={{.Year}}/month={{.Month}}/day={{.Day}}"
    - compress:
        type: "parquet"
        schema: |
          {
            "timestamp": { "type": "INT64" },
            "message": { "type": "STRING" }
          }
```

**Example** S3 storage use:

```yaml
pipeline:
  processors:
    - partition:
        field: "timestamp"
        pattern: "year=%Y/month=%m/day=%d"
    - compress:
        format: "parquet"
        compression: "snappy"
```

**Example** Elastic stack use:

```yaml
pipeline:
  processors:
    - normalize:
        target_format: "ecs"
    - date:
        field: "timestamp"
        target_field: "@timestamp"
    - geoip:
        field: "source.ip"
        target_field: "source.geo"
```


### Queries

Prepare data for efficient querying by identifying common query patterns, creating indexes via pre-computed query fields, simplifying nested structures through field flattening, ensuring type conformity, and creating pre-aggregated views and summaries, 

**Example** Prepare for analytics:

```yaml
pipeline:
  processors:
    - aggregate:
        window: "5m"
        metrics:
          - count: "requests"
          - avg: "duration"
        dimensions: ["service", "endpoint"]
    - calculate:
        field: "error_rate"
        expression: "errors / requests * 100"
```

### Aggregation

Optimize data for analytics by aggregating time windows and key dimensions with rollups, creating event groups through correlation, and preparing statistical summaries through direct and derived metrics.

**Example** Integrate with _Microsoft Sentinel_:

```yaml
pipeline:
  processors:
    - normalize:
        target_format: "asim"
    - extract_fields:
        fields: ["EventID", "Computer", "Channel"]
    - route:
        field: "EventID"
        routes:
          security: "4624,4625"
          system: "7036,7040"
```

### Anomaly Detection

Identify patterns and anomalies by detecting common sequences, identifying outliers and flagging unusual patterns through threshold detection, establishing normal behavior through baselines, and providing trend analyses by tracking pattern changes.

**Example** Create security analysis:

```yaml
pipeline:
  processors:
    - detect_anomalies:
        field: "login_count"
        algorithm: "mad"
        window: "1h"
    - alert:
        condition: "anomaly_score > 0.9"
        channel: "security_alerts"
```

## Best Practices

The following guidelines must be observed for effective post-processing:

### Requirements

Use data formats native to the destination, optimize compression ratios, and match schemas.

### Performance

Balance aggregation windows, monitor processing latency, and always cache computed results.

### Storage

Remove unnecessary fields, use appropriate data types, and optimize partitioning schemes.

### Queries

Pre-compute common aggregations, index important fields, and structure data to make it more suitable for common queries.

:::tip
The ultimate purpose is aligning the data with the capabilities and requirements of the destination to maximize performance and minimize storage costs.
:::
