# Azure Blob Storage

## Synopsis

```yaml
- id: <numeric>
  name: <string>
  description: <string>
  type: azblob
  pipelines: <pipeline[]>
  status: <boolean>
  properties:
    account: <string>
    tenant_id: <string>
    client_id: <string>
    client_secret: <string>
    container: <string>
    name: <string>
    type: <string>
    compression: <string>
    schema: <string>
    format: <string>
    no_buffer: <boolean>
    max_retry: <numeric>
    retry_interval: <numeric>
    timeout: <numeric>
    max_size: <numeric>
```

## Description

Creates a target that writes log messages to _Azure Blob Storage_ with support for various file formats, authentication methods, and retry mechanisms. Inherits file format capabilities from the base target.

## Configuration

The following are the minimum requirements to define the target.

|Field|Required|Default|Description|
|---|---|---|---|
|`id`|Y||Unique identifier|
|`name`|Y||Target name|
|`description`|N|-|Optional description|
|`type`|Y||Must be `azblob`|
|`pipelines`|N|-|Optional post-processor pipelines|
|`status`|N|`true`|Enable/disable the target|

### Azure

|Field|Required|Default|Description|
|---|---|---|---|
|`account`|Y||Azure storage account name|
|`tenant_id`|Y||Azure tenant ID|
|`client_id`|Y||Azure client ID|
|`client_secret`|Y||Azure client secret|
|`container`|N|`"vmetric"`|Blob container name|

### Connection

|Field|Required|Default|Description|
|---|---|---|---|
|`max_retry`|N|`5`|Maximum number of upload retries|
|`retry_interval`|N|`10`|Base interval between retries in seconds|
|`timeout`|N|`30`|Connection timeout in seconds|
|`max_size`|N|`0`|Maximum file size in bytes|

:::note
When `max_size` is reached, the current file is uploaded to blob storage and a new file is created. For unlimited file size, set the field to `0`.
:::

### Files

|Field|Required|Default|Description|
|---|---|---|---|
|`name`|N|`"vmetric.{{.Timestamp}}.{{.Extension}}"`|Blob name template|
|`type`|N|`"json"`|File format (`json`, `avro`, `ocf`, `parquet`)|
|`compression`|N|`"zstd"`|Compression algorithm|
|`schema`|N|-|Data schema for Avro/OCF/Parquet formats|
|`no_buffer`|N|`false`|Disable write buffering|
|`format`|N|-|Field normalization format (`ecs`, `cim`, `asim`, `cef`, `leef`, or `csl`)|

## Examples

The following upload configurations are available.

### JSON

The minimum configuration for a _JSON_ blob storage:

```yaml
- id: 1
  name: basic_blob
  type: azblob
  properties:
    account: "mystorageaccount"
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "00000000-0000-0000-0000-000000000000"
    client_secret: "your-client-secret"
```

### Parquet

Configuration for daily partitioned _Parquet_ files:

```yaml
- id: 2
  name: parquet_blob
  type: azblob
  properties:
    account: "mystorageaccount"
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "00000000-0000-0000-0000-000000000000"
    client_secret: "your-client-secret"
    container: "logs"
    type: "parquet"
    compression: "zstd"
    name: "logs/year={{.Year}}/month={{.Month}}/day={{.Day}}/data_{{.Timestamp}}.parquet"
    max_size: 536870912  # 512MB
```

### High Reliability

Configuration with enhanced retry settings:

```yaml
- id: 3
  name: reliable_blob
  type: azblob
  pipelines:
    - checkpoint
  properties:
    account: "mystorageaccount"
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "00000000-0000-0000-0000-000000000000"
    client_secret: "your-client-secret"
    max_retry: 10
    retry_interval: 30
    timeout: 60
    no_buffer: true
```

:::warning
Failed uploads are retried automatically based on the `max_retry` and `retry_interval` settings.

The `retry interval` setting is based on exponential backoff, so each retry will wait longer than the previous one (i.e. _retry_interval_ * 2 ^ _attempt_in_seconds_).

Files are only deleted after successful upload.
:::
