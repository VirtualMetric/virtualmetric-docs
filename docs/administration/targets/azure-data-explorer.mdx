# Azure Data Explorer

## Synopsis

```yaml
- id: <numeric>
  name: <string>
  description: <string>
  type: azdx
  pipelines: <pipeline[]>
  status: <boolean>
  properties:
    tenant_id: <string>
    client_id: <string>
    client_secret: <string>
    endpoint: <string>
    database: <string>
    table: <string>
    type: <string>
    max_retry: <numeric>
    retry_interval: <numeric>
    max_size: <numeric>
```

## Description

Creates an Azure Data Explorer (Kusto) target that ingests data directly into Azure Data Explorer tables. Supports multiple data formats including Parquet, JSON, and AVRO (OCF), with configurable batch sizes and retry mechanisms.

## Configuration

The following are the minimum requirements to define the target.

### Target Properties

|Field|Required|Default|Description|
|---|---|---|---|
|`id`|Y|-|Unique identifier|
|`name`|Y|-|Target name|
|`description`|N|-|Optional description|
|`type`|Y|-|Must be `azdx`|
|`pipelines`|N|-|Optional post-processor pipelines|
|`status`|N|`true`|Enable/disable the target|

### Azure Properties

|Field|Required|Default|Description|
|---|---|---|---|
|`tenant_id`|Y|-|Azure tenant ID|
|`client_id`|Y|-|Azure client ID|
|`client_secret`|Y|-|Azure client secret|
|`endpoint`|Y|-|Azure Data Explorer cluster endpoint|
|`database`|N|`vmetric`|Target database name|
|`table`|N|`vmetric`|Target table name|
|`type`|N|`parquet`|Data format type: `parquet`, `json`, or `ocf`|

### Connection Properties

|Field|Required|Default|Description|
|---|---|---|---|
|`max_retry`|N|`5`|Maximum number of retry attempts|
|`retry_interval`|N|`10`|Base interval between retries in seconds|
|`max_size`|N|`0`|Maximum batch size in bytes (0 for unlimited)|

## Data Formats

The target supports three data formats:

1. **Parquet** (default)
   - Columnar storage format
   - Efficient compression
   - Best for analytical workloads

2. **JSON**
   - Human-readable format
   - Flexible schema
   - Good for debugging

3. **OCF** (AVRO)
   - Binary serialization
   - Schema evolution support
   - Efficient for streaming

## Examples

### Basic Configuration

The minimum required configuration for Parquet ingestion:

```yaml
- id: 1
  name: basic_adx
  type: azdx
  properties:
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "11111111-1111-1111-1111-111111111111"
    client_secret: "your-client-secret"
    endpoint: "https://cluster.region.kusto.windows.net"
```

### Custom Database and Table

Configuration with specific database and table:

```yaml
- id: 2
  name: custom_adx
  type: azdx
  properties:
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "11111111-1111-1111-1111-111111111111"
    client_secret: "your-client-secret"
    endpoint: "https://cluster.region.kusto.windows.net"
    database: "logs"
    table: "system_events"
    type: "json"
```

### High-Volume Configuration

Configuration optimized for high-volume ingestion:

```yaml
- id: 3
  name: high_volume_adx
  type: azdx
  properties:
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "11111111-1111-1111-1111-111111111111"
    client_secret: "your-client-secret"
    endpoint: "https://cluster.region.kusto.windows.net"
    type: "parquet"
    max_retry: 10
    retry_interval: 30
    max_size: 536870912  # 512MB
```

### Normalized Data Ingestion

Using data normalization before ingestion:

```yaml
- id: 4
  name: normalized_adx
  type: azdx
  pipelines:
    - normalize_logs
  properties:
    tenant_id: "00000000-0000-0000-0000-000000000000"
    client_id: "11111111-1111-1111-1111-111111111111"
    client_secret: "your-client-secret"
    endpoint: "https://cluster.region.kusto.windows.net"
    format: "ecs"  # Normalize to Elastic Common Schema
```

:::note
- The target automatically validates table existence before starting ingestion
- Data is buffered locally until `max_size` is reached or explicit flush is triggered
- Retry mechanism uses exponential backoff (retry_interval * 2^attempt)
:::

:::warning
- Make sure the service principal has appropriate permissions on the database and table
- Large batch sizes may impact memory usage during ingestion
- Consider cluster capacity when setting retry intervals
:::