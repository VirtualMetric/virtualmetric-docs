# Example: Routing Data

This section will help you get started with routes to forward data to specific destinations, walking you through some common use cases.

## Scenario

We will configure a route that receives data from two different devices, processes the data points with a pre-configured pipeline, and forwards the results to two different file targets.

## Setup

Before configuring our route, we have to make sure that the _devices_, _pipelines_, and _targets_ to be used in it are properly configured and accessible.

## Basic Relay

The simplest route only relays the data as is. The one we configured in the previous section was a typical example.

But the arrangements can be more intricate.

```yaml title="multiple-routes.yml"
routes:
  - name: syslog_route
    targets:
      - name: syslog_to_json
      - name: aggregate_logs
  - name: windows_route
      targets:
        - name: windows_to_json
        - name: aggregate_logs
```

Save this configuration in a file named `multiple-routes.yml` in our [working directory](./configuration-basics.mdx#working-directory). 

As the syntax indicates, multiple routes can be specified: that is, the same source data can be forwarded to multiple destinations, or a destination may receive data from multiple sources.

:::warning[attemtion]
If a destination is receiving data from multiple sources, the data must be [normalized](../configuration/pipelines/normalization.mdx) to conform to the layout.

For the `aggregate_logs` target in the above route configuration, we will be using "`ecs`" as the layout.
:::

Here, we will be sending the data to both a JSON file and a Parquet file for storage. Hence, we need more definition files for various devices and targets, and we also have to eliminate definitions we do not need to keep things transparent:

* Leave the `from-syslog.yml` file as is

* Delete the `syslog-to-console.yml` and `to-console.yml` files

* Replace the route definition in `syslog-to-json.yml` with the following target definition:

  ```yaml title="syslog-to-json.yml"
  targets:
    - name: syslog_to_json
      type: file
      properties:
        location: "<vm_root>/config/Examples"
        name: "from-syslog.json"
  ```

* Create a file with this device definition:

  ```yaml title="from-windows.yml"
  devices:
    - id: 2
      name: from_windows
      type: windows
      properties:
        protocol: udp
        address: 0.0.0.0
        port: 5985
  ```

* Create two separate files with these target definitions:

  - ```yaml title="windows-to-json.yml"
    targets:
      - name: windows_to_json
        type: file
        properties:
          location: "<vm_root>/config/Examples"
          name: "from-windows.json"
    ```

  - ```yaml title="aggregate-logs.yml"
    targets:
      - name: aggregate_logs
        type: file
        format: "parquet"
        properties:
          location: "<vm_root>/config/Examples"
          name: "aggregate-logs.parquet"
          format: "ecs"
    ```

With the above, we have

* 2 devices to ingest data: one for `syslog`, the other for `windows` log data;
* 3 targets, two in JSON (one for `syslog`, the other for `windows` data) and the other in Parquet format.

Now we are ready to use the routes in our `multiple-routes.yml` file.

## Complex Relay

A more significant function of routes is to direct the incoming data to relevant destinations after processing them, if need be. As such they orchestrate the traffic.

This involves the following steps.

### Selection

Since the routing operation occurs amidst high telemetry traffic, pipelines can also be used to selectively process specific data streams.

This can be done using multiple device types:

```yaml
routes:
  - name: syslog_routes
    if: device.type == 'syslog'
    pipelines:
      - name: syslog_normalize
    targets:
      - name: syslog_to_json

  - name: windows_routes
    if: device.type == 'windows'
    pipelines:
      - name: windows_normalize
    targets:
      - name: windows_to_json
```

These routes will collect two streams of data, one from a _syslog_ and one from a _windows_ device, normalize them using the specific pipelines of each, and then direct them to their respective JSON file targets.

:::note
This time we have specified two different targets&mdash;`syslog_to_json` and `windows_to_json`&mdash;which have to be configured elsewhere, either locally or in their own YAML files. We leave that as an exercise to the reader.
:::

The selection can also be done using _datasets_:

```yaml
routes:
  - name: security_dataset
    if: dataset.name == 'security_logs'
    pipelines:
      - name: security_processing
    targets:
      - name: security_to_json

  - name: performance_dataset
    if: dataset.name == 'performance_metrics'
    pipelines:
      - name: metrics_processing
    targets:
      - name: metrics_to_json
```

The first route will collect data from a dataset used for security logs and store it in a security-focused JSON file, whereas the second will handle performance metrics and store them in a separate metrics JSON file.

### Transformation

A route can use pipelines as part of its operation in order to process and transform the data. Multiple pipelines can be specified to apply to various streams of data.

We can have a single pipeline, defined _in-situ_, tasked with a simple functionality such as parsing logs:

```yaml
routes:
  - name: syslog_routes
    pipelines:
      - name: parse_logs
        processors:
          - grok: "%{SYSLOGTIMESTAMP}"
```

This configuration is intended to extract the `timestamp` value in _Syslog_ before sending it to our target.

We can also use several pipelines consecutively:

```yaml
routes:
  - name: syslog_routes
    pipelines:
      - name: normalize
      - name: enrich
      - name: aggregate
    targets:
      - name: to_json
```

The specified 3 pre-defined pipelines are used for purposes that should be obvious from their names: normalizing, enriching, and aggregating. The data will then be routed to a JSON file.

### Direction

The same data can be sent to multiple targets, a technique known as _mirroring_:

```yaml
routes:
  - name: mirror_routes
    description: "Store logs in multiple parquet files"
    pipelines:
      - name: normalize
    targets:
      - name: primary_json
      - name: backup_json
      - name: analytics_json
```

Here, the data will be received by 3 different targets in JSON format: a primary storage file, a backup storage file, and an analytics platform file.

## Monitoring

Once we configure our route, we can monitor the stream it generates using the command line:

<Tabs groupId="os-options">
  <TabItem value="windows" label="Windows" default>
    ```powershell
    .\vmetric-director
    ```
  </TabItem>
  
  <TabItem value="linux" label="Linux" default>
    ```bash
    ./vmetric-director
    ```
  </TabItem>

  <TabItem value="macos" label="macOS" default>
    ```bash
    ./vmetric-director
    ```
  </TabItem>
</Tabs>

Press <kb-short>Ctrl+C</kb-short> to exit the process.

---

Up to this point, we have seen how to configure and run the elements of a telemetry data flow. In the next section, we will learn how to create an end-to-end pipeline.
