# Example: Routing Data

This section will help you get started with routes to forward data to specific destinations, walking you through a common use case.

## Scenario

We will configure a route that receives data from a _Syslog_ device, processes the data points with a pre-configured pipeline, and forwards the results to a **Sentinel** target.

:::warning[attention]
Before configuring a route, verify that the devices, pipelines, and targets to be used in the route are ready and accessible.
:::

## Basic Relay

The simplest route only relays the data as is:

```yaml
routes:
  - name: basic_forward
    description: "Forward all logs to storage"
    targets:
      - name: storage
```

As the syntax indicates, multiples routes can be specified. The same source data can be forwarded to multiple destinations, or a destination may receive data from multiple sources.

In the above example, we are forwarding raw data to a previously configured target named `storage`.

## Pipeline Use

A route can use pipelines as part of its operation. Multiple pipelines can be specified to be applied to various streams of data.

We can have a single pipeline:

```yaml
routes:
  - name: process_logs
    description: "Process and store logs"
    pipelines:
      - name: normalize_logs
    targets:
      - name: storage
```

This configuration will [normalize](../configuration/pipelines/normalization.mdx) the data before sending it to the target named `storage`.

We can also use several pipelines consecutively:

```yaml
routes:
  - name: complex_processing
    description: "Multi-stage processing"
    pipelines:
      - name: normalize
      - name: enrich
      - name: aggregate
    targets:
      - name: analytics
```

The specified 3 pipelines are used for purposes that should be obvious from their names: normalizing, enriching, and aggregating. The data will then be routed to a target used for analytics.

## Selection

Since the routing operation occurs amidst high telemetry traffic, pipelines can also be used to selectively process specific data streams.

This can be done using multiple device types:

```yaml
routes:
  - name: syslog_route
    if: device.type == 'syslog'
    pipelines:
      - name: syslog_normalize
    targets:
      - name: syslog_storage

  - name: windows_route
    if: device.type == 'windows'
    pipelines:
      - name: windows_normalize
    targets:
      - name: windows_storage
```

This route will collect two streams of data, one from a _syslog_ and one from a _windows_ device, normalize them using the specific pipelines of each, and then direct them to their respective targets.

The selection can also be done using datasets:

```yaml
routes:
  - name: security_dataset
    if: dataset.name == 'security_logs'
    pipelines:
      - name: security_process
    targets:
      - name: security_analytics

  - name: performance_dataset
    if: dataset.name == 'performance_metrics'
    pipelines:
      - name: metrics_process
    targets:
      - name: metrics_platform
```

The first route will collect data from a dataset used for security logs, whereas the second from another one used for performance metrics.

## Forwarding

The same data can be sent to multiple targets, a technique known as _mirroring_:

```yaml
routes:
  - name: mirror_logs
    description: "Store logs in multiple locations"
    pipelines:
      - name: normalize
    targets:
      - name: primary_storage
      - name: backup_storage
      - name: analytics_platform
```

Here, the data will be received by 3 different targets: a primary storage, a backup storage, and an analytics platform.

## Monitoring

Once our route is ready, we can monitor the stream it generates using the CLI:

<Tabs>
  <TabItem value="windows" label="Windows" default>
    ```powershell
    vmetric-director.exe -mode=route -name=mirror_logs
    ```
  </TabItem>
  
  <TabItem value="linux" label="Linux" default>
    ```bash
    vmetric-director -mode=route -name=mirror_logs
    ```
  </TabItem>

  <TabItem value="macos" label="macOS" default>
    ```bash
    vmetric-director -mode=route -name=mirror_logs
    ```
  </TabItem>
</Tabs>

Now we will see how to create an end-to-end pipeline in the next section.
