# Example: Basic Routing

This chapter will help you get started with routes to forward data to specific destinations, walking you through common scenarios.

## Prerequisites

:::note
Before configuring routes, verify that the pipelines and targets to be used in the routes are already configured and accessible.

See the previous example chapters for configuring [targets](/docs/getting-started/example-target.mdx) and [pipelines](/docs/getting-started/example-pipeline.mdx).
:::

## Overview

Route configurations can be classified as follows&mdash;in each case, use of pipelines is optional:

|Source|Target|Description|
|---|---|---|
|Single|Single|Data from a single source e.g. _S1_ directed to a single destination e.g. _T1_|
||Multiple|The same data e.g. _S1_ distributed to multiple destinations e.g. _T1_ and _T2_|
|Multiple|Single|Data from multiple sources e.g. _S1_ and _S2_ directed to a single target e.g. _T1_|
||Multiple|Data from multiple sources directed to multiple targets. The combinations may get complex such as sources _S1_ and _S3_ distributed to targets _T2_, _T3_, and _T4_ while source _S2_ is distributed to targets _T1_ and _T4_|

The data may be transformed by one or more pipelines before being forwarded to destinations.

The simplest route only relays the data as is:

```yaml
routes:
  - name: basic_forward
    description: "Forward all logs to storage"
    targets:
      - name: storage
```

As the syntax indicates, multiples routes can be specified. The same source data can be forwarded to multiple destinations, or a destination may receive data from multiple sources.

In the above example, we are forwarding raw data to a previously configured target named `storage`.

## Using Pipelines

A route can use pipelines as part of its operation. Multiple pipelines can be specified to be applied to various streams of data.

We can have a single pipeline:

```yaml
routes:
  - name: process_logs
    description: "Process and store logs"
    pipelines:
      - name: normalize_logs
    targets:
      - name: storage
```

This configuration will [normalize](/docs/configuration/pipelines/normalization.mdx) the data before sending it to the target named `storage`.

We can also use several pipelines consecutively:

```yaml
routes:
  - name: complex_processing
    description: "Multi-stage processing"
    pipelines:
      - name: normalize
      - name: enrich
      - name: aggregate
    targets:
      - name: analytics
```

The specified 3 pipelines are used for purposes that should be obvious from their names: normalizing, enriching, and aggregating. The data will then be routed to a target used for analytics.

## Selection

Since the routing operation occurs amidst high telemetry traffic, pipelines can also be used to selectively process specific data streams.

This can be done using multiple device types:

```yaml
routes:
  - name: syslog_route
    if: device.type == 'syslog'
    pipelines:
      - name: syslog_normalize
    targets:
      - name: syslog_storage

  - name: windows_route
    if: device.type == 'windows'
    pipelines:
      - name: windows_normalize
    targets:
      - name: windows_storage
```

This route will collect two streams of data, one from a _syslog_ and one from a _windows_ device, normalize them using the specific pipelines of each, and then direct them to their respective targets.

The selection can also be done using datasets:

```yaml
routes:
  - name: security_dataset
    if: dataset.name == 'security_logs'
    pipelines:
      - name: security_process
    targets:
      - name: security_analytics

  - name: performance_dataset
    if: dataset.name == 'performance_metrics'
    pipelines:
      - name: metrics_process
    targets:
      - name: metrics_platform
```

The first route will collect data from a dataset used for security logs, whereas the second from another one used for performance metrics.

## Conditionals

The filtering required for selecting the data for a route can be done using conditional statements, as seen from some of the examples above. The conditions can be as simple as picking a specific device type:

```yaml
routes:
  - name: firewall_logs
    description: "Process firewall logs"
    if: device.type == 'firewall'
    pipelines:
      - name: firewall_pipeline
    targets:
      - name: security_storage
```

Or they can be complex, drilling down to the attributes of the data to be collected such as severity of security breaches, date range of the data collected, etc.:

```yaml
routes:
  - name: critical_errors
    if: log.severity == 'critical' && log.date >= '2024.05.01'
    pipelines:
      - name: urgent_process
    targets:
      - name: alerts
      - name: storage
```

This route will collect critical errors that have occurred after a certain date, and will forward them to two separate targets.

## Forwarding

The same data can be sent to multiple targets, a technique known as _mirroring_:

```yaml
routes:
  - name: mirror_logs
    description: "Store logs in multiple locations"
    pipelines:
      - name: normalize
    targets:
      - name: primary_storage
      - name: backup_storage
      - name: analytics_platform
```

Here, the data will be received by 3 different targets: a primary storage, a backup storage, and an analytics platform.

## Monitoring

Once we configure our route, we can monitor the stream it generates using the command line:

<Tabs groupId="os-options">
  <TabItem value="windows" label="Windows" default>
    ```powershell
    .\vmetric-director -mode=route -name=critical_errors
    ```
  </TabItem>
  
  <TabItem value="linux" label="Linux" default>
    ```bash
    ./vmetric-director -mode=route -name=critical_errors
    ```
  </TabItem>

  <TabItem value="macos" label="macOS" default>
    ```bash
    ./vmetric-director -mode=route -name=critical_errors
    ```
  </TabItem>
</Tabs>

{/* TODO: Complete */}
