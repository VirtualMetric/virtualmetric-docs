---
pagination_next: null
---

# Example: A Syslog-To-Parquet Data Stream

This section will help you configure and run a full, end-to-end data flow using devices, targets, and a pipeline, walking you through a common use case.

:::info
To avoid any confusion, delete all YAML files created in other examples. This guarantees that only our current configurations are run.

To execute the code examples, navigate to `<vm_root>`:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```powershell
    Set-Location <vm_root>
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    cd <vm_root>
    ```
  </TabItem>
</Tabs>

Filenames do not include the path to avoid variation in syntax for different platforms. All configuration files will be placed in our [working directory](./overview.mdx#working-directory).
:::

## Scenario

We will create a pipeline that ingests logs from a _Syslog_ source, processes and categorizes them, and forwards them to a _Parquet_ file for storage and analysis.

We will implement the following scenario:

- **Ingest Syslog data** - Define the parameters of a _Syslog_ listener to receive messages.

- **Extract user related information** - Define the parameters to extract specific information from _Syslog_ data and map them to fields that will be stored in a _Parquet_ file.

- **Route the data flow** - Define the parameters that will relate the _source_ to the _pipeline_ that will process the ingested data and the _destination_ that will receive the processed result.

- **Write the data to a Parquet file** - Define the parameters for sending the messages to a _Parquet_ file as the destination.

We need to work in an environment that has access to _Syslog_ messages. We also need write permissions to the directory where the Parquet file will be stored, although since we will be using our working directory that is taken care of.

We assume by now you understand the underlying logic of a pipeline since you have reviewed the [previous example](./example-reading-json-with-a-pipeline.mdx).

## Setup

For convenience, we will place all our configurations in a single file named `syslog-to-parquet.yml` which we will again place in our working directory.

### Step 1: Configure the device

First, we have to define a device that will receive the log data. We will use the configuration we have created in our [ingesting data example](./example-ingesting-data.mdx) as our starting point, modifying it as fits our needs:

```yml title="syslog-to-parquet.yml"
devices:
  - id: 1
    name: from_syslog
    type: syslog
    properties:
      protocol: udp
      port: 514
      framing: delimiter
      line_delimiter: "\n"
      buffer_size: 16384
      batch_size: 1000
```

This configuration will create a UDP _Syslog_ server listening on port `514`. We specified the line feed character as the delimiter for message framing.

We also set appropriate buffer and batch sizes to consume enough data in one pass.

Put this device configuration in our file.

### Step 2: Configure the target

Next, we will define a target for _Parquet_ file storage in our configuration:

```yml title="syslog-to-parquet.yml"
targets:
  - name: to_parquet
    type: parquet
    properties:
      location: "./config/Examples"
      name: "from-syslog.parquet"
      batch_size: 1000
      flush_interval: 300
      compression: snappy
```

With this configuration, we are setting up a _Parquet_ file target that will store the processed _Syslog_ data.

We have also specified batching and flush intervals for optimal performance, and enabled **Snappy** compression for efficient storage.

Put this configuration in our file.

### Step 3: Configure the pipeline

We are now ready to configure our pipeline:

```yml title="syslog-to-parquet.yml"
pipelines:
  - name: extract_user_event
    processors:
      - grok:
          field: syslog.message
          patterns:
            - %{SYSLOGTIMESTAMP:timestamp} %{USERNAME:user_name} %{GREEDYDATA:message_body}
          ignore_missing: true
      - set:
          field: User
          value: user_name
      - concat:
          sources: ["Said '", message_body, "' on ", timestamp.date, " at ", timestamp.time]
          separator: ""
          target: Message
      - remove:
          field: syslog.message
          ignore_missing: true
      - remove:
          field: syslog.timestamp
          ignore_missing: true
```

This configuration will:

- parse the _Syslog_ header information using the [`grok`](../../configuration/pipelines/processors/grok.mdx) processor&mdash;reading the field `syslog.message` and extracting `timestamp`, `user_name`, and `message_body` from it;

- set the fields `User`, and `Message` with the data extracted;

- clean up the temporary fields `syslog.message` and `syslog.timestamp`.

:::note
To avoid raising an exception, we chose to ignore failures and missing fields.
:::

Put this configuration in our file.

### Step 4: Configure the route

Finally, we have to create a route to connect our device to the pipeline and then to our target. Put the following definition in our configuration file:

```yml title="syslog-to-parquet.yml"
routes:
  - name: syslog_to_parquet
    devices:
      - name: from_syslog
    pipelines:
      - name: extract_user_event
    targets:
      - name: to_parquet
```

This configuration will connect to the _Syslog_ server, apply the `extract_user_event` pipeline to the ingested data, and then write it to the _Parquet_ file specified.

### Step 5: Run the data stream

Let's put it all together: we have defined a device, a target, and a pipeline, and specified a route that will link the device to the pipeline and the target. At this point we have everything ready.

:::note
We will use the `mode` parameter to tell **Director** that we will run a _pipeline_, and then use the pipeline's `name` parameter to specify it on the CLI.
:::

As we did above√º, we will first validate the pipeline:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director -pipeline -path=".\config\Examples\syslog-to-parquet.yml" -name=syslog_to_parquet -validate
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -pipeline -path="./config/Examples/syslog-to-parquet.yml" -name=syslog_to_parquet -validate
    ```
  </TabItem>
</Tabs>

Once our pipeline is validated, we can start running our configuration:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director
    ```
  </TabItem>
</Tabs>

We will once again use the `generator` mode (or a traditional platform tool) to send our "`Hello world`" messages to _Syslog_:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```powershell
    .\vmetric-director -generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
  </TabItem>
</Tabs>

Now, **Director** should be receiving _Syslog_ messages, processing them through our pipeline, and sending them to our _Parquet_ file.

Press <kb-short>Ctrl+C</kb-short> to exit the processes on both terminals.


TODO: Copy 1- Screen output 2- Parquet file contents

## Monitoring

Check the logs for any errors, and verify that the _Parquet_ file was created with the processed _Syslog_ data.

TODO: Copy the log output.
