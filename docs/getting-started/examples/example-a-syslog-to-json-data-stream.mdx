---
pagination_next: null
---

# Example: A Syslog-To-JSON Data Stream

This section will help you configure and run a full, end-to-end data flow using devices, targets, and a pipeline, walking you through a common use case.

:::warning[attention]
To avoid any confusion, delete all YAML files created in other examples. This guarantees that only our current configurations are run.

To execute the code examples, navigate to `<vm_root>`:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```powershell
    Set-Location <vm_root>
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    cd <vm_root>
    ```
  </TabItem>
</Tabs>

Filenames do not include the path to avoid variation in syntax for different platforms. All configuration files will be placed in our [working directory](./overview.mdx#working-directory).
:::

## Scenario

We will create a pipeline that ingests logs from a _Syslog_ source, processes and categorizes them, and forwards them to a _JSON_ file for storage and analysis.

We will implement the following scenario:

- **Ingest data** - Listen to _Syslog_ to receive messages.

- **Extract information** - Extract specific information from the _Syslog_ messages and map them to fields that will be stored in a _JSON_ file.

- **Write the information** - Write the fields mapped from the extracted information to a _JSON_ file.

We need to work in an environment that has access to _Syslog_ messages. We also need write permissions to the directory where the _JSON_ file will be stored, although since we will be using our working directory that is taken care of.

We assume by now you understand the underlying logic of a pipeline since you have reviewed the [previous example](./example-reading-json-with-a-pipeline.mdx).

## Setup and Trial

For convenience, we will place all our configurations in a single file named `syslog-to-json.yml` which we will again place in our working directory.

### Step 1: Configure the device

First, we have to define a device that will receive the log data. We will use the configuration in our [ingesting data](./example-ingesting-data.mdx) example since that will adequately serve our needs:

```yml title="syslog-to-json.yml"
devices:
  - id: 1
    name: from_syslog
    type: syslog
    status: true
    properties:
      protocol: udp
      address: 127.0.0.1
      port: 514
```

This configuration will create a UDP _Syslog_ server listening on port `514`.

Put this device configuration in our file.

### Step 2: Configure the target

Next, we will define a target for the _JSON_ file storage:

```yml title="syslog-to-json.yml"
targets:
  - name: to_json
    type: file
    status: true
    properties:
      location: C:\Users\<username>\Documents\Logs
      name: syslog-event-data.json
      compression: zstd
      format: "json"
```

Note that we have specified a certain path for `location`.

:::warning[attention]
You should create the folder you want to use for `location` in advance, and make sure that you have write permissions to it.
:::

With this configuration, we are setting up a _JSON_ file target that will store the processed _Syslog_ data. We have also enabled **ZTSD** compression for efficient storage.

Put this target configuration in our file.

### Step 3: Configure the pipeline

We are now ready to configure our pipeline:

```yml title="syslog-to-json.yml"
pipelines:
  - name: extract_user_event
    processors:
      - grok:
          field: message
          patterns:
            - "%{DATA}: %{GREEDYDATA:raw_message} - %{NUMBER}"
      - set:
          field: Event
          copy_from: raw_message
      - remove:
          field: message
          ignore_missing: true
      - remove:
          field: raw_message
          ignore_missing: true
```

This configuration will 

- using the [`grok`](../../configuration/pipelines/processors/grok.mdx) processor, parse the _Syslog_ header information to read the field `raw_message`

- write the `raw_message` value to the `Event` field in the _JSON_ file

- remove the `message` and `raw_message` fields of _Syslog_ from the output

:::note
To avoid raising an exception, we chose to ignore missing fields.
:::

Put this pipeline configuration in our file.

### Step 4: Configure the route

Finally, we have to create a route to connect our device to the pipeline and then to our target. Put the following definition in our configuration file:

```yml title="syslog-to-json.yml"
routes:
  - name: syslog_to_json
    devices:
      - name: from_syslog
    pipelines:
      - name: extract_user_event
    targets:
      - name: to_json
```

This configuration will 

- connect to the _Syslog_ server and receive data from it,
- apply the `extract_user_event` pipeline to the ingested data, and
- write the processed data to the _JSON_ file specified.

Put this route configuration in our file.

### Step 5: Run the data stream

Let's put it all together: we have defined a device, a target, and a pipeline, and specified a route that will link the device to the pipeline and the target. At this point we have everything ready.

We can now start running our configuration. First, invoke **Director**'s `-console` switch to monitor the status messages:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director -console
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -console
    ```
  </TabItem>
</Tabs>

We will once again use our own tool to send our messages to _Syslog_. Switch to the other terminal, type the following, and press <kb-short>Enter</kb-short>:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```powershell
    .\vmetric-director -generator -now -mode syslog -count 1 -address "127.0.0.1:514" -message "John Doe says 'Hello world'"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -generator -now -mode syslog -count 1 -address "127.0.0.1:514" -message "John Doe says 'Hello world'"
    ```
  </TabItem>
</Tabs>

As **Director** starts sending the messages, the screen should look like this:

```console
1 messages sent successfully in 0 ms with 0 error(s) at 2025-06-29 22:44:28.5619632 +0300 +03 m=+0.054953001
1 messages sent successfully since 2025-06-29 22:44:28.5619632 +0300 +03 m=+0.054953001

1 messages sent successfully in 0 ms with 0 error(s) at 2025-06-29 22:44:29.5634994 +0300 +03 m=+1.056489201
2 messages sent successfully since 2025-06-29 22:44:28.5619632 +0300 +03 m=+0.054953001
...
```

After sending a limited number of messages&mdash;say 5 of them&mdash;press <kb-short>Ctrl+C</kb-short> to exit the process on that terminal.

Now, if you switch back to the other terminal, you will see that **Director** was receiving our _Syslog_ messages, processing them through our pipeline, and sending them to our _JSON_ file. As it was executing, it printed status messages on the screen like these:

```console
[2025-06-29 22:44:31] [Information] [vmetric-director] [syslog-55555] Processing Syslog messages.. Number of Messages: 1
[2025-06-29 22:44:31] [Information] [vmetric-director] [syslog-55555] Completed processing of vmdb.syslog.55555.1751226271537836500.1.vmfl logs. Number of processed logs: 1
[2025-06-29 22:44:32] [Information] [vmetric-director] Completed processing of to_json target logs. Number of processed logs: 1
[2025-06-29 22:44:32] [Information] [vmetric-director] Completed processing of syslog logs on extract_user_event pipeline for syslog_to_json route. Number of processed logs: 2
...
```

Now go to the directory we have specified in `location` above, and check to see whether the file `syslog-event-data.json` has been created. If you ran the test without any errors, it should be there, and its contents should look like this:

```json
{"Event":"John Doe says 'Hello world'"}
{"Event":"John Doe says 'Hello world'"}
{"Event":"John Doe says 'Hello world'"}
{"Event":"John Doe says 'Hello world'"}
{"Event":"John Doe says 'Hello world'"}
```

## Monitoring

Check the logs in the [logs directory](./overview.mdx#logging-setup). If there were no errors, it should be blank.
