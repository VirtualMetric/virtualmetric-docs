# Example: Reading JSON With a Pipeline

This section will help you get started with configuring and running a very simple pipeline, walking you through a common use case.

:::tip
For a detailed discussion of pipelines, see [this section](../../configuration/pipelines/overview.mdx#configuration).
:::

:::info
To avoid any confusion, delete all YAML files created in other examples. This guarantees that only our current configurations are run.

To execute the code examples, navigate to `<vm_root>`:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    ```powershell
    Set-Location <vm_root>
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    cd <vm_root>
    ```
  </TabItem>
</Tabs>

Filenames do not include the path to avoid variation in syntax for different platforms. All configuration files will be placed in our [working directory](./overview.mdx#working-directory).
:::

In this example, we will use a feature of **Director** that facilitates designing and testing pipelines.

:::warning[attention]
In all the following code samples, note that we have to specify the paths and names of the YAML files we are using. This is specific to the `-pipeline` mode.
:::

## Scenario

To understand how a pipeline works at the very basic level, we will create a very simple input file in JSON format and a very simple pipeline that does only one transformation. Afterwards, we will write the transformed data to another JSON file.

Using **Director**'s pipeline validation and testing functionality, we will see the pipeline in action.

## Step 1: Create Test Data

First, create a JSON file named `sample-data.json` in our working directory, and put the following sample data in it:

```json title="sample-data.json"
{
  "raw_data": "{\"words\": \"hello world\"}"
}
```

## Step 2: Define Pipeline Logic

Then, create a YAML file in our working directory named `convert-words.yml`, and put the following pipeline definition in it:

```yaml title="convert-words.yml"
pipelines:
  - name: convert_words
    processors:
      - json:
          field: raw_data
          add_to_root: true
      - uppercase:
          field: words
          target_field: converted_words
```

:::tip
When configuring a pipeline, we use the identifiers in the `name` fields of the components.
:::

Here is what this pipeline will do:

- Look for the field named `raw_data` in the JSON file we will feed it.
- Using the [`json`](../../configuration/pipelines/processors/json.mdx) processor, grab its contents and&mdash;since we turned on its `add_to_root` setting&mdash;move the `words` field one level up.
- Using the [`uppercase`](../../configuration/pipelines/processors/uppercase.mdx) processor, convert the words to uppercase and write them to a field named `converted_words`.

## Step 3: Validate The Pipeline

To see whether our pipeline is valid, we enter the following command in the terminal:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director -validate -path=".\config\Examples\convert-words.yml"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -validate -path="./config/Examples/convert-words.yml"
    ```
  </TabItem>
</Tabs>

If our pipeline is valid, this command will return:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    [OK] No issues found.
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    [OK] No issues found.
    ```
  </TabItem>
</Tabs>

And that is indeed the case. Now, to `visualize` the output, enter the following command:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director -pipeline -path=".\config\Examples\convert-words.yml" -name=convert_words -visualize
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -pipeline -path="./config/Examples/convert-words.yml" -name=convert_words -visualize
    ```
  </TabItem>
</Tabs>

This should return the following:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    {
      "converted_words": "HELLO WORLD",
      "raw_data": "{\"words\": \"hello world\"}",
      "words": "hello world"
    }
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    {
      "converted_words": "HELLO WORLD",
      "raw_data": "{\"words\": \"hello world\"}",
      "words": "hello world"
    }
    ```
  </TabItem>
</Tabs>

This means, **Director** is able to recognize the pipeline we have defined by its name and generate the expected output.

Now we can test our pipeline.

## Step 4: Test The Pipeline

To see our pipeline in action, enter the following in the terminal and check its status message:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    .\vmetric-director -pipeline -path=".\config\Examples\convert-words.yml" -name convert_words -input ".\config\Examples\sample-data.json" -output ".\config\Examples\processed-sample-data.json"
    Successfully exported to .\config\Examples\processed-sample-data.json
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    ./vmetric-director -pipeline -path="./config/Examples/convert-words.yml" -name convert_words -input "./config/Examples/sample-data.json" -output "./config/Examples/processed-sample-data.json"
    Successfully exported to ./config/Examples/processed-sample-data.json
    ```
  </TabItem>
</Tabs>

Here is what this test does:

* Consume the data in the input file named `sample-data.json` in our working directory
* Using the pipeline we named `convert_words`, extract and convert the words in the `words` field to `uppercase`
* Write the converted names to a new field named `converted_words`
* Save the results, along with the original `raw_data`, to an output file named `processed-sample-data.json` in our working directory.

:::note
The output file should be created when the test is run.
:::

## Step 5: Check The Processed Data

Now open the file named `processed-sample-data.json` to see the results. It should appear like this:

```json title="processed-sample-data.json"
{
    "converted_words": "HELLO WORLD",
    "raw_data": "{\"words\": \"hello world\"}",
    "words": "hello world"
}
```

---

Now we can proceed to designing an end-to-end _data stream_.
