# Example: A Syslog-To-Parquet Pipeline

This section will help you get started with configuring and running a pipeline, walking you through a common use case.

:::tip
For a detailed discussion, see [this section](../configuration/pipelines/overview.mdx#configuration).
:::

## Scenario

We will create a pipeline that ingests logs from a _Syslog_ source, processes and categorizes them, and forwards them to a **Parquet file** for storage and analysis.

We will implement the following scenario:

- **Ingest Syslog data** - Define the parameters of a _Syslog_ listener to receive messages.

- **Extract user related information** - Define the parameters to extract specific information from _Syslog_ data and map them to fields that will be stored in a _Parquet_ file.

- **Route the data flow** - Define the parameters that will relate the _source_ to the _pipeline_ that will process the ingested data and the _destination_ that will receive the processed result.

- **Write the data to a Parquet file** - Define the parameters for sending the messages to a _Parquet_ file as the destination.

We need to work in an environment that has access to _Syslog_ messages. We also need write permissions to the directory where the Parquet file will be stored, although since we will be using our [working directory](./introduction.mdx#working-directory) that is taken care of.

However, we must first understand the underlying logic of a pipeline.

### Preliminary Test

To understand how a pipeline works at the very basic level, we will create a very simple input file in JSON format and a very simple pipeline that does only one transformation, and using **Director**'s pipeline validation and testing facilities, we will see the pipeline in action.

### Sample Case

:::tip
For the following, nagivate to `<vm_root>` in your terminal, and continue working there since any paths given will be relative.
:::

#### Create Test Data

First, create a JSON file named `sample-data.json` in our working directory, and put the following sample data in it:

```json
{
  "words": ["hello", "world"]
}
```

#### Define Pipeline Logic

Then, create a YAML file in our working directory named `convert-words.yml`, and put the following pipeline definition in it:

```yaml
pipelines:
  - name: convert_words
pipelines:
  - name: convert_words
      processors:
        - uppercase:
            field: words
            target_field: convertedWords
```

:::tip
When configuring a pipeline, we use the identifiers in the `name` fields of the components.
:::

:::warning
The YAML formatting of pipelines requires that all entries after `- name:` is indented one tab.
:::

Here is what this pipeline will do:

- Look for the field named `words` in the JSON file we will feed it.
- Using the `json` processor's `add_to_root` flag, copy the values it has spotted in that field to the root of the output document.
- Using the [`foreach`](../configuration/pipelines/processors/foreach.mdx) processor&mdash;which applies its specified `processor` (here, [`uppercase`](../configuration/pipelines/processors/uppercase.mdx)) to each element in the list returned by the meta field `"_ingest._value"`&mdash;convert each item to uppercase.

#### Validate Pipeline

To see whether our pipeline is valid, we enter the following command in the terminal:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director -validate -path=".\config\Examples\convert-words.yml"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director -validate -path="./config/Examples/convert-words.yml"    vmetric-director -validate -path="./config/Examples/convert-words.yml
    ```
  </TabItem>
</Tabs>

If our pipeline is valid, this command will return:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    [OK] No issues found.
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    [OK] No issues found.
    ```
  </TabItem>
</Tabs>

Since that is indeed the case, we can now run our test.

#### Run Test

To see our pipeline in action, enter the following in the terminal:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director -mode pipeline -path=".\config\Examples\convert-words.yml" -name convert_words -input ".\config\Examples\sample-data.json" -output ".\config\Examples\processed-sample-data.json"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director -mode pipeline -path="./config/Examples/convert-words.yml" -name convert_words -input "./config/Examples/sample-data.json" -output "./config/Examples/processed-sample-data.json"
    ```
  </TabItem>
</Tabs>

Here is what this test will do:

* Consume the data in the input file named `sample-data.json` that we put in our working directory
* Using the pipeline we named `convert_words`, convert the words in the `words` field to `uppercase`
* Write the converted names to a new field named `convertedWords`
* Save the results to an output file named `processed-sample-data.json` in our working directory.

:::note
The output file should be created when the test is run.
:::

#### Check Processed Data

Now open the file named `processed-sample-data.json` to see the results. It should appear like this:

```json
{
  "convertedWords": ["HELLO", "WORLD"]
}
```

Now we can proceed to creating our data flow design.

## Setup

For convenience, we will place all our configurations in a single file named `syslog-to-parquet.yml` which we will again place in our working directory.

:::note
To avoid any confusion, delete all the other YAML files we have previously created so that nothing other than our current configuration is run.
:::

### Step 1: Configure The Device

First, we have to define a device that will receive the log data. We will use the configuration we have created in our [ingesting data example](./example-ingesting-data.mdx) as our starting point, modifying it as fits our needs:

```yml title="syslog-to-parquet.yml"
devices:
  - id: 1
    name: from_syslog
    type: syslog
    properties:
      protocol: udp
      port: 514
      framing: delimiter
      line_delimiter: "\n"
      buffer_size: 16384
      batch_size: 1000
```

For details of the available fields, see the [Syslog](../appendix.mdx#syslog) format.

Put this device configuration in your file.

This configuration will create a UDP _Syslog_ server listening on port `514`. We specified the line feed character as the delimiter for message framing. We also set appropriate buffer and batch sizes to consume enough data in one pass.

### Step 2: Configure The Target

Next, we will define a target for _Parquet_ file storage in our configuration:

```yml title="syslog-to-parquet.yml"
targets:
  - name: to_parquet
    type: parquet
    properties:
      location: "<vm_root>/config/Examples"
      name: "from-syslog.parquet"
      batch_size: 1000
      flush_interval: 300
      compression: snappy
```

For details of the file layout, see the [Parquet](../appendix.mdx#parquet) format.

With this configuration, we are setting up a Parquet file target that will store processed syslog data. We have specified batching and flush intervals for optimal performance, and enabled Snappy compression for efficient storage.

### Step 3: Create A Processing Pipeline

We are now ready to configure our pipeline. Put the following in our configuration file:

```yml title="syslog-to-parquet.yml"
pipelines:
  - name: extract_user_event
      processors:
        - grok:
            field: message
            patterns:
              - "%{SYSLOGBASE} %{GREEDYDATA:syslog.message}"
        - grok:
            field: syslog.message
            patterns:
              - "%{DATA:event.action} %{WORD:user.name} from %{IP:source.ip}"
            ignore_failure: true
        - set:
            field: user
            value: user.name
        - set:
            field: event
            value: event.action
        - set:
            field: message
            value: syslog.message
        - remove:
            field:          
              - syslog.message
            ignore_missing: true
```

This configuration will do the following:

* Parse the _Syslog_ header information using the [`grok`](../configuration/pipelines/processors/grok.mdx) processor&mdash;reading the fields `syslog.message`, `event.action`, `user.name`, and `source.ip`&mdash;and will extract authentication event details

* Set the fields `event`, `user`, and `message` with the data extracted from _Syslog_

* Clean up the temporary field `syslog.message`.

:::note
To avoid raising exceptions, we chose to ignore failures and missing fields.
:::

### Step 4: Configure the Route

Finally, we have to create a route to connect our device to the pipeline and then to our target. Put the following definition in our configuration file:

```yml title="syslog-to-parquet.yml"
routes:
  - name: syslog_to_parquet
    devices:
      - name: from_syslog
    pipelines:
      - name: extract_user_event
    targets:
      - name: to_parquet
```

This configuration will connect to the _Syslog_ server, apply the `extract_user_event` pipeline to the ingested data, and then write it to the _Parquet_ file specified.

## Monitoring

Let's put it all together: we have defined a device, a target, and a pipeline, and specified a route that will link the device to the pipeline and the target. At this point we have everything ready.

:::note
We will use the `mode` parameter to tell **Director** that we will run a "pipeline", and then use the pipeline's `name` parameter to specify it on the CLI.
:::

As we did above√º, we will first validate the pipeline:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
</Tabs>

Once our pipeline is validated, we can start running our configuration:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director
    ```
  </TabItem>
</Tabs>

We will once again use **Generator** or a traditional platform tool to send our "`Hello world`" messages to _Syslog_:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    - Using **Generator**

    ```powershell
    vmetric-generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    - Using **Generator**
  
    ```bash
    vmetric-generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
    
    - Using **System Logger**

    ```bash
    logger -n 127.0.0.1 -P 514 "Hello world"
    ```
  </TabItem>
</Tabs>

Now **Director** should be receiving _Syslog_ messages, processing them through our pipeline, and sending them to our Parquet file.

Press <kb-short>Ctrl+C</kb-short> to exit the process.

Check the logs for any errors, and verify that the _Parquet_ file was created with the processed _Syslog_ data.
