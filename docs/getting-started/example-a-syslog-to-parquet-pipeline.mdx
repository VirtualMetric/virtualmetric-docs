# Example: A Syslog-To-Parquet Pipeline

This section will help you get started with configuring and running a pipeline, walking you through a common use case.

:::tip
For a detailed discussion, see [this section](../configuration/pipelines/overview.mdx#configuration).
:::

## Scenario

We will create a pipeline that ingests logs from a _Syslog_ source, processes and categorizes them, and forwards them to a **Parquet file** for storage and analysis.

We will implement the following scenario:

- **Ingest Syslog data** - Define the parameters of a _Syslog_ listener to receive messages.

- **Extract user related information** - Define the parameters to extract specific information from _Syslog_ data and map them to fields that will be stored in a _Parquet_ file.

- **Route the data flow** - Define the parameters that will relate the _source_ to the _pipeline_ that will process the ingested data and the _destination_ that will receive the processed result.

- **Write the data to a Parquet file** - Define the parameters for sending the messages to a _Parquet_ file as the destination.

We need to work in an environment that has access to _Syslog_ messages. We also need write permissions to the directory where the Parquet file will be stored, although since we will be using our [working directory](./introduction.mdx#working-directory) that is taken care of.

However, we must first understand the underlying logic of a pipeline. For this, see the  [previous example](./example-reading-json-with-a-pipeline.mdx).

## Setup

For convenience, we will place all our configurations in a single file named `syslog-to-parquet.yml` which we will again place in our working directory.

:::note
To avoid any confusion, delete all the other YAML files we have previously created so that nothing other than our current configuration is run.
:::

### Step 1: Configure The Device

First, we have to define a device that will receive the log data. We will use the configuration we have created in our [ingesting data example](./example-ingesting-data.mdx) as our starting point, modifying it as fits our needs:

```yml title="syslog-to-parquet.yml"
devices:
  - id: 1
    name: from_syslog
    type: syslog
    properties:
      protocol: udp
      port: 514
      framing: delimiter
      line_delimiter: "\n"
      buffer_size: 16384
      batch_size: 1000
```

For details of the available fields, see the [Syslog](../appendix.mdx#syslog) format.

Put this device configuration in your file.

This configuration will create a UDP _Syslog_ server listening on port `514`. We specified the line feed character as the delimiter for message framing. We also set appropriate buffer and batch sizes to consume enough data in one pass.

### Step 2: Configure The Target

Next, we will define a target for _Parquet_ file storage in our configuration:

```yml title="syslog-to-parquet.yml"
targets:
  - name: to_parquet
    type: parquet
    properties:
      location: "<vm_root>/config/Examples"
      name: "from-syslog.parquet"
      batch_size: 1000
      flush_interval: 300
      compression: snappy
```

For details of the file layout, see the [Parquet](../appendix.mdx#parquet) format.

With this configuration, we are setting up a Parquet file target that will store processed syslog data. We have specified batching and flush intervals for optimal performance, and enabled Snappy compression for efficient storage.

### Step 3: Create A Processing Pipeline

We are now ready to configure our pipeline. Put the following in our configuration file:

```yml title="syslog-to-parquet.yml"
pipelines:
  - name: extract_user_event
<TabItem value="bash" label="Bash">            field: syslog.message
            patterns:
              - "%{DATA:event.action} %{WORD:user.name} from %{IP:source.ip}"
            ignore_failure: true
        - set:
            field: user
            value: user.name
        - set:
            field: event
            value: event.action
        - set:
            field: message
            value: syslog.message
        - remove:
            field:          
              - syslog.message
            ignore_missing: true
```

This configuration will do the following:

* Parse the _Syslog_ header information using the [`grok`](../configuration/pipelines/processors/grok.mdx) processor&mdash;reading the fields `syslog.message`, `event.action`, `user.name`, and `source.ip`&mdash;and will extract authentication event details

* Set the fields `event`, `user`, and `message` with the data extracted from _Syslog_

* Clean up the temporary field `syslog.message`.

:::note
To avoid raising exceptions, we chose to ignore failures and missing fields.
:::

### Step 4: Configure the Route

Finally, we have to create a route to connect our device to the pipeline and then to our target. Put the following definition in our configuration file:

```yml title="syslog-to-parquet.yml"
routes:
  - name: syslog_to_parquet
    devices:
      - name: from_syslog
    pipelines:
      - name: extract_user_event
    targets:
      - name: to_parquet
```

This configuration will connect to the _Syslog_ server, apply the `extract_user_event` pipeline to the ingested data, and then write it to the _Parquet_ file specified.

## Monitoring

Let's put it all together: we have defined a device, a target, and a pipeline, and specified a route that will link the device to the pipeline and the target. At this point we have everything ready.

:::note
We will use the `mode` parameter to tell **Director** that we will run a "pipeline", and then use the pipeline's `name` parameter to specify it on the CLI.
:::

As we did above√º, we will first validate the pipeline:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
</Tabs>

Once our pipeline is validated, we can start running our configuration:

<Tabs>
  <TabItem value="powershell" label="PowerShell">
    ```PowerShell
    vmetric-director
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    ```bash
    vmetric-director
    ```
  </TabItem>
</Tabs>

We will once again use **Generator** or a traditional platform tool to send our "`Hello world`" messages to _Syslog_:

<Tabs>
  <TabItem value="powershell" label="PowerShell" default>
    - Using **Generator**

    ```powershell
    vmetric-generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
  </TabItem>
  <TabItem value="bash" label="Bash">
    - Using **Generator**
  
    ```bash
    vmetric-generator -now -mode=syslog -count=1 -address="127.0.0.1:514" -message="Hello world"
    ```
    
    - Using **System Logger**

    ```bash
    logger -n 127.0.0.1 -P 514 "Hello world"
    ```
  </TabItem>
</Tabs>

Now **Director** should be receiving _Syslog_ messages, processing them through our pipeline, and sending them to our Parquet file.

Press <kb-short>Ctrl+C</kb-short> to exit the process.

Check the logs for any errors, and verify that the _Parquet_ file was created with the processed _Syslog_ data.
