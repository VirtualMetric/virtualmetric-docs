# Example: A Syslog-To-Parquet Pipeline

This section will help you get started with configuring and running a pipeline, walking you through a common use case.

## Scenario

We will create a pipeline that ingests logs from a _Syslog_ source, processes and categorizes them, and forwards them to a **Parquet file** for storage and analysis.

We will implement the following scenario:

* **Create a device** - Define the parameters of a `syslog` listener to receive messages. See the fields available in the [Syslog](../appendix.mdx#syslog) format

* **Create a target** - Define the format for sending the messages to a `parquet` file as the destination

* **Specify processors** - Using the `grok` processor, extract the `message` and `syslog.message` fields and map them to structured fields

To be able to use our pipeline, we will also define a route to specify the _source_ and the _destination_, along with the pipeline we have configured.

## Setup

We must work in an environment that has access to _Syslog_ messages, and since we have privileges for that, that is taken care of. We also need write permissions to the directory where Parquet files will be stored, and since we will be using our [working directory](./configuration-basics.mdx#working-directory) that also is taken care of.

:::note
For this section, nagivate to `<vm_root>` and continue working there since any paths given will be relative.
:::

However, we must first understand the logic of a pipeline.

## Pipeline Logic

To understand how a pipeline works at the very basic level, we will create a very simple input file in JSON format and a very simple pipeline that does only one transformation, and using **Director**'s pipeline validation and testing facilities, we will see the pipeline in action.

### Test Case

First, create a JSON file named `sample-data.json` in our working directory, and put the following sample data in it:

```json
{
  "names": ["john", "june", "mark", "mary"]
}
```

Then, create a YAML file in our working directory named `convert-names.yml`, and put the following pipeline definition in it:

```yaml
pipelines:
  - name: convert_names
    processors:
      - uppercase:
        field: names
        target_field: convertedNames
```

This pipeline will look for the field named `names` in the JSON file we will direct to it, and then, using the [`uppercase`](../configuration/pipelines/processors/uppercase.mdx) processors, convert the names to&mdash;you guessed it&mdash;uppercase.

### Validation

To see whether our pipeline is valid, enter the following command in your terminal:

<Tabs>
  <TabItem value="windows" label="Windows">
    ```PowerShell
    vmetric-director -mode=pipeline -name=convert_names -validate
    ```
  </TabItem>
  <TabItem value="linux" label="Linux">
    ```bash
    vmetric-director -mode=pipeline -name=convert_names -validate
    ```
  </TabItem>
  <TabItem value="macos" label="macOS">
    ```bash
    vmetric-director -mode=pipeline -name=convert_names -validate
    ```
  </TabItem>
</Tabs>

If our pipeline setup is valid, this should return `true` to the command line. If that is the case, we can now run our test.

### Test Run

Enter the following in the command line:

<Tabs>
  <TabItem value="windows" label="Windows">
    ```PowerShell
    vmetric-director -mode pipeline -name convert_names -input ".\config\Examples\sample-data.json" -output ".\config\Examples\processed-sample-data.json"
    ```
  </TabItem>
  <TabItem value="linux" label="Linux">
    ```bash
    vmetric-director -mode pipeline -name convert_names -input "./config/Examples/sample-data.json" -output "./config/Examples/processed-sample-data.json"
    ```
  </TabItem>
  <TabItem value="macos" label="macOS">
    ```bash
    vmetric-director -mode pipeline -name convert_names -input "./config/Examples/sample-data.json" -output "./config/Examples/processed-sample-data.json"
    ```
  </TabItem>
</Tabs>

Here is what this test will do:

* Consume the data in the input file named `sample-data.json` in our working directory
* Using the pipeline named `convert_names`, convert the names in the `names` field to `uppercase`
* Write the converted names to a new field named `convertedNames`
* Save the results to the output file named `processed-sample-data.json` in our working directory.

:::note
The output file should be created when the test is run.
:::

Now open the file named `processed-sample-data.json` to see the results. It should appear as this:

```json
{
  "names": ["john", "june", "mark", "mary"],
  "convertedNames": ["JOHN", "JUNE", "MARK", "MARY"]
}
```

Now that we have seen a pipeline works, we can proceed to creating our end-to-end data flow with a device, a pipeline, and a target for storage.

## Design

For convenience, we will place all our configurations in a single file named `syslog-to-parquet.yml` and place it again in our working directory. In order to avoid any confusion, delete all the YAML files we have previously created so that nothing other than our current configuration is run.

## Step 1: Configure The Device

First, we have to define a device that will receive the log data. We will use the configuration we have created in our [ingesting data example](./example-ingesting-data.mdx) as our starting point, modifying it as fits our needs:

```yml title="syslog-to-parquet.yml"
devices:
  - id: 1
    name: from_syslog
    type: syslog
    properties:
      protocol: udp
      port: 514
      framing: delimiter
      line_delimiter: "\n"
      buffer_size: 16384
      batch_size: 1000
```

Put this device configuration in your file.

This configuration will create a UDP _Syslog_ server listening on port `514`. We have specified the line feed character as the delimiter for message framing. We also set appropriate buffer and batch sizes.

## Step 2: Configure The Target

Next, we will define a target for **Parquet file** storage in our file:

```yml title="syslog-to-parquet.yml"
targets:
  - name: to_parquet
    description: "Parquet file output target"
    type: parquet
    properties:
      location: "<vm_root>/config/Examples"
      name: "from-syslog.parquet"
      batch_size: 1000
      flush_interval: 300
      compression: snappy
```

With this configuration, we are setting up a Parquet file target that will store processed syslog data. We have specified batching and flush intervals for optimal performance, and enabled Snappy compression for efficient storage.

## Step 3: Create A Processing Pipeline

We are now ready to configure our pipeline. Put the following in our configuration file:

```yml title="syslog-to-parquet.yml"
pipelines:
  - name: parse_syslog
    description: "Process Syslog data for Parquet storage"
    processors:
      - grok:
          field: message
          patterns:
            - "%{SYSLOGBASE} %{GREEDYDATA:syslog.message}"
      - grok:
          field: syslog.message
          patterns:
            - "%{DATA:event.action} %{WORD:user.name} from %{IP:source.ip}"
          ignore_failure: true
      - set:
          field: event.kind
          value: event
      - set:
          field: event.category
          value: "syslog"
      - rename:
          fields:
            - from: timestamp
              to: event.created
            - from: source.ip
              to: src.ip
            - from: user.name
              to: user.name_orig
      - remove:
          field:
            - syslog.message
            - message
          ignore_missing: true
```

This configuration will do the following:

* Parse the _Syslog_ header information using the [`grok`](../configuration/pipelines/processors/grok.mdx) processor—reading the fields `syslog.message`, `event.action`, `user.name`, and `source.ip`—and will extract authentication event details

* Set the common event metadata `event.kind` and `event.category`

* Rename fields to create a consistent schema structure

* Clean up the temporary fields.

:::note
To avoid raising exceptions, we chose to ignore failures and missing fields.
:::

## Step 4: Configure the Route

Finally, we have to create a route to connect our device to the pipeline and then to our target. Put the following definition in our configuration file:

```yml title="syslog-to-parquet.yml"
routes:
  - name: syslog_to_parquet
    description: "Route Syslog data to Parquet file"
    devices:
      - name: from_syslog
    pipelines:
      - name: parse_syslog
    targets:
      - name: to_parquet
```

This configuration will connect to the _Syslog_ server, apply the `parse_syslog` pipeline to process the ingested data, and then write it to the **Parquet file** specified.

## Monitoring

Let's put it all together: we have defined a device and a target, configured a pipeline to specify how the data will be processed, and specified a route that will use this pipeline. At this point we have everything ready.

:::note
We will use the `mode` parameter to tell **Director** that we will run a "pipeline", and then use the pipeline's `name` parameter to specify it on the CLI.
:::

We will first validate the pipeline:

<Tabs>
  <TabItem value="windows" label="Windows">
    ```PowerShell
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
  <TabItem value="linux" label="Linux">
    ```bash
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
  <TabItem value="macos" label="macOS">
    ```bash
    vmetric-director -mode=pipeline -name=syslog_to_parquet -validate
    ```
  </TabItem>
</Tabs>

When **Director** is invoked with our configuration file, it starts receiving _Syslog_ messages on port `1514` and processing them through the pipeline and sending them to our specified Parquet file.

Press <kb-short>Ctrl+C</kb-short> to exit the process.

Check the logs for any errors, and verify that the Parquet file is being created with the processed syslog data.
